[
  "Machine Learning-based Thermally-Safe Cache Contention\nMitigation in Clustered Manycores\nMohammed Bakr Sikal, Heba Khdr, Martin Rapp, J ¨org Henkel\nKarlsruhe Institute of Technology , Karlsruhe, Germany\n{bakr.sikal, heba.khdr, martin.rapp, henkel }@kit.edu\nAbstract —We present the first technique that mitigates cache con-\ntention under thermal constraints in clustered manycores. We show by\nmeans of extensive experiments that significant performance gains in\nthis scenario can be achieved. The bac",
  "ce gains in\nthis scenario can be achieved. The background is that concurrently-\nrunning applications on manycore clusters compete for the shared cache,\nslowing down their execution. In addition, heavy parallel computations\non physically-close cores increase temperatures to non-sustainable levels,\nwhich in turn triggers a throttle down of voltage/frequency levels and\nhence performance is compromised. These problems are not unknown,\nbut as our analysis shows, tackling them independently is sub-opt",
  "ysis shows, tackling them independently is sub-optimal. We\nintroduce the first task migration technique that jointly mitigates cache\ncontention while enforcing the thermal constraint at the same time. It\nworks in conjunction with cluster-level dynamic voltage and frequency\nscaling. Our technique needs to predict the impact of task migration\non performance considering cache contention. Since it is impossible to\nderive an analytical model for cache contention that is both sufficiently\naccurate and",
  " contention that is both sufficiently\naccurate and practically feasible to implement, we employ an accurate,\nyet lightweight neural network (NN) model. As a result, we can operate\nthe manycore system at higher performance while safely staying within\nthermal constraints. We report a significant step forward in this paper\nand unveil new potentials for performance optimization.\nIndex Terms —Application migration, performance maximization, re-\nsource management, cache contention, DVFS, machine learn",
  " management, cache contention, DVFS, machine learning\nI. I NTRODUCTION\nPerformance maximization on manycores has been bottlenecked by\nthe processor-memory gap for decades. To alleviate this problem, chip\nmicroarchitectures host more advanced memory hierarchies compris-\ning multi-level caches. One example is the modern AMD Zen 3\nclustered manycores [1] shown in Fig. 1, where cores in each cluster\nshare the same last-level cache (LLC). Executing multiple applica-\ntions simultaneously on manycores ",
  "ltiple applica-\ntions simultaneously on manycores can degrade their performance for\ntwo reasons. First, concurrently-running applications on one cluster\ncompete for the limited available cache. Such cache contention slows\ndown application execution, thereby degrading the overall system\nperformance. The second type of interference between concurrently-\nrunning applications is the heat transfer between their cores, which\nmight occur even across clusters. If temperature increases beyond\nsafe thresh",
  "sters. If temperature increases beyond\nsafe thresholds, the Thermal Control Circuitry (TCC), which typically\nexists on commercial processors [2] [3], is triggered to cool down\nthe chip by setting the voltage/frequency (V/f) of all cores to the\nminimum level, irrespective of the core causing the violation. Clearly,\nthis degrades the performance of all running applications.\nCache contention has been mitigated with architecture-level mech-\nanisms, e.g., cache partitioning [4]. However, this require",
  ".g., cache partitioning [4]. However, this requires special\nhardware support [5]. Recently, researchers have started to mitigate\ncache contention at system level through application mapping opti-\nmization, which is an always-available means on modern platforms.\nThe work in [6] employs machine learning (ML) to estimate cache\ncontention on clusters and maps newly-arrived applications to the\ncluster with the lowest predicted contention. This technique aims at\nbalancing contention between clusters, ",
  "ue aims at\nbalancing contention between clusters, thereby mitigating it through-\nout the chip. Nevertheless, cache contention varies throughout execu-\ntion time due to changes in execution phases, thus the desired balance\nwill be lost. A more advanced approach [7] adjusts applicationCluster 1\nCore 1 Core 2 Core 3 Core 4\nCore 5 Core 6 Core 7 Core 8LLCCluster N\nCore 1 Core 2 Core 3 Core 4\nCore 5 Core 6 Core 7 Core 8LLC\nMemory ControllersDRAM\nFig. 1. The AMD Zen 3microarchitecture [1], as an exampl",
  ". The AMD Zen 3microarchitecture [1], as an example of a clustered\nmany-core, where the LLC is shared by the cores in each cluster, which may\nbenefit from our novel technique.\nmapping at runtime through application migration to continuously\nbalance/mitigate cache contention throughout execution time.\nThermal interference between applications has been widely tackled\nby system-level resource management techniques, as they realize that\nperformance cannot be maximized without enforcing thermal safet",
  "annot be maximized without enforcing thermal safety.\nTo this end, application migration and dynamic voltage and frequency\nscaling (DVFS) have been used. Migrating an application to colder\ncores may enable it to continue executing without downscaling the\nV/f levels, or even to execute at a higher V/f level on destination\ncores if they have sufficient thermal margins, leading to performance\nimprovements, as shown in [8]. When application migration is not\nsufficient to maintain thermal safety, V/f ",
  "is not\nsufficient to maintain thermal safety, V/f levels can be downscaled.\nIn summary, the state-of-the-art performance maximization tech-\nniques focus on either mitigating cache contention or maintaining\nthermal safety. This is sub-optimal . To maximize performance, both\ninterference problems need to be jointly tackled. In the following\nmotivational example, we first show how migrating applications\nconsidering one interference problem, e.g., cache contention, can ex-\nacerbate the other interfe",
  "che contention, can ex-\nacerbate the other interference problem, e.g., temperature. Secondly,\nwe show why cache contention is a complex behavior.\nA. Motivational Example\nWe consider here three clusters of a clustered manycore1, con-\nstrained by a critical temperature Tcrit of 80◦C. This example\n(shown in Fig. 2) illustrates the impact of application migration\non performance and temperature without the interference of any\ntemperature emergency control, such as TCC [2].\nInitially, two applications",
  "trol, such as TCC [2].\nInitially, two applications2,lu.cont andx264 are running in parallel\non cluster 1, while two others, water.sp and barnes , are running\nindividually on clusters 2 and 3, respectively. The performance\nof applications is quantified by their execution times. lu.cont is\nour application of interest. We observe a slowdown of 23% in its\nperformance on cluster 1 compared to its performance when run\nindividually on a cluster. This slowdown indicates a strong cache\ncontention with x2",
  "owdown indicates a strong cache\ncontention with x264 . Thus, we test whether lu.cont would benefit\nfrom a migration to another cluster.\nWhen migrating lu.cont to cluster 2, we observe an improvement\nof 16% in its performance, with a negligible degradation of 1% in\nwater.sp’s performance. While this migration improves the overall\nsystem performance by reducing cache contention, it results in a peak\n1Details about the used simulation setup are provided in Section VI.\n2The applications used in this",
  "ided in Section VI.\n2The applications used in this work are from PARSEC [9],SPLASH-2 [10].2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247708\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "lu.cont x264\n−23 % ±0 %water.sp barnesCluster 1 Cluster 2 Cluster 3Initial\nstate\nx264 lu.cont water.sp barnes83.5◦CFirst\nmigration∆Performance: +16 % −1 % ±0 %\nx264 water.sp lu.cont barnes78.9◦CSecond\nmigration∆Performance: ±0 % +8 % −0.3 %\nFig. 2. The initial state shows a strong cache contention on cluster 1,\nindicated by a 23% slowdown of lu.cont’s execution compared to its execution\nindividually on a cluster. Two migrations are conducted to reduce contention\nand compared to the initial state",
  "educe contention\nand compared to the initial state. It is observed that application migration\nexposes a trade-off between cache contention and temperature.\ntemperature of 83.5◦C, violating Tcrit. That would trigger TCC to\ncool down the chip by running all cores at the minimum V/f level,\nthereby degrading or even erasing performance gains from mitigating\ncache contention. When lu.cont is migrated to cluster 3, its perfor-\nmance improved by 8% without degrading barnes’s performance and\nwithout vio",
  "out degrading barnes’s performance and\nwithout violating Tcrit, thereby sustaining performance gains. This\nanalysis shows the importance of considering both cache contention\nand temperature within the application migration process to maximize\nperformance under a temperature constraint.\nTo further investigate cache contention behavior in this example,\nwe analyze the LLC and DRAM accesses since they are indicators for\ncache contention. In Fig. 3, we show the LLC and DRAM accesses\nof each of the th",
  "e show the LLC and DRAM accesses\nof each of the three applications that co-execute with lu.cont , along\nwith its corresponding execution time. We can see that x264 , which\nleads to the longest execution time for lu.cont , pointing out the\nhighest cache contention, has the highest LLC/DRAM accesses, as\nwould be expected. Nevertheless, water.sp , which leads to the shortest\nexecution time for lu.cont , pointing out the lowest cache contention,\ndoes not have the lowest LLC/DRAM accesses. This is co",
  " not have the lowest LLC/DRAM accesses. This is counter-\nintuitive. We can infer that depending on one or two indicators of\ncache contention is not sufficient to estimate it , while the state-of-the-\nart technique [7] adopts only DRAM accesses as the cache contention\nindicator. Additionally, we observe that on cluster 3, lu.cont has\na slowdown of 14% (compared to its execution time individually\non a cluster) while the co-running application barnes only shows\na 3% slowdown. Hence, cache contentio",
  "s only shows\na 3% slowdown. Hence, cache contention on a cluster can result in\ndifferent slowdowns in the performance of different applications . This\ninvestigation shows that the relation between cache contention and\napplication performance is complex .\nB. Challenges and Contributions\nMotivated by our findings shown in the motivational example,\nwe introduce the first application migration technique that aims at\nmitigating cache contention to maximize performance, while main-\ntaining thermal saf",
  "imize performance, while main-\ntaining thermal safety . Migrating an application from one cluster to\nanother can reduce the contention on the source cluster and increase\nthe contention on the destination cluster , if the latter executes other\napplications as well. Thus, application migration can improve the\noverall system performance if contention on the destination cluster\nis less than contention on the source cluster, and if this migration is\nthermally safe. Hence, there is a need to predict m",
  "hermally safe. Hence, there is a need to predict migration impact\non the chip temperature and on the overall system performance\nconsidering cache contention on both source and destination clusters.\nWhile estimating the chip temperature for a different application-\nto-core mapping is straightforward using the well-known RC thermal\nmodel [11], estimating the migration impact on performance consid-x264 water.sp barnes00.511.5LLC/DRAM Metrics\n(normalized to x264 ) LLC Accesses DRAM Accesses Exec. Ti",
  "ized to x264 ) LLC Accesses DRAM Accesses Exec. Time of lu.cont\n11.11.2\n16 %8 %\nNorm. Exec. Time\noflu.cont (ms)\nFig. 3. The execution time of lu.cont is impacted differently when co-executed\nwith different applications: x264 ,water.sp orbarnes on one cluster. Depending\nonly on DRAM or LLC accesses, as the state-of-the-art technique [7] does,\nis not sufficient to estimate cache contention.\nering contention is challenging due to the following reasons:\n1) Cache contention between applications is co",
  "ns:\n1) Cache contention between applications is complex and hard to be\nestimated analytically. First, it has diverse impacts on the performance\nof concurrently-running applications, depending on their instruction\nsequence, number and timing of cache accesses, etc. Additionally,\nrelevant cache contention indicators do not provide clear information\nabout how to quantify the impact of cache contention on application\nperformance, as demonstrated in Fig. 3.\n2) It is not feasible to estimate cache con",
  "ig. 3.\n2) It is not feasible to estimate cache contention impact by profiling\nthe concurrent execution of applications at design time because the\nnumber of potential execution scenarios is exponential considering\nall possible configurations, e.g., number of concurrent applications\non the cluster, number of parallel threads per application, V/f levels\nof the clusters, and arrival times of applications. Varying any of these\nparameters can change the cache contention behavior.\nThis work tackles the",
  "e cache contention behavior.\nThis work tackles the challenge of predicting the impact of cache\ncontention by employing an NN model, since an analytical model\nis not possible under reasonable effort. In particular, we develop an\nNN model to predict the impact of an application migration on the\nperformance of all applications running on source and destination\nclusters, by taking as input the most relevant performance counters\nthat indicate cache contention. This model enables us to build a\nsmart c",
  "ntention. This model enables us to build a\nsmart cache contention mitigation technique, called SmartCM , that\nperiodically invokes the model to evaluate the impact of potential\nmigrations on the overall system performance. The impact of these\nmigrations on the chip temperature will be also checked, so that the\nmigration expected to lead to the highest overall performance without\nviolating Tcritis performed. Additionally, DVFS is employed on the\nclusters to exploit available thermal margins and t",
  "lusters to exploit available thermal margins and to avoid potential\nthermal violations that may occur due to changes in the execution\nphases of applications.\nOur novel contributions are:\n•We introduce a task migration technique to, for the first time, jointly\nmitigate cache contention and enforce thermal safety, unveiling new\nperformance optimization potentials.\n•This technique is enabled by an accurate, yet lightweight, NN\nmodel to predict the impact of a migration on performance con-\nsidering ",
  "mpact of a migration on performance con-\nsidering cache contention on source/destination clusters.\nII. R ELATED WORK\nA large body of resource management techniques in the literature,\ne.g.,[12–15], aim at maximizing performance under a temperature\nconstraint using different means, such as DVFS, application map-\nping/migration. The work in [13] uses application mapping by\nselecting different patterns of active/inactive cores, depending on\napplication characteristics, to achieve a better thermal di",
  "on characteristics, to achieve a better thermal distribution\non the chip. The resulting thermal headroom is then exploited to\nboost the V/f levels of cores, thereby improving performance. Other\nworks propose to dynamically adapt application mapping at runtime\nby migrating threads from hot cores to cold ones to further improve\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "Restrictions apply.",
  "performance. The technique in [8] uses adaptive power budgeting\nto periodically adjust thread-to-core mappings at runtime to enable\nboosting the colder cores to higher V/f levels. Nevertheless, none of\nthese techniques has considered the cache contention problem.\nThe problem of cache contention has been tacked by several works\nusing architecture-level mechanisms [5], such as cache partitioning.\nThe work in [4] partitions the shared LLC to applications based\non a performance prediction model. The",
  "tions based\non a performance prediction model. They target a single global\nLLC, which is a simpler problem as no application migration to\nassign applications to clusters is required. Because architecture-level\ntechniques require special hardware support, several researchers have\ntackled cache contention at system level. A model to estimate cache\ncontention-induced slowdowns in application executions is proposed\nin [16]. This model is extracted from design-time profiling of appli-\ncations and can",
  "om design-time profiling of appli-\ncations and cannot be used for unknown workloads. Two recent tech-\nniques have targeted cache contention mitigation by optimizing appli-\ncation mapping. In [6], an ML-based application mapping technique\nis proposed to predict cache contention on clusters, and accordingly\nmap applications to the cluster with the least predicted contention.\nThe authors in [7] propose to migrate applications between clusters\nat runtime to consciously mitigate contention. Particula",
  "time to consciously mitigate contention. Particularly, their\ntechnique periodically monitors the shared bus accesses from each\ncluster, i.e., DRAM accesses in case of LLC cache misses, as the\ncache contention indicator, and accordingly migrates applications\nbetween cluster to mitigate contention. Additionally, they propose to\nuse an NN to predict the maximum V/f levels of clusters that do not\nviolate a power constraint. Their technique is the closest work to our\nSmartCM , as they aim at mitigati",
  "sest work to our\nSmartCM , as they aim at mitigating contention through application\nmigration while satisfying a power constraint.\nIn summary, none of the state-of-the-art cache mitigation tech-\nniques has considered the thermal safety, as it seems as an in-\ndependent problem, typically enforced by hardware solutions, e.g.,\nTCC. Our motivational example has shown for the first time how\nsolving these problems independently is sub-optimal, while solving\nthem jointly helps exploiting the full poten",
  "lving\nthem jointly helps exploiting the full potential of performance maxi-\nmization.\nIII. P ROBLEM FORMULATION\nWe target a many-core processor with Nhomogeneous cores split\nintoCclusters, each contains n=N/C cores that share one LLC.\nCluster-level DVFS is supported, i.e., cores within a cluster coperate\nat the same V/f level fc. The matrix Q= [qi,c]N×Cdefines the\ncore attribution to clusters, where qi,c= 1 iff core ibelongs to\ncluster c. The well-known RC-thermal model [11], denoted as TM,\nis u",
  "l-known RC-thermal model [11], denoted as TM,\nis used to estimate the steady-state temperatures of all cores, T= [ti]N\nbased on their power consumption (dynamic and leakage), denoted\nasP. We target an open system [17] where Kmulti-threaded\napplications arrive to the system at a-priori unknown times. Multiple\napplications can be mapped to the same cluster. Application-to-\ncluster mappings are identified by a binary matrix G= [gc,k]C×K.\nEach application kcan run hkthreads in parallel, thereby requ",
  "ation kcan run hkthreads in parallel, thereby requiring\nhkcores to execute, following the common one-thread-per-core\nmodel [18]. Thread-to-core mappings of applications are identified by\na binary matrix V= [vi,k]N×K. The instructions per second (IPS) are\nused as a proxy metric to measure the performance of an application k\nthroughout execution time, denoted as IPS k. The objective is to\nmaximize the overall system performance, quantified by the response\ntime of applications, through cache conten",
  "esponse\ntime of applications, through cache contention mitigation , while\nmaintaining thermal safety.\nWe introduce SmartCM to solve this problem as illustrated\nin Fig. 4. Our NN model is trained at design time to predict the\nimpact of potential migrations on performance (Section IV) and usedbySmartCM at runtime (Section V-A). Additionally, SmartCM peri-\nodically selects the V/f levels of the clusters to exploit any available\nthermal margins on the cores to further improve performance and to\navoi",
  "e cores to further improve performance and to\navoid any potential thermal violation that could occur due to changes\nin execution phases of applications (Section V-B).\nIV. NN FOR CONTENTION -AWARE PERFORMANCE PREDICTION\nEstimating the impact of application migration on performance\nconsidering contention is too complex to be tackled by analytical\nmodels, as detailed in Section I-B. SmartCM addresses this by\nemploying an NN model. In the following subsections, we present\nour methodology of generati",
  "ubsections, we present\nour methodology of generating training/test data and selecting the\nfeature set and topology of our NN model.\nA. Training Data Generation\nTo generate training data for our intended NN model, we first\nexecute many migration scenarios using several multi-threaded ap-\nplications and collect traces from their executions. In a migration\nscenario, one application of interest (AoI) is migrated from a source\ncluster s, operating at frequency fsand executing other applica-\ntions γ, ",
  "frequency fsand executing other applica-\ntions γ, to a destination cluster doperating at frequency fdand\nexecuting applications ω. As mentioned in Section I-B, the number\nof such scenarios is exponential considering all combinations of\napplications, their arrival times, number of threads, etc. Thus, we\nrun simulations with one or two multi-threaded applications on a\ncluster (arriving to the system at the same time), each with 4 parallel\nthreads, as shown in Fig. 4. It is important to note that o",
  "as shown in Fig. 4. It is important to note that our model is\nused in our evaluation experiments (Section VI) to predict migration\nimpacts for unseen scenarios that run multiple, i.e., more than two,\napplications per cluster with 1, 2, 3, or 4 parallel threads.\nIn each migration scenario, the goal is to measure the slowdown\nexerted by γandωon AoI, before and after migration, respectively.\nThis can be achieved by comparing the performance of AoI when ex-\necuted individually against its performanc",
  "hen ex-\necuted individually against its performance when co-executed with γ\norω, The features in each scenario are selected from the Performance\nCounters (PCs) of the cores, i.e., performance and cache/memory\nstatistics, that execute applications on clusters sanddbefore and\nafter migration. Since AoI, γandωcould be executing at different\nV/f levels on their corresponding clusters sandd, frequencies fsand\nfdare also included in the feature set. The label of the model is the\nIPS of AoI after migra",
  "e label of the model is the\nIPS of AoI after migration. Since cache contention varies depending\non the execution phases of co-running applications, and to allow the\nmodel to grasp this aspect, each execution trace is divided into slices\nof a fixed length, equal to the migration epoch of the system, i.e.,\n10ms, as follows. First, we extract slice St, delimited by ( t1,t2), from\nthe trace of AoI and γon cluster sbefore migration, and their PC AoI\nand PC γ. PC AoIonStare the PCs of AoI when co-runn",
  "nd PC γ. PC AoIonStare the PCs of AoI when co-running with γ\nfromt1tot2on cluster sat frequency fs. PCγare the PCs of γwhen\nco-running with AoI in the same slice. We also extract the start and\nend instructions (insAoI\n1,insAoI\n2) executed by AoI between t1andt2.\nSince AoI will be co-running with ωon cluster dafter migration at\nfrequency fd, we also extract slice Dt′from their trace, corresponding\nto the same instructions (insAoI\n1,insAoI\n2) and delimited by ( t′\n1,t′\n2). This\nallows us to comput",
  "elimited by ( t′\n1,t′\n2). This\nallows us to compute IPS AoI′on slice Dt′, the label of our model,\ndenoting the performance of AoI when co-running with ωon cluster d\nbetween t′\n1andt′\n2at frequency fd. Then we extract a third slice,\nDt, from the individual trace of ωand its corresponding PC ω. The\nobtained slices St,DtandDt′represent one migration scenario of\napplication AoI, and contain the characteristics of AoI, γandωbefore\nmigration, and the performance of AoI after migration. Each retained\ns",
  "erformance of AoI after migration. Each retained\nslice is one entry in our training dataset, and the same slicing process\nis repeated for all other migration scenarios. Finally, our training data\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "AoI App.\nγCluster s @ fs\nApp.\nωCluster d @ fd\nApp.\nγCluster s @ fs\nAoI App.\nωCluster d @ fdBefore Migration After MigrationMapping Traces\nFeatures Label\nPCAoI,with γPCγ,with AoI PCωindividual fsfdIPS AoI,with ω\n. . . . . . . . . . . . . . . . . .Dt′ St Dt\nTrainingFind Mig. Candidates\nEvaluate Mig. Destinations\nThermal Safety Check\nSelect Best Mig. and ExecuteAlg. 1every 10 ms\nContention-\nAware\nPerformance\nModelPCs,\nfs,\nfd\nIPS\nCluster-Level DVFSevery 1 msThermal\nModelP\nT′\nP\nT′Design Time Run Time",
  "ery 1 msThermal\nModelP\nT′\nP\nT′Design Time Run Time\nFig. 4. Overview of SmartCM : at design time, training data is generated to train an NN model. At runtime, the NN is employed periodically to predict the\npotential performance impact of migrations, then selects the migration that would result in the best performance gains without violating the thermal constraint.\ncomprises ∼2M rows, each defined by the feature tuple (PC AoI, PCγ,\nPCω,fs,fd) and the label, IPS AoI′.\nB. Feature Selection and Model",
  "he label, IPS AoI′.\nB. Feature Selection and Model Topology\nThe training data comprises the frequencies fsandfd, and\nrecorded PCs, i.e., periodic IPS and multiple memory- and cache-\nrelated statistics of AoI, γandω. This dataset is randomly split into\n75% training and 25% test data. We first train a model with the\nhighest possible accuracy, and then reduce its complexity to decrease\nthe overhead. We use Lasso Regression to identify features with the\nhighest predictability of the label IPS AoI′, ",
  "the\nhighest predictability of the label IPS AoI′, and the Pearson Product-\nMoment Correlation matrix to analyze their correlations. The model\nwith the highest accuracy uses 6 hidden layers, each containing 256\nneurons and achieves a very high accuracy with a mean-absolute-\npercentage error (MAPE) score of 1.5%. Yet, the inference time with\nthis model on our target architecture reaches 35 µs, resulting in an\nunacceptable overhead at runtime. Thus, we decrease the overhead by\nreducing the number o",
  " we decrease the overhead by\nreducing the number of features and the topology (number and width\nof layers). The NN with the best trade-off between overhead and\naccuracy uses six features ( fs,fd, IPS, LLC accesses, LLC misses\nand DRAM accesses) and consists of 4 hidden dense layers (64, 128,\n128 and 64 neurons) with ReLU activation and one output layer of\n1 neuron with linear activation. It achieves a MAPE score of 2.3%.\nThe inference time on our target platform is as low as 13 µs.\nV. S MART CAC",
  " target platform is as low as 13 µs.\nV. S MART CACHE CONTENTION MITIGATION\nThe right part of Fig. 4 shows the runtime phase of SmartCM .\nAt each migration epoch i.e., 10 ms3,SmartCM invokes the trained\nNN to predict the impact of potential migrations on the overall\nsystem performance and performs the migration that would maximize\nperformance without violating Tcrit. At each DVFS epoch, i.e., 1 ms4,\nSmartCM adjusts the V/f levels of the clusters to exploit any available\nthermal margins on the cor",
  "o exploit any available\nthermal margins on the cores to further improve the performance,\nand to avoid any potential thermal violation that could occur due to\nchanges in the execution phases of running applications.\nA. ML-Based Application Migration\nOur migration policy (Algorithm 1) executes the following steps:\n1) Migration Candidate Search: SmartCM starts by finding the\nlist of applications that are eligible for migration. AoI is eligible for\nmigration, if other applications γare running on th",
  "igration, if other applications γare running on the same source\ncluster s. Since the target system supports cluster-level DVFS, both\nAoI and γare running at the same V/f level fs.\n2) Destination Cluster Lookup: For each candidate AoI, we\nsearch for potential destination clusters having enough free cores to\n3Equal to Linux control epoch, where task migration can be performed [19].\n4Equal to DVFS epoch in commercial processors, e.g., [20].Algorithm 1 SmartCM Migration Policy\nInput: # of apps K, im",
  " 1 SmartCM Migration Policy\nInput: # of apps K, improvement factor I, current power P\nOutput: application to migrate AoI, destination cluster d\nX← {} ▷empty list of migration operations\nfor each AoI∈ {1, . . . , K }do ▷iterate over running apps\ns← {ℓ:gℓ,AoI= 1} ▷source cluster\nγ← {k:gs,k= 1} \\ { AoI} ▷co-running apps with AoI\nif|γ|<1then continue ▷AoI is running individually\nfor each d∈ {1, . . . , C } \\ {s}do▷iterate over other clusters\nJ← {i:qi,d= 1∧ ∀k vi,k= 0}▷free cores on cluster d\nif|J|< ",
  ",d= 1∧ ∀k vi,k= 0}▷free cores on cluster d\nif|J|< h AoIthen continue ▷not enough free cores on d\nω← {k:gd,k= 1} ▷apps running on cluster d\nIPSt←IPS AoI+IPSγ+IPSω ▷current performance\nIPS′\nt←PREDICT (fs, fd,AoI, γ, ω) ▷new performance\n∆Perf←IPS′\nt/IPSt−1\nif∆Perf> I∧SAFE(fs, fd,AoI, s, d)then\nX←X∪ {(AoI, s, d, ∆Perf)} ▷save migration\nxbest←arg max X(∆Perf)▷migration with best performance\nprocedure PREDICT (fs, fd,AoI, γ, ω )▷performance prediction\nIPS′\nAoI←PM(fs, fd,PCAoI,PCγ,PCω)\nIPS′\nγ←PM(fs, fs",
  "IPS′\nAoI←PM(fs, fd,PCAoI,PCγ,PCω)\nIPS′\nγ←PM(fs, fs,PCγ,PCAoI,0)\nIPS′\nω←PM(fd, fd,PCω,0,PCAoI)\nreturn IPS′\nAoI+IPS′\nγ+IPS′\nω\nprocedure SAFE(fs, fd,AoI, s, d ) ▷thermal safety check\nP←Po+PAoI(s→d, fs→fd)▷power with AoI running on d\nˆT←maxTM(P) ▷peak steady-state temperature on chip\nreturn ˆT < T crit\nhost AoI. If a destination cluster dis found, a potential migration x\nis added to the set of potential migrations, referred to as X, but will\nonly be performed if it is predicted to improve the overal",
  "performed if it is predicted to improve the overall system\nperformance and thermally safe.\n3) Migration Performance Evaluation: A migration xwill\nimpact only the performance of source and destination clusters. This\nmigration improves the overall system performance if the accumu-\nlated performance of source and destination clusters after migration,\ndenoted as IPS′\nt, is higher than IPS tbefore migration. Although\nonly AoI will be migrated, migration xalso affects the performance\nof the other appl",
  "on xalso affects the performance\nof the other applications running on the source and destination\nclusters, i.e., γandω, respectively, since the cache contention on\nclusters will change. When AoI is co-running with more than one\napplication on the source cluster, IPS γis the average IPS of all co-\nrunning applications, and PC γare the accumulated PCs of all the\nactive cores on cluster s, excluding those assigned to AoI. This also\napplies to ωon the destination cluster d. We therefore define IPS t",
  "e destination cluster d. We therefore define IPS t\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "as IPS t=IPS AoI+IPSγ+IPSω.\nTo evaluate the performance impact of this migration, we employ\nour NN to predict the new performances of AoI, γandωafter\nmigration. First, we predict IPS′\nAoIgiven PC AoIand PC γon the source\ncluster sand PC ωon the destination cluster d. Secondly, to predict\nIPS′\nγ, we invoke the NN with PC γ, PC AoIand zeros instead of PC ω,\nas there will be no contention between γand AoI after the latter is\nmigrated. Similarly, to predict IPS′\nω, we invoke the NN with PC ω,\nzeros ",
  "predict IPS′\nω, we invoke the NN with PC ω,\nzeros instead of PC γand PC AoI, since ωare initially running on the\ndestination cluster without AoI. At this stage, IPS′\ntafter migration is\nthe sum of IPS′\nAoI, IPS′\nγand IPS′\nω. Then, the change in the IPS of\nAoI,γandωis computed as ∆Perf=IPS′\nt/IPSt−1. If∆Perf> I,\nmigration xis predicted to result in an improved system performance,\nwhere Iis empirically set to account for model mispredictions.\n4) Thermal Safety Evaluation: If performance improvemen",
  "ermal Safety Evaluation: If performance improvement is\npredicted in the previous step, SmartCM must additionally ensure\nthat migration xis thermally safe. For this purpose, we use the\nRC-thermal model [11] to predict the steady-state temperatures of\nall cores. Since cluster dcould be operating at a V/f level fd\ndifferent from fs, AoI will also operate at fdafter migration. Thus,\nthe new power consumption PAoIneeds to be scaled from fsto\nfdto estimate the new steady-state temperature T′by the the",
  "mate the new steady-state temperature T′by the thermal\nmodel. If T′does not violate Tcrit, migration xis thermally safe,\nits corresponding ∆Perfxis saved, and the search for other possible\nmigrations continues. At the end of the search, the thermally safe\nmigration xbestwith the highest predicted ∆Perfbestis performed.\nB. Cluster-Level DVFS\nTo operate clusters at the maximum V/f levels without violating\nTcrit,SmartCM periodically boosts or throttles down the V/f levels\nof clusters, to adapt to r",
  "les down the V/f levels\nof clusters, to adapt to runtime variations in the power consumption\nof applications due to their varying execution phases. At each DVFS\nepoch, the steady-state temperature is estimated using the RC-thermal\nmodel [11] given the current power consumption. If a thermal\nviolation is predicted, SmartCM iteratively identifies which clusters\nto throttle down. In each search iteration, a cluster cthat has the\ncore with the highest temperature is added to the list of clusters to\n",
  "t temperature is added to the list of clusters to\nbe throttled down to the next lower V/f level. Then, the steady-state\ntemperature is re-estimated assuming that cluster cwill be throttled\ndown, thereby power is scaled down. The search continues until the\npredicted highest temperature is below Tcrit. At the end of this step,\nthe DVFS policy throttles down the selected clusters to their selected\nnew V/f levels. In case no thermal violation is predicted in the current\nDVFS epoch, and there are the",
  "icted in the current\nDVFS epoch, and there are thermal margins on the cores, SmartCM\nupscales the V/f levels of the clusters by only one step (if this is\nthermally safe). Upscaling by one step reduces the scaling errors of\nthe power consumption from the current V/f level to the next one,\nthus increases the accuracy of temperature prediction.\nVI. E XPERIMENTATION RESULTS\nWe run simulations on an augmented version of the Sniper [21]\nsimulator with McPAT [22] and HotSpot [11] integration for power\n",
  "McPAT [22] and HotSpot [11] integration for power\nand temperature estimations, respectively. We model a many-core\nprocessor organized in 8 clusters of 8 cores, similar to the commercial\nAMD Zen 3 [1] microarchitecture. Each core has 64 KB of L1\ncache and 256 KB of L2 cache. All cores within the same cluster\nshare 8 MB of LLC and one memory controller. Per-cluster DVFS\nsets frequencies between 1GHz and 4GHz with steps of 200MHz.\nThe default HotSpot cooling parameters are used with an ambient\ntemp",
  "t cooling parameters are used with an ambient\ntemperature of 45◦C and Tcrit is 80◦C. Our simulations use 15\ndifferent multi-threaded applications from two benchmark suites\nPARSEC [9] and SPLASH-2 [10]. These applications cover a wide\nrange of different domains, e.g., financial analytics, computer vision,4050607080250300350400\n13%Norm. Avg.\nResp. Time (ms)\n405060708012%\n405060708015%Our\nSmartCM\nSoA [7]\n4050607080−2002040Norm. Improvement\nover [7] (%)\n4050607080405060708045%\nAverage,\nOverall\nPerfo",
  "(%)\n4050607080405060708045%\nAverage,\nOverall\nPerformanceMaximum\n75%-Quant.\nMedian\n25%-Quant.\nMinimum(a) Workload 1\nAvg. Application Arrival Rate (per Second)(b) Workload 2(c) Workload 3\nFig. 5. SmartCM shows significant improvements in the performance of the\nthree different mixed workloads, reaching up to 45% for some applications,\ncompared to SoA [7]. On average, SmartCM improves the overall system\nperformance by 11% and by up to 15% compared to the SoA [7].\nimage processing etc. Each applicati",
  " the SoA [7].\nimage processing etc. Each application can be executed with 1, 2, 3\nor 4 parallel threads. We generate three different mixed workloads,\neach with 40 randomly selected applications, 50% of which are\nmemory-intensive. The arrival times of applications are sampled\nfrom a Poisson distribution with 5 different arrival rates, to consider\nvarious system utilization values. It is important to note that in these\nevaluation experiments, 40 applications with different numbers of\nthreads arriv",
  "plications with different numbers of\nthreads arrive to the system at different arrival rates, resulting in up\nto 8 applications running concurrently on a cluster. These scenarios\nareunseen by our NN model during training, and thus enable us to\nevaluate the generalization of the NN prediction model.\nComparison Technique: As mentioned in Section II, the state-of-\nthe-art technique [7], denoted as SoA, is the closest to SmartCM , as\nit aims at mitigating cache contention through application migrati",
  "ating cache contention through application migration\nunder a power constraint. At each migration epoch, SoA rearranges\napplications on a cluster depending on the accumulated shared bus\naccesses, as to balance the cache contention effects across the chip.\nIn addition, SoA uses an NN to predict the maximum V/f level\nof the clusters that would not violate the power constraint of the\nchip. However since SmartCM considers a thermal constraint , we\nreplace the power constraint of SoA by a Thermal Safe",
  "lace the power constraint of SoA by a Thermal Safe Power (TSP)\nconstraint [23]. In particular, TSP is the maximum power that can\nbe consumed by each active core without violating Tcrit, depending\non the number of active cores on the chip. Adapting SoA to consider\nTSP is necessary to enable a fair comparison with our SmartCM , as\nboth are now targeting the same goal, i.e., performance maximization\nunder a thermal constraint. To train their model, we used the same\nsimulation traces used to train o",
  "we used the same\nsimulation traces used to train ours.\nExperimental Results: Fig. 5 (top) shows the normalized average\nresponse time of each workload at each arrival rate for SmartCM\nand SoA. We observe that SmartCM always achieves a shorter\naverage response time compared to SoA, indicating higher overall\nsystem performance, by 11% on average and by up to 15%. This\nindicates that SmartCM achieves more reductions in cache contention\ncompared to SoA, thanks to our accurate model that employs multi",
  "A, thanks to our accurate model that employs multiple\nrelevant cache indicators, while SoA considers only one metric, i.e,\nDRAM accesses. To provide a detailed comparison, we compare the\nexecution time of each application in the workload with SmartCM\nto the execution time of the same application with SoA. The im-\nprovement/degradation percentages in the application execution time\nwhen SmartCM is applied compared to SoA are reported in Fig. 5\n(bottom). We observe that the performed migrations by ",
  "tom). We observe that the performed migrations by SmartCM\nhave impacted applications differently in each workload and arrival\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "40506070806080100Chip Temperature (◦C)\n4050607080Thermal Constraint Our SmartCM SoA [7]\n4050607080Outliers\n99%-Quant.\n75%-Quant.\nMedian\n25%-Quant.\n1%-Quant.\nOutliers(a) Workload 1\nAvg. Application Arrival Rate (per Second)(b) Workload 2 (c) Workload 3\nFig. 6. SmartCM guarantees a thermally-safe operation of the system\nwhile significantly improving the performance of all three mixed workloads.\nContrarily, SoA [7] shows frequent violations of the thermal constraint.\ntime, compared to SoA. While th",
  "hermal constraint.\ntime, compared to SoA. While the performance of most applications is\nimproved (by up to 45%), the performance of a few other applications\nis degraded, which can be interpreted as follows.\nFirstly, as explained previously, each performed migration is ex-\npected to benefit the applications on the source cluster, but degrade\nthe performance of the applications on the destination one. Secondly,\nthe DVFS policy of SoA boosts some clusters to higher frequencies\nthan those selected b",
  "usters to higher frequencies\nthan those selected by SmartCM . However, as can be seen in Fig. 6,\nthese few improvements come at the expense of thermal violations,\nreaching up to 98◦C, although TSP values were used to train\ntheir model. This is explained by the fact that the decisions of\ntheir DVFS policy are directly predicted by their NN. Consequently,\nany inaccuracy in the NN prediction is directly reflected in the\nselected V/f level, which in turn affects the temperature of the\nchip. It follo",
  "turn affects the temperature of the\nchip. It follows that mispredicting a higher V/f level boosts the\nperformance of applications on the cluster, but leads to violating the\npower constraint, i.e, TSP. On the other hand, SmartCM only uses\nthe NN for performance prediction in the migration policy and only\nperforms the application migrations that are predicted to result in\nperformance gains higher than factor I, i.e., a predicted performance\ngain lower than Ican be a model misprediction. In the eve",
  "wer than Ican be a model misprediction. In the event of\na performance misprediction, migrating an application might indeed\nharm the overall system performance. Nevertheless, a migration is\nonly performed if it passes the thermal safety check. Similarly, our\nDVFS periodically estimates the steady-state temperatures before\nboosting the V/f levels of clusters and only boosts each cluster by\none frequency step per epoch, enabling more accurate power/thermal\npredictions, thus avoiding potential therm",
  "thermal\npredictions, thus avoiding potential thermal violations. Finally, our\nSmartCM only performs 1.5migrations on average per application,\ni.e., an application is migrated less than two times on average\nthroughout its execution, indicating a stable operation of SmartCM .\nOverhead discussion: To evaluate the run-time overhead of our\ntechnique, we deploy SmartCM as an application, running on one\ncore on the same target platform at 4GHz. We report the average run-\ntime overhead when running the ",
  "t the average run-\ntime overhead when running the workloads studied in Section VI. The\nmigration policy of SmartCM takes 170 µs on average (only 1.7% of\nthe migration epoch), out of which, only 39 µs are required for model\ninference. This is enabled by our choice of an optimized topology of\nthe model, as explained in Section IV-B. Our DVFS policy takes\n9.5µs on average, which is only 0.95% of the DVFS epoch. In\nsummary, the overhead of SmartCM is negligible.\nVII. C ONCLUSION\nWe have presented th",
  " negligible.\nVII. C ONCLUSION\nWe have presented the first technique that addresses cache con-\ntention under thermal constraints in clustered manycores. The means\nare application migration in cooperation with DVFS. The complexity\nof shared cache contention was solved with a small neural network\nthat was trained at design time, and successfully generalized to unseen\nscenarios at runtime. Our technique significantly improves the overallsystem performance compared to the state of the art by up to 45",
  "mance compared to the state of the art by up to 45% and\nby 11% on average, while enforcing thermal safety on the chip. This\nis a major step forward in the field since the overhead is practically\nnegligible and the technique can be directly applied to commercially\navailable clustered manycores without constraints.\nREFERENCES\n[1] (2020) AMD ”Zen 3” Core Architecture. [Online]. Available:\nhttps://www.amd.com/en/technologies/zen-core-3\n[2]Intel 64 and IA-32 Architectures Software Developer’s Manual ",
  "d IA-32 Architectures Software Developer’s Manual , Intel\nCorporation, 2016.\n[3]Intel® Joule™ Developer Kit , Intel Corporation, 2016.\n[4] Y . Kim, A. More, E. Shriver, and T. Rosing, “Application performance\nprediction and optimization under cache allocation technology,” in\nDesign, Automation & Test in Europe (DATE) . IEEE, 2019.\n[5] G. Gracioli, A. Alhammad, R. Mancuso, A. A. Fr ¨ohlich, and R. Pel-\nlizzoni, “A survey on cache management mechanisms for real-time\nembedded systems,” ACM Comput. ",
  "isms for real-time\nembedded systems,” ACM Comput. Surv. , vol. 48, no. 2, nov 2015.\n[6] N. Mishra, J. D. Lafferty, and H. Hoffmann, “ESP: A Machine Learn-\ning Approach to Predicting Application Interference,” in International\nConference on Autonomic Computing (ICAC) , 2017, pp. 125–134.\n[7] T. Marinakis, S. Kundan, and I. Anagnostopoulos, “Meeting Power\nConstraints While Mitigating Contention on Clustered Multiprocessor\nSystem,” IEEE Embedded Systems Letters (ESL) , vol. 12, no. 3, 2019.\n[8] B. ",
  "tems Letters (ESL) , vol. 12, no. 3, 2019.\n[8] B. Pourmohseni, S. Wildermann, F. Smirnov, P. E. Meyer, and J. Teich,\n“Task migration policy for thermal-aware dynamic performance opti-\nmization in many-core systems,” IEEE Access , vol. 10, 2022.\n[9] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC Benchmark\nSuite: Characterization and Architectural Implications,” in Parallel Ar-\nchitectures and Compilation Techniques (PACT) . ACM, 2008.\n[10] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, an",
  "0] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta, “The\nSPLASH-2 Programs: Characterization and Methodological Consider-\nations,” Int. Symp. Computer Architecture (ISCA) , 1995.\n[11] W. Huang, S. Ghosh, S. Velusamy, K. Sankaranarayanan, K. Skadron,\nand M. R. Stan, “HotSpot: A Compact Thermal Modeling Methodology\nfor Early-Stage VLSI Design,” IEEE Trans. Very Large Scale Integration\n(VLSI) Systems , vol. 14, no. 5, pp. 501–513, 2006.\n[12] M. Rapp, M. B. Sikal, H. Khdr, and J. Henkel, “",
  "2] M. Rapp, M. B. Sikal, H. Khdr, and J. Henkel, “SmartBoost: Lightweight\nML-Driven Boosting for Thermally-Constrained Many-Core Proces-\nsors,” in Design Automation Conference (DAC) , 2021.\n[13] A. Kanduri, M.-H. Haghbayan, A. M. Rahmani, M. Shafique, A. Jantsch,\nand P. Liljeberg, “adBoost: Thermal Aware Performance Boosting\nThrough Dark Silicon Patterning,” IEEE Trans. Computers (TC) , 2018.\n[14] X. Wang, A. K. Singh, and S. Wen, “Exploiting dark cores for per-\nformance optimization via pattern",
  "k cores for per-\nformance optimization via patterning for many-core chips in the dark\nsilicon era,” in Int. Symposium on Networks-on-Chip (NOCS) , 2018.\n[15] Y . G. Kim, M. Kim, J. Kong, and S. W. Chung, “An Adaptive Thermal\nManagement Framework for Heterogeneous Multi-Core Processors,”\nIEEE Trans. on Computers (TC) , vol. 69, no. 6, pp. 894–906, 2020.\n[16] L. Subramanian, V . Seshadri, A. Ghosh, S. Khan, and O. Mutlu, “The\nApplication Slowdown Model: Quantifying and Controlling the Impact\nof In",
  "odel: Quantifying and Controlling the Impact\nof Inter-Application Interference at Shared Caches and Main Memory,”\ninInternational Symposium on Microarchitecture (MICRO) , 2015.\n[17] D. G. Feitelson and L. Rudolph, “Metrics and Benchmarking for Parallel\nJob Scheduling,” in Workshop on Job Scheduling Strategies for Parallel\nProcessing . Springer, 1998.\n[18] S. Boyd-Wickizer, H. Chen, R. Chen, Y . Mao, M. F. Kaashoek, R. Morris\net al. , “Corey: An Operating System for Many Cores,” in Symp.\nOperatin",
  "perating System for Many Cores,” in Symp.\nOperating System Design and Implementation (OSDI) , 2008.\n[19] V . Pallipadi and A. Starikovskiy, “The Ondemand Governor,” The Linux\nSymposium , 2006.\n[20] Intel Turbo Boost Technology in Intel Core Microarchitecture (Nehalem)\nBased Processors , Intel Corporation, 2008.\n[21] T. E. Carlson, W. Heirman, and L. Eeckhout, “Sniper: Exploring the\nLevel of Abstraction for Scalable and Accurate Parallel Multi-Core\nSimulation,” in High Performance Computing, Netw",
  "e\nSimulation,” in High Performance Computing, Networking, Storage and\nAnalysis (SC) . ACM, 2011.\n[22] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and\nN. P. Jouppi, “The McPAT Framework for Multicore and Manycore\nArchitectures: Simultaneously Modeling Power, Area, and Timing,”\nACM Trans. Arch. and Code Opt. (TACO) , 2013.\n[23] S. Pagani, H. Khdr, J.-J. Chen, M. Shafique, M. Li, and J. Henkel,\n“Thermal Safe Power (TSP): Efficient Power Budgeting for Heteroge-\nneous Manycore Syst",
  " Power Budgeting for Heteroge-\nneous Manycore Systems in Dark Silicon,” IEEE Trans. Computers (TC) ,\nvol. 66, no. 1, pp. 147–162, 2017.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:51 UTC from IEEE Xplore.  Restrictions apply.",
  "PowerPruning: Selecting Weights and Activations for\nPower-Efficient Neural Network Acceleration\nRichard Petri1, Grace Li Zhang2, Yiran Chen3, Ulf Schlichtmann1, Bing Li1\n1Technical University of Munich,2Technical University of Darmstadt,3Duke University\nEmail: {richard.petri, ulf.schlichtmann, b.li }@tum.de, grace.zhang@tu-darmstadt.de, yiran.chen@duke.edu\nAbstract —Deep neural networks (DNNs) have been successfully applied\nin various fields. A major challenge of deploying DNNs, especially on ed",
  "ajor challenge of deploying DNNs, especially on edge\ndevices, is power consumption, due to the large number of multiply-and-\naccumulate (MAC) operations. To address this challenge, we propose\nPowerPruning, a novel method to reduce power consumption in digital\nneural network accelerators by selecting weights that lead to less power\nconsumption in MAC operations. In addition, the timing characteristics of\nthe selected weights together with all activation transitions are evaluated.\nThe weights and ",
  "vation transitions are evaluated.\nThe weights and activations that lead to small delays are further selected.\nConsequently, the maximum delay of the sensitized circuit paths in the\nMAC units is reduced even without modifying MAC units, which thus\nallows a flexible scaling of supply voltage to reduce power consumption\nfurther. Together with retraining, the proposed method can reduce power\nconsumption of DNNs on hardware by up to 73.9% with only a slight\naccuracy loss.\nI. I NTRODUCTION\nDeep neural",
  "slight\naccuracy loss.\nI. I NTRODUCTION\nDeep neural networks (DNNs) have been successfully applied\nin various fields, e.g., image/speech recognition. In DNNs, a huge\nnumber of multiply-and-accumulate (MAC) operations with weights\nneed to be executed, which correspondingly causes a high power\nconsumption in hardware. This high power consumption poses chal-\nlenges in applying DNNs on power-constrained computing scenarios,\ne.g., plant disease detection in agriculture [1] and medical diagnosis\ndevice",
  "on in agriculture [1] and medical diagnosis\ndevices [2].\nTo overcome the challenge above, various methods on software\nand hardware levels have been explored. On the software level,\npruning has been proposed to reduce the number of weights in\nDNNs and thus power consumption. For example, [3] proposes to\nprune weights with small absolute values to reduce the computation\ncost while maintaining inference accuracy. In addition, structure\npruning [4] is further developed to facilitate the mapping of D",
  "s further developed to facilitate the mapping of DNNs\nonto hardware. Besides pruning, quantization [5] is another major\ncategory of methods to reduce the computation cost of DNNs. With\nquantization, MAC units are implemented to process only integer\ninstead of floating-point arithmetic, thus leading to a significant power\nreduction [6].\nOn the hardware level, various architectures have been proposed to\nexplore how MAC units are organized and how data flow through the\naccelerators to reduce power ",
  "ata flow through the\naccelerators to reduce power consumption. The systolic array from\nGoogle [7], [8] adopts a weight-stationary data flow, where weights\nare stationary and activations and partial sums are moved across the\narray to maximize data reuse. Accordingly, the amount of memory\naccess and thus power consumption can be reduced. In addition,\nthe Eyeriss structure [9] uses a row-stationary data flow where the\nmultiplication of rows of filters and activations is computed in a MAC\narray to r",
  "rs and activations is computed in a MAC\narray to reduce data movement and thus power consumption.The hardware architectures above have also been extended to\nreduce power consumption further. For example, a clock-gating\nscheme is proposed in [10] to disable the operations of unused\nMAC units to reduce dynamic power consumption. In [11], power-\ngating unused processing elements is proposed to reduce leakage\npower in idle hardware units. In addition, an earlystop technique\nin hardware has been prop",
  ", an earlystop technique\nin hardware has been proposed in [12] to skip unnecessary MAC\noperations, though a complex control logic is needed to implement\nthis technique. Furthermore, GreenTPU in [13] scales the supply\nvoltage of the computing logic down to near-threshold levels while\nkeeping a high compute performance. But this method requires\ncomplex control logic to detect timing errors on-the-fly and to track\nactivation sequences that cause timing errors. Similarly, Minerva [14]\nproposes a vol",
  "ing errors. Similarly, Minerva [14]\nproposes a voltage scaling of memory units storing weights while\nexploiting the flexibility of neural networks to tolerate weight errors.\nDifferent from the previous methods, most of which require special\nhardware architecture or control logic, we propose PowerPruning,\na novel method exploiting the power and timing characteristics\nof weights and activations to reduce power consumption without\nmodifying MAC units. PowerPruning is the first technique to evaluate",
  "s. PowerPruning is the first technique to evaluate\nthe power and timing properties of each individual weight value\nand adjust neural networks accordingly. This technique is compatible\nwith the previous methods for power reduction of executing neural\nnetworks and can be integrated with them seamlessly. The key\ncontributions are summarized as follows:\n•The power consumption of weight values is evaluated with respect\nto activations when the MAC operations are executed on hardware.\nAfterwards, weigh",
  "ations are executed on hardware.\nAfterwards, weight values that lead to less power consumption\nin MAC operations are preferred for training neural networks to\nenhance the power efficiency.\n•We consider the actual delays of the MAC operations in hardware\nwith respect to weight values and activations. In training neural\nnetworks, the weight values and activations that sensitize paths\nwith small delays are selected. Correspondingly, the circuit can\nrun faster without modifying MAC units. We then sc",
  "run faster without modifying MAC units. We then scale the supply\nvoltage to reduce the power consumption while maintaining the\noriginal computational performance.\n•Neural networks are retrained by restricting weights and activations\nto the selected values while maximizing the inference accuracy.\nWith the selected weights and activations, power consumption of\nDNNs can be reduced by up to 73.9% with only a slight accuracy\nloss.\nThe rest of the paper is structured as follows. Section II explains\nth",
  "r is structured as follows. Section II explains\nthe motivation of this work. Section III elaborates the details of the\nproposed technique. Experimental results are presented in Section IV\nand conclusions are drawn in Section V. 979-8-3503-2348-1/23/$31.00 © 2023 IEEE2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247868\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 U",
  "rsity. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "II. M OTIVATION\nIn executing DNNs on hardware platforms, the huge number of\nMAC operations may consume much power. Existing methods often\nintroduce hardware modifications, which may incur extra hardware\ncost or make the design specific for individual neural networks. On\nthe contrary, we address this power consumption issue by examining\nthe power and timing properties of the weight values and activations.\nA MAC unit calculates the multiplication of a weight and an\nactivation and adds the result t",
  "f a weight and an\nactivation and adds the result to a partial sum, as illustrated in\nFigure 1. Assume the weight of a neural network is quantized to\nnbits. Correspondingly, there are 2npossible weight values. These\nweight values are one of the inputs to the digital logic implementing\nthe MAC operations. Since different weight values cause different\nsignal switching activities inside the MAC units, they also exhibit\ndifferent average power consumption with respect to the activation\ntransitions an",
  "tion with respect to the activation\ntransitions and partial sum transitions. For example, the weight values\n2n, n= 0,1, . . . , n −2, lead to less power consumption, because the\nmultiplication with these weight values are actually shift operations\nand can thus activate fewer signal propagations in the circuit.\nTo demonstrate the different power consumption of weight values,\nwe evaluated the average power consumption of different weight\nvalues in a MAC unit of a 64×64systolic array. We simulated\n",
  "a MAC unit of a 64×64systolic array. We simulated\nthe execution of LeNet-5 processing 100 pictures randomly selected\nfrom the CIFAR-10 dataset. During simulation, we collected statistics\nof the switching activities of various signals inside the systolic array.\nBased on this data we estimated the average power consumption of\neach weight value using Power Compiler from Synopsys.\nFigure 2 illustrates the average power consumption of the weight\nvalues obtained by the simulation described above. Acco",
  "s obtained by the simulation described above. According to\nthis figure, different weight values can lead to substantially different\naverage power consumption. For example, the quantized weight\nvalue -105 has a large average power consumption 1,066 µW, while\nthe quantized weight value -2 has only 596 µW.According to this\nobservation, by restricting neural networks to prefer the weight\nvalues with small average power consumption, the overall power\nconsumption of executing neural networks can be lo",
  "consumption of executing neural networks can be lowered.\nBesides different power characteristics, different weights also ex-\nhibit different timing profiles in a MAC unit. Inside a MAC unit\nshown in Figure 1, there are many combinational paths, which have\ndifferent delays and are triggered by specific input data, i.e., weight,\nactivation, and partial sum. If the weight is fixed to a given value,\nsome combinational paths in the MAC unit cannot be sensitized.\nAccordingly, the delay of the MAC unit",
  "sensitized.\nAccordingly, the delay of the MAC unit may differ with respect to\ndifferent weight values. To demonstrate this difference, we conducted\ntiming analysis of the MAC unit with fixed weight values and all\nactivation transitions using Modelsim.\nFigure 3 illustrates the delay profiles of two quantized weight\nvalues -105 and 64, where the x-axis shows the delay and the y-\naxis shows the frequency of this delay appearing with respect to all\npossible activation transitions. Figure 3 confirms ",
  "ossible activation transitions. Figure 3 confirms that different weight\nvalues lead to different delays. In addition, it shows that the delays\ncan be reduced further if some activations can be pruned from the\nneural network, e.g., the activation transitions triggering delays on the\nfar right end of the x-axis. Since the clock period of a circuit is deter-\nmined by the maximum delay of all the combinational paths, the clock\nfrequency of the MAC unit and thus the computational performance\ncan be i",
  "it and thus the computational performance\ncan be increased by pruning weights and activations according to\ntheir timing profiles. Alternatively, the supply voltage can be lowered\nActivation\nWeight determined\nby training and \nﬁxed during power \nand delay evaluationPartial sumInput-driven, transitions \ncause power consumptionCombined transition of\nactivation and partial sum:\nMAC unitFig. 1: Power and delay charac-\nterization of MAC unit.\n900 µW thresholdPower Consumption [µW]\n1000\n800\n600\n400\nQuan",
  "esholdPower Consumption [µW]\n1000\n800\n600\n400\nQuantized Weight Values\n−127\n127\n−100\n−75\n−50\n−25\n0\n25\n50\n75\n100Fig. 2: Average power consump-\ntion of quantized weight values.\nQuantized weight value 64,\n  maximum delay: 134 ps\nFrequency25005000\nDelay [ps]25 50 75 100 125 150 175Quantized weight value -105,\n   maximum delay: 179 ps \nFrequency250500\nDelay [ps]25 50 75 100 125 150 175\nFig. 3: Delay profiles of a MAC unit for two quantized weight values.\nThe arrows point to the maximum delay of a give",
  "s.\nThe arrows point to the maximum delay of a given weight value with\nrespect to all the activation transitions.\nto reduce power consumption further, while maintaining the original\nclock frequency.\nIII. W EIGHT AND ACTIVATION SELECTION FOR\nPOWER -EFFICIENT NEURAL NETWORK ACCELERATION\nIn this section, we introduce the proposed PowerPruning method to\nreduce power consumption in digital neural network accelerators. The\nweight selection according to the average power consumption is first\nexplained i",
  "the average power consumption is first\nexplained in Section III-A. Afterwards, the selection of weights and\nactivations with respect to their timing characteristics is explained in\nSection III-B. The retraining of neural networks by restricting weights\nand activations to the selected values to reduce power consumption\nis described in Section III-C.\nA. Weight selection according to power consumption\nAs shown in Figure 2, different weights in a MAC unit lead\nto different average power consumption.",
  " unit lead\nto different average power consumption. To take advantage of this\ncharacteristic to reduce power consumption of DNN accelerators, the\naverage power consumption of all the 8-bit integer weight values in\na MAC unit should be evaluated. To do this, the input of the MAC\nunit corresponding to the weight is fixed to a given value, as shown\nin Figure 1. The various combinations of activation transitions and\npartial sum transitions are fed into the other inputs of the MAC unit\nto obtain the s",
  "o the other inputs of the MAC unit\nto obtain the switching activities of the MAC unit. Based on these\nswitching activities the power consumption for the fixed weight value\ncan be evaluated using Power Compiler from Synopsys.\nTwo challenges in evaluating the average power consumption of a\nweight should be addressed. First, the number of combined transitions\nof activations and partial sums is huge, e.g., 2(8+22) ×2= 260≈1018,\nwhen the activations and the partial sums are quantized to 8 and\n22 bits",
  "nd the partial sums are quantized to 8 and\n22 bits, respectively, for a 64×64systolic array. ×2is due to\nthe fact that the power consumption is caused by the transitions\nfrom a combination of activation and partial sum to another com-\nbination, instead of the static values of the activation and partial\n2\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "Transition From\n127\n-128\nTransition To\n127(a)\nTransition From\n50\n0\nTransition To\n50 (b)\nFig. 4: Transition distributions of activations and partial sums of a\nMAC unit. (a) Activation transition distribution. (b) Partial sum transition\ndistribution.\nsum. Accordingly, simulating all these transitions to identify the\npower consumption is very time-consuming. Second, just sampling\nall possible combined transitions of activations and partial sums does\nnot reflect the probabilities of such transitions",
  "\nnot reflect the probabilities of such transitions when executing neural\nnetworks in a systolic array. For example, a combined transition\nmay appear more frequently than other transitions, so that it should\ncontribute more to the result of power evaluation than others.\nTo deal with these challenges, we first identify the transition\ndistributions for activations and partial sums with real data executing\non the systolic array, described as follows. In addition, we partition\nthe value range of the ",
  " In addition, we partition\nthe value range of the partial sum into a small number of bins to\nreduce the partial sum transition space and then evaluate the transition\nprobability from one bin to another bin.\n1) Evaluation of activation transition distribution: For the 8-\nbit activation as an input to a MAC unit, the total number of\npossible transitions is 28×2= 216. To obtain the activation transition\ndistribution, we simulate the activities of a systolic array and count\nthe frequency of each ind",
  "systolic array and count\nthe frequency of each individual transition. For example, for LeNet-5\non CIFAR10, we randomly select 100 pictures and execute the neural\nnetwork on the systolic array. In total we counted approximately 1017\nactivation transitions. Since this number is larger than the number of\npossible transitions 216, the result will well exhibit the distribution\nof the activation transitions.\nFigure 4(a) shows the resulting activation transition distribution,\nwhere darker colors repres",
  "ransition distribution,\nwhere darker colors represent a lower probability and brighter colors\na higher probability. In this figure, the bright diagonal line clearly\nindicates that most transitions appear between activations with similar\nvalues, while activation transitions from very high to very low values\nand vice versa are very unlikely to happen.\n2) Evaluation of partial sum transition distribution and transition\nspace reduction: A partial sum has 22 bits in a systolic array with\nthe size of ",
  " has 22 bits in a systolic array with\nthe size of 64×64, which results in 222×2= 244≈1.8×1013\npossible transitions. If we would simulate 100 pictures on the systolic\narray, we can obtain approximately 2.2×108partial sum transitions,\nwhich is much smaller than the number of possible transitions and\ncannot produce a trustworthy distribution. Increasing the number of\npictures in simulation is not a viable solution due to runtime. To\nsolve this problem, we partition the value range of the partial su",
  "em, we partition the value range of the partial sum\ninto a small number of bins. Accordingly, instead of evaluating the\ntransition probability of individual partial sum values, we evaluate\nthe transition probability from one bin to another bin.\nTo partition all partial sums into a small number of bins, the\nswitching activities of partial sums from one bin to another bin should\nbe maintained as similar as possible. To achieve this goal, we grouppartial sums according to the similarity in their bi",
  "rtial sums according to the similarity in their bit numbers. Specif-\nically, a predetermined number of bins is first specified. Afterwards,\npartial sums are randomly selected and assigned into each bin. The\nremaining partial sums are iteratively assigned into the most matching\nbins by measuring the similarity between a specific partial sum and a\nbin. This similarity is evaluated by counting the number of different\nbits between this partial sum and those in the bins. The partial sum\nis assigned t",
  "d those in the bins. The partial sum\nis assigned to the bin where the partial sum has the least number of\ndifferent bits in average. In the experiments, 50 bins were used.\nAfter the partition of partial sums into bins, we simulated 100\npictures and assigned the real transitions into these bins. Afterwards,\nthe probabilities of the transitions between bins can be identified,\nsimilar to the evaluation of the activation transition distribution\nin Section III-A1. Figure 4(b) shows the partial sum tr",
  "ction III-A1. Figure 4(b) shows the partial sum transition\ndistribution of the bins. It can be observed that the partial sum\ntransitions are not evenly distributed. For example, the bright diagonal\nline from the upper left corner to the lower right corner indicates that\nthere are many transitions between partial sums with similar values.\nIn addition, the bright vertical and horizontal lines also demonstrate\nintensive partial sum transitions.\n3) Weight selection: With the distributions identified",
  "eight selection: With the distributions identified above, we\nsample 10,000 transitions of both activations and partial sums ac-\ncording to their probabilities. The combined transitions are used to\nsimulate the activities of the MAC unit with the weight input fixed\nto specific values. The resulting switching activities are then used to\ncalculate the average power consumption of the MAC unit for this\nweight. This simulation is repeated for each individual weight value\nand the result is shown in Fi",
  "ividual weight value\nand the result is shown in Figure 2, where the power consumption\nof each weight varies greatly. In this result, there is also a trend that\nweights close to zero have especially low power consumption, with\nweight zero having by far the lowest.\nBased on the result of power analysis we first conduct conventional\npruning to maximize the number of weights with zero value to reduce\npower consumption. Afterwards, we select weight values that lead to\nsmall power consumption by setti",
  "lues that lead to\nsmall power consumption by setting a power threshold, e.g., 900 µW\nin Figure 2. By setting the threshold lower, we can achieve potentially\nmore power savings by excluding more high-power weight values.\nHowever, the accuracy of the DNN may degrade. Therefore, a tradeoff\nbetween power saving and inference accuracy should be made.\nB. Weight and activation selection according to timing profiles\nAccording to Figure 3, weight values exhibit different timing char-\nacteristics. Even fo",
  "xhibit different timing char-\nacteristics. Even for the same weight, different activation transitions\nlead to different delays. To identify weight values and activations\nwith small delays, the timing of each weight value with respect to\nactivation transitions and partial sum transitions in the MAC unit\nshould be analyzed. Two types of timing analysis methods, dynamic\ntiming analysis and static timing analysis, are available for this\ntask. The former is conducted by applying input transitions int",
  "mer is conducted by applying input transitions into a\ncircuit and evaluating the delays of correspondingly triggered paths,\nwhile the latter evaluates the delay statically without considering the\ncorresponding triggered paths. The latter is conservative since the\ndelays of some paths that are not activated are also included and the\nclock frequency of the circuit may be unnecessarily lowered.\nTo evaluate the timing profile of a weight value, an intuitive idea\nis to fix the weight input into the M",
  "tuitive idea\nis to fix the weight input into the MAC unit and then apply dynamic\ntiming analysis with all the transition patterns of activations and\npartial sums to simulate the unit. The challenge of this method is\n3\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "WeightProduct[0]\nMultiplierMAC Unit\nAdderResultPartial Sum\nProduct[1]\nProduct[2]\nProduct[3]5 4 6\n8 3\n0 2\n0 1 Activation1\n1➞2Fig. 5: Concept of timing analysis of the MAC unit.\nFig. 6: Concept of weight and activation selection for delay reduction.\nthat the number of combined transitions of activations and partial\nsums is huge, as described in Section III-A. Simulating the delay of\nthe MAC unit with respect to all these combinations is thus time-\nconsuming.\nTo reduce the runtime of timing analysi",
  "consuming.\nTo reduce the runtime of timing analysis, we separate the timing\nanalysis of the multiplier and adder in the MAC unit. Specifically, we\napply static timing analysis on the adder to avoid the consideration\nof input transitions, because the number of inputs to the adder is very\nlarge. On the other hand, the multiplier is evaluated using accurate\ndynamic timing analysis, since the delay of the multiplier usually\ndominates the delay of the MAC unit and this delay can be lowered\nby filteri",
  " MAC unit and this delay can be lowered\nby filtering out some weight values and activation values.\nTo conduct dynamic timing analysis of the multiplier for a weight\nvalue, we simulate the multiplier by fixing the weight input and\nenumerating the 28×2possible transitions of the activations. Static\ntiming analysis of the adder is conducted by the built-in timing\nanalyzer in Design Compiler from Synopsys. To incorporate the\nrelation between the timing paths in the multiplier and the adder,\nwe evalu",
  "ng paths in the multiplier and the adder,\nwe evaluate the largest delay starting from each individual bit of the\nproduct to the output of the adder. Afterwards, the largest delay of\nthe MAC unit with respect to the given weight value is calculated by\nadding the delays from the input activation to the output bits of the\nmultiplier and the delays from the corresonding product bits to the\noutput of the adder.\nFigure 5 illustrates the concept of timing analysis of the MAC\nunit, where the quantized w",
  "ng analysis of the MAC\nunit, where the quantized weight 1, the activation transition from\nquantized 1 to quantized 2, and four product bits at the output of the\nmultiplier are used as example. With the dynamic timing analysis\napplied on the multiplier, the delays from the input activation to\nProduct[0] and Product[1] are 5 and 8, respectively. The delays to the\nother two product bits can be 0 if the combinational paths to them are\nnot activated by the activation transition. With static timing an",
  "y the activation transition. With static timing analysis\napplied on the adder, the delays from the output bits of the multiplier\nto the output of the adder are 4, 3, 2, 1, respectively. Assume the\ndelay from the partial sum to the output of the adder is 6 returned\nby static timing analysis. The largest delay of the MAC unit is thus\nmax{5 + 4 ,8 + 3 ,6}= 11 .\nThe timing analysis method described above is applied for each\nweight individually. After that, all the delays of weights with respect\nto a",
  " that, all the delays of weights with respect\nto activation transitions can be obtained. To select weights andactivations with small delays, we first set a delay threshold and\niteratively remove weights or activations that lead to delays larger\nthan the given threshold. The iterations end until all the delays of the\nremaining weights and activations are smaller than the given delay\nthreshold. Figure 6 illustrates an example with the delay threshold\nset to 90. In the first step, we find the large",
  "ld\nset to 90. In the first step, we find the largest delay 99. Since 99 is\nlarger than the specified threshold, we have to remove either w1,a5\nora8to exclude the correponding combination. Since the removal\nof either w1,a5ora8also affects other combinations in Figure 6,\nit is difficult to find the optimal sequence to remove the weigths\nand activaitons. Accordingly, we randomly remove any of them and\nthen remove the other combinations containing the removed weight\nor activation in Figure 6. For ex",
  "e removed weight\nor activation in Figure 6. For example, removing w1also leads to the\nremoval of the first combination in Figure 6. To avoid local optimum,\nwe execute this process several times and choose which weight or\nactivation to remove in each step randomly. The removal process ends\nwhen the maximum delay of all the combinations is lower than the\ngiven threshold 90. The result is a set of weights and activations that\nsatisfy the delay requirements. While pruned weight values can be\navoided",
  "rements. While pruned weight values can be\navoided during training of neural networks, the filtering of activations\nneeds to be integrated into the activation function after each layer.\nC. Neural network training for power reduction\nTo reduce power consumption of DNNs on hardware, we first\napply conventional pruning to remove weights whose absolute values\nare close to zero. Afterwards, we select weights that lead to small\npower consumption by setting a power threshold. The initial power\nthreshol",
  "ting a power threshold. The initial power\nthreshold is 900 µW and it is iteratively reduced to select weights.\nIn each iteration, the neural networks are retrained with the selected\nweights to verify the inference accuracy. During retraining, we force\nthe weights to take the restricted values in the forward propagation.\nIn the backward propagation, the straight through estimator [15] is\nadopted to skip the restriction operation. The iterations end when the\ninference accuracy starts to drop notic",
  "d when the\ninference accuracy starts to drop noticeably.\nAfter the power threshold is determined, we then select weight\nvalues and activations that lead to small delays by setting a delay\nthreshold. The initial delay threshold is 170 psand the delay threshold\nis iteratively reduced by 10 psto select weight values and activations.\nIn each iteration, the neural networks are retrained and verified. When\nthe inference accuracy drops by around 5% of the original inference\naccuracy of the neural netwo",
  "he original inference\naccuracy of the neural networks, the best training result is returned.\nWhen executing the neural networks, if the original clock fre-\nquency should be maintained, we can lower the supply voltage to\nreduce power reduction. We use the results in [16] to determine the\nrelation between supply voltage and the delay of the circuit. The\nscaling of dynamic power consumption and leakage is conducted\naccording to [17].\nIV. E XPERIMENTAL RESULTS\nTo verify the proposed method, we teste",
  "AL RESULTS\nTo verify the proposed method, we tested four different neural\nnetwork and dataset combinations, as shown in the first column of\nTable I. The weights and activations were quantized to eight bits.\nThe neural networks were trained using Tensorflow while considering\nquantization [5]. In Tensorflow, the number of 8-bit weights is 255\ninstead of 256 to maintain the weight distribution symmetrical while\nthe number of 8-bit activations is 256. After training, small weights\nwere pruned to com",
  ". After training, small weights\nwere pruned to compress the neural network. We then applied the\nproposed method to reduce the power consumption. For LeNet-5,\n4\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "TABLE I: Experimental results of proposed method.\nTotal Power Consumption [mW]\nAccuracy Standard HW Optimized HW #Selected Max Delay\nRed.V oltage Scaling\nFactor V SHW V OHW Network-Dataset Orig. Prop. Orig. Prop. Red. Orig. Prop. Red. Wei. Act.\nLeNet-5-CIFAR-10 80.7% 78.4% 281.6 152.1 46.0% 280.4 73.1 73.9% 32 176 40 ps 0.71/0.8 13.7% 6.4%\nResNet-20-CIFAR-10 91.9% 88.9% 469.9 230.6 50.9% 427.7 173.4 59.4% 32 176 40 ps 0.71/0.8 12.7% 10.6%\nResNet-50-CIFAR-100 79.9% 78.4% 509.1 278.7 45.3% 510.8 1",
  "50-CIFAR-100 79.9% 78.4% 509.1 278.7 45.3% 510.8 140.8 72.4% 40 220 30 ps 0.73/0.8 10.6% 5.2%\nEfficientNet-B0-Lite-ImageNet 74.4% 72.9% 152.0 106.7 29.8% 134.2 78.5 41.5% 76 244 20 ps 0.75/0.8 8.8% 8.0%\nLeNet-5-CIFAR-10Accuracy (%)\n50\n55\n60\n65\n70\n75\n80Power Consumption [mW]\n0\n50\n100\n150\n200\n250\n300\nBaseline\nPruned\nProposed\nResNet-20-CIFAR-10Accuracy (%)\n50\n60\n70\n80\n90Power Consumption [mW]\n0\n100\n200\n300\n400\n500\nBaseline\nPruned\nProposed\nLeakage Power\nDynamic Power\nAccuracy\nResNet-50-CIFAR-100Accu",
  "wer\nDynamic Power\nAccuracy\nResNet-50-CIFAR-100Accuracy (%)\n50\n55\n60\n65\n70\n75\n80Power Consumption [mW]\n0\n100\n200\n300\n400\n500\n600\nBaseline\nPruned\nProposed\nEfficientNet-B0-Lite-ImageNetAccuracy (%)\n50\n55\n60\n65\n70\n75Power Consumption [mW]\n0\n25\n50\n75\n100\n125\n150\nBaseline\nPruned\nProposed\nFig. 7: Comparison with conventional pruning, evaluated on Optimized\nHW.\nResNet-20 and ResNet-50, Nvidia Quadro RTX 6000 GPU 24 GB\nwas used for training, and for EfficientNet-B0-Lite Nvidia A100 80\nGB GPU was used. Th",
  "ientNet-B0-Lite Nvidia A100 80\nGB GPU was used. The number of times to execute the selection of\nweight and activation with small delays in Section III-B is set to 20\nin the experiments.\nTo demonstrate the effectiveness of the proposed method on differ-\nent types of accelerators, two different hardware implementations of\nsystolic array were evaluated. In the optimized hardware architecture\n(Optimized HW), clock gating of a MAC unit in case of a zero weight\nto reduce dynamic power consumption and ",
  "ro weight\nto reduce dynamic power consumption and power gating of whole\nunutilized columns in the systolic array to reduce both the dynamic\nand static power consumption are applied. In the standard architecture\n(Standard HW), none of these power-saving features were applied.\nThe power consumption during inference was estimated with\nPower Compiler by simulating the systolic array executing the neural\nnetworks using Modelsim. The simulation was conducted using a\nnetlist description of the systolic",
  "ducted using a\nnetlist description of the systolic array synthesized with the NanGate\n15 nm cell libraries [18] and the clock frequency around 5 GHz.\nSince cycle-accurate simulations are extremely time-consuming, for\nResNet-20, ResNet-50 and EfficientNet-B0-Lite only the convolu-\ntional layers with the largest number of MAC operations were\nsimulated and compared.\nTable I summarizes the experimental results of the proposed\nmethod. The original accuracy and the accuracy with our method\nare shown i",
  "uracy and the accuracy with our method\nare shown in the second and third columns. According to these two\ncolumns, the accuracy degradation is relatively small, With a slight\naccuracy loss, a significant reduction in power consumption up to73.9% can be achieved, as shown in the sixth and ninth columns,\ndemonstrating the effectiveness of the proposed method in enhancing\npower efficiency of digital accelerators for neural networks. This is\nespecially useful for edge devices where power consumption ",
  "y useful for edge devices where power consumption is a\nmajor issue.\nWhen executing the neural networks on Standard HW, the total\npower including dynamic and leakage power was reduced by up to\n50.9% (sixth column). On Optimized HW, the power saving was\neven greater with a power reduction up to 73.9% (ninth column).\nThe relatively smaller power reduction on Standard HW was caused\nby the leakage power consumption from the MAC units that were\nnot gated even when they are not used.\nTo reduce the maxi",
  "ed even when they are not used.\nTo reduce the maximum delay of a MAC unit, which was 180 ps\nafter synthesis, we only selected a subset of weight values and\nactivations that lead to small delays of the MAC operations. The\nnumber of selected weight values and selected activation values are\nshown in the tenth column (Wei.) and the eleventh column (Act.).\nAccording to the tenth column, the number of selected weight values\nis reduced significantly, e.g., from 255 to 32 in LeNet5 and ResNet-\n20. On th",
  "g., from 255 to 32 in LeNet5 and ResNet-\n20. On the contrary, most activation values still remain to maintain a\ngood inference accuracy. The delay reduction due to the weight and\nactivation selection is shown in the twelfth column (Max Delay Red.).\nIn identifying delay reduction, our search granularity was 10 ps. This\ncan be lowered if necessary, but at the expense of more runtime.\nTo reduce power consumption further, the supply voltage is\nlowered by the ratio shown in thirteenth column (V oltag",
  "d by the ratio shown in thirteenth column (V oltage Scaling\nFactor). For example, for LeNet-5-CIFAR-10, the supply voltage was\nreduced from 0.8 V to 0.71 V while still maintaining the original\nclock frequency. The relation between supply voltage scaling and\ncircuit delay was evaluated according to the simulation results in\n[16]. The last two columns show the percentage of power reduction\ncontributed by voltage scaling. For Standard HW (column V SHW)\nand Optimized HW (column V OHW), voltage scali",
  "HW)\nand Optimized HW (column V OHW), voltage scaling can reduce\npower consumption by up to 13.7% and 10.6%, respectively.\nTo demonstrate the advantage of the proposed method over conven-\ntional pruning, we show the comparison of the power consumption\nand the inference accuracy of conventional pruning and the proposed\nmethod in Figure 7. According to this comparison, the proposed\nmethod can significantly reduce the power consumption of a pruned\nneural network further with only a slight accuracy l",
  "ural network further with only a slight accuracy loss.\nTo demonstrate the tradeoff between the number of selected weight\nvalues and the inference accuracy, we used different thresholds to\nselect weight values according to their power consumption and\nevaluated the accuracy by restricting the neural networks to these\nweight values. Figure 8 illustrates the results. As expected, a lower\npower threshold leads to a lower inference accuracy. However, there is\nstill a good potential for power reduction",
  "here is\nstill a good potential for power reduction before significant accuracy\ndegradation appears. For example, for ResNet-50-CIFAR-100 the\n5\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "LeNet-5-CIFAR-10Accuracy (%)\n50\n55\n60\n65\n70\n75\n80Power Consumption [mW]\n0\n25\n50\n75\n100\n125\nPower Threshold [µW]/\nNumber of Weight Values\nNone\n255\n900\n86\n850\n61\n825\n48\n800\n36\nResNet-20-CIFAR-10Accuracy (%)\n50\n60\n70\n80\n90Power Consumption [mW]\n0\n50\n100\n150\n200\n250\n300\nPower Threshold [µW]/\nNumber of Weight Values\nNone\n255\n900\n86\n850\n61\n825\n48\n800\n36\nResNet-50-CIFAR-100Accuracy (%)\n50\n55\n60\n65\n70\n75\n80Power Consumption [mW]\n0\n50\n100\n150\n200\n250\nPower Threshold [µw]/\nNumber of Weight Values\nNone\n255",
  "r Threshold [µw]/\nNumber of Weight Values\nNone\n255\n900\n86\n850\n61\n825\n48\n800\n36\nLeakage Power\nDynamic Power\nAccuracy\nEfficientNet-B0-Lite-ImageNetAccuracy (%)\n50\n55\n60\n65\n70\n75Power Consumption [mW]\n0\n25\n50\n75\n100\n125\nPower Threshold [µW]/\nNumber of Weight Values\nNone\n255\n900\n86\n850\n61\n825\n48\n Fig. 8: Tradeoff between accuracy and the number of selected weight\nvalues, evaluated on Optimized HW.\nLeNet-5-CIFAR-10\nResNet-20-CIFAR-10\nResNet-50-CIFAR-100\nEfficientNet-\nB0-Lite-ImageNetAccuracy (%)\n50\n6",
  "00\nEfficientNet-\nB0-Lite-ImageNetAccuracy (%)\n50\n60\n70\n80\n90\nMaximum Delay [ps]/\nNumber of Activation Values (EfficientNet)\n180\n256\n170\n234(238)\n160\n221(225)\n150\n179(178)\n140\n73\nFig. 9: Tradeoff between accuracy and the number of selected activation\nvalues.\npower threshold can be lowered down to 800 µW, which corresponds\nto 36 weight values, leading to total power savings of 22.8% with\nonly a negligible accuracy loss.\nFigure 9 shows the tradeoff between accuracy and the number of\nactivation valu",
  "between accuracy and the number of\nactivation values. The results are obtained by restricting neural net-\nworks with different numbers of activation values based on a weight\nselection threshold 825 µW for LeNet5, ResNet-20 and ResNet-50,\nand a higher threshold 900 µW for EfficientNet-B0-Lite. The different\nnumbers of activation values reflect different maximum delays on\nthe MAC unit. In this figure, the left most point corresponds to\nthe full activation space with 256 activation values. As the n",
  "ivation space with 256 activation values. As the number\nof activation values and thus the maximum delay decreases, the\ninference accuracy is first well-maintained and then drops. Before the\nturning point, there is optimization potential we took advantage of\nto enhance computational performance or reduce power consumption\nby voltage scaling.\nV. C ONCLUSION\nIn this paper, we have proposed PowerPruning, a novel method\nto reduce power consumption in digital neural network accelerators\nby selecting w",
  "digital neural network accelerators\nby selecting weights that lead to less power consumption in MACoperations. The timing characteristics of the selected weights together\nwith activation transitions are also evaluated. We then selected\nweights and activations that lead to small delays, so that either the\nclock frequency of the MAC units can be improved or voltage scaling\ncan be applied to reduce power consumption further. Together with\nretraining, the proposed method can reduce power consumption",
  ", the proposed method can reduce power consumption of\nDNNs on hardware by up to 73.9% with only a slight accuracy\nloss. The proposed method does not modify MAC units and can be\ncombined seamlessly with existing hardware architectures for power-\nefficient neural network acceleration.\nACKNOWLEDGMENT\nThis work is funded by the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) – Project-ID 504518248 and\nby the National Science Foundation (NSF) 2112562.\nREFERENCES\n[1] W.-L. Chen et al",
  "ion (NSF) 2112562.\nREFERENCES\n[1] W.-L. Chen et al. , “RiceTalk: Rice blast detection using internet of things\nand artificial intelligence technologies,” IEEE Internet of Things Journal ,\nvol. 7, no. 2, 2020.\n[2] S. Hassantabar et al. , “MHDeep: Mental health disorder detection system\nbased on wearable sensors and artificial neural networks,” ACM Trans.\nEmbed. Comput. Syst. , 2022.\n[3] S. Han et al. , “Deep compression: Compressing deep neural network\nwith pruning, trained quantization and huffm",
  "twork\nwith pruning, trained quantization and huffman coding,” in International\nConference on Learning Representations (ICLR) , 2016.\n[4] W. Wen et al. , “Learning structured sparsity in deep neural networks,”\ninInternational Conference on Neural Information Processing Systems\n(NIPS) , 2016.\n[5] B. Jacob et al. , “Quantization and training of neural networks for\nefficient integer-arithmetic-only inference,” in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2018.\n[6] A. Gh",
  "n and Pattern Recognition (CVPR) , 2018.\n[6] A. Gholami et al. , “A survey of quantization methods for efficient neural\nnetwork inference,” CoRR , 2021.\n[7] N. P. Jouppi et al. , “In-datacenter performance analysis of a tensor\nprocessing unit,” in International Symposium on Computer Architecture\n(ISCA) , 2017.\n[8] “Edge TPU,” https://cloud.google.com/edge-tpu.\n[9] Y .-H. Chen et al. , “Eyeriss: An energy-efficient reconfigurable accelera-\ntor for deep convolutional neural networks,” IEEE Journal",
  " deep convolutional neural networks,” IEEE Journal of Solid-State\nCircuits (ISSCC) , vol. 52, no. 1, 2017.\n[10] N. D. Gundi et al. , “EFFORT: Enhancing energy efficiency and error\nresilience of a near-threshold tensor processing unit,” in Asia and South\nPacific Design Automation Conference (ASP-DAC) , 2020.\n[11] P. Pandey et al. , “UPTPU: Improving energy efficiency of a tensor pro-\ncessing unit through underutilization based power-gating,” in ACM/IEEE\nDesign Automation Conference (DAC) , 2021.\n",
  "M/IEEE\nDesign Automation Conference (DAC) , 2021.\n[12] V . Akhlaghi et al. , “SnaPEA: Predictive early activation for reducing\ncomputation in deep convolutional neural networks,” International Sym-\nposium on Computer Architecture (ISCA) , 2018.\n[13] P. Pandey et al. , “GreenTPU: Improving timing error resilience of a\nnear-threshold tensor processing unit,” in ACM/IEEE Design Automation\nConference (DAC) , 2019.\n[14] B. Reagen et al. , “Minerva: Enabling low-power, highly-accurate deep\nneural netw",
  "abling low-power, highly-accurate deep\nneural network accelerators,” in ACM/IEEE International Symposium on\nComputer Architecture (ISCA) , 2016.\n[15] Y . Bengio et al. , “Estimating or propagating gradients through stochastic\nneurons for conditional computation,” ArXiv , 2013.\n[16] W. Lee et al. , “Dynamic thermal management for FinFET-based\ncircuits exploiting the temperature effect inversion phenomenon,” in\nIEEE/ACM International Symposium on Low Power Electronics and\nDesign (ISLPED) , 2014.\n[",
  "ow Power Electronics and\nDesign (ISLPED) , 2014.\n[17] N. Pinckney et al. , “Impact of FinFET on near-threshold voltage\nscalability,” IEEE Design & Test , vol. 34, no. 2, 2017.\n[18] “15nm Open-Cell library and 45nm freePDK,” https://si2.org/open-cell-\nlibrary/.\n6\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:55 UTC from IEEE Xplore.  Restrictions apply.",
  "PROPHET: Pr edictive O n-Chip P ower Meter in\nHardware Accelerat or for DNN\nJian Peng\nHong Kong University of\nScience and Technology\njpengai@connect.ust.hkTingyuan Liang\nHong Kong University of\nScience and Technology\ntliang@connect.ust.hkZhiyao Xie\nHong Kong University of\nScience and Technology\neezhiyao@ust.hkWei Zhang\nHong Kong University of\nScience and Technology\nwei.zhang@ust.hk\nAbstract —On-chip power meters play a critical role in power\nmanagement by generating timely and accurate power tra",
  "gement by generating timely and accurate power traces at\nruntime. However, both performance-counter-based and existing\nRTL-based on-chip power meters have difficulty in providing\nsufficient response time for fast power and voltage management\nscenarios. Additionally, they can be costly to implement for\nlarge-scale DNN accelerators with many homogeneous pro-\ncess elements. To address these limitations, this paper pro-\nposes PROPHET, a data-pattern-based predictive on-chip power\nmeter targeting mul",
  "based predictive on-chip power\nmeter targeting multiply-accumulate-based DNN accelerators.\nBy sampling pre-defined data patterns during memory access,\nPROPHET can predict power consumption before it actually hap-\npens. In our experiments, PROPHET predicts power consump-\ntion dozens of clock cycles in advance, with a temporal resolution\nof 4 clock cycles and NMAE <7%and area overhead <2%\nfor various systolic-array-based DNN accelerators. PROPHET\nhas the potential to enable fine-grained power mana",
  "as the potential to enable fine-grained power management and\noptimization for large-scale DNN accelerators, improving their\nenergy efficiency.\nIndex Terms —pattern-based, power prediction, on-chip power\nmeter, DNN accelerator\nI. I NTRODUCTION\nEfficient power management is indispensable for attaining\nhigh energy efficiency and guaranteeing the system stability\nof modern hardware design. In power management, on-chip\npower meters (OPMs) play a critical role by providing accurate\nand timely power tr",
  "cal role by providing accurate\nand timely power traces. However, the specific requirements\nfor OPMs may vary, depending on the intended application.\nFor example, dynamic voltage and frequency scaling (DVFS),\nwhich is managed by the system firmware and/or operating\nsystem (OS), only requires coarse-grained temporal resolution\nin power tracing. In contrast, techniques for fast power man-\nagement, voltage boosting, and voltage drop mitigation require\nfine-grained temporal resolution and short respo",
  "e\nfine-grained temporal resolution and short response time. For\nexample, dynamic LdI/dt voltage noise effects can develop\nwithin 20 nanoseconds in modern computing architectures [1].\nPrevious studies have presented power models using per-\nformance counters for coarse-grained power tracing, target-\ning DVFS and temperature-suitability management. Recently,\nsome automatic frameworks have been proposed to construct\nRTL-based OPMs with low overhead and fine-grained tempo-\nral resolution. For instanc",
  "nd fine-grained tempo-\nral resolution. For instance, the PowerProbe proposed in [2]\ncan achieve temporal resolution at the level of tens of cycles.\nSimmani [3] and APOLLO [1] can further achieve per-cycle\nresolution for microprocessors. However, these RTL-based\nOPMs can be costly to implement on DNN accelerators, asthey will select proxies from all the homogeneous processing\nelements, some of which are repeated.\nMoreover, even with per-cycle temporal resolution, OPMs\nmay still not leave sufficie",
  "oral resolution, OPMs\nmay still not leave sufficient response time for timely power\nmanagement in some scenarios. For example, for voltage\nemergency mitigation in single-core CPUs, unsuppressed volt-\nage emergencies dramatically increase when the feedback loop\ndelay is greater than one cycle [4]. Therefore, proactive voltage\nmanagement based on architectural events has been proposed\nfor voltage noise smoothing and voltage emergency mitigation\n[4], [5]. However, these prediction models can only p",
  ", [5]. However, these prediction models can only predict\nwhether an emergency will occur, rather than providing an\naccurate power trace that enables more detailed guidance to\nproactive management strategies.\nTo bridge this gap, this paper proposes a novel data-\npattern-based OPM for float-point DNN accelerators named\nPROPHET. We identified some patterns with strong corre-\nlation with power consumption. By sampling them during\nmemory access, we successfully predict upcoming power\nconsumption doze",
  "ccessfully predict upcoming power\nconsumption dozens of clock cycles in advance. Compared\nwith previous works that select proxies from RTL signals or\nperformance counters, our proposed power model can achieve\naccurate power prediction with low overhead. Experiments on\nsystolic-array-based DNN accelerators have demonstrated our\nmodel’s accuracy and fine-grained prediction capabilities. Our\ncontributions are summarized below.\n•We propose PROPHET, a fine-grained predictive power\nmodel for DNN accel",
  " fine-grained predictive power\nmodel for DNN accelerators that enables improved power\nmanagement and increased energy efficiency. To our\nbest knowledge, PROPHET is the first predictive power\nmodel for DNN accelerators. PROPHET achieves power\nprediction in advance by sampling target data patterns\nduring memory access.\n•The target data patterns captured by PROPHET can\naccurately reflect the power consumption of multiply-\naccumulate-based DNN accelerators. This observation\nalso applies to other DNN",
  "rators. This observation\nalso applies to other DNN accelerators with a regular\nPE array and the multiply-accumulate (MAC) structure.\n•PROPHET achieves low-overhead and high-performance\nat the same time. The area and power overhead of\nPROPHET are lower than 2% in our experiments, and\nthe temporal resolution can achieve 4 clock cycles with\nMAE <7%.2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247979\nAuthorized licensed",
  "10.1109/DAC56929.2023.10247979\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "L1 Buf ferData \nReshapeWEI Buf fer\nN x M x K\nPE arryOUT\nBuffer\nIF Buf fer\nSliding Window and Accumulation\nTrained Power\nModelPredict Power in\nAdvanceSignal State as Patterns......\nLogic Tree for Sampling=0?\nRow 0=0?=0?\nRow 0=0?=0?\nRow 0=0?Fig. 1: PROPHET predicts the runtime power in DNN ac-\ncelerators. The red block is PROPHET on write ports of the\ninput-feature (IF) buffer. And the blue block is PROPHET on\nread ports. Both power models have the same structure.\nII. R ELATED WORKS\nA. On-chip Pow",
  " same structure.\nII. R ELATED WORKS\nA. On-chip Power Meters\nPrior research has investigated the use of performance\ncounters to estimate the power consumption during runtime\nfor both CPU and GPU architectures [6]–[9]. In these studies,\nmicro-architecture events such as cache misses and the number\nof retired instructions are counted within each power measure-\nment window, which typically spans several thousand cycles.\nA regression model is then trained to estimate the average\npower consumption wit",
  "ined to estimate the average\npower consumption within each measurement window based\non the event count. Nonetheless, these models are typically\nutilized in coarse-grained management scenarios and lack\nflexibility due to limited access to certain event counters.\nIn recent years, some RTL-based runtime power models\nhave been proposed for fast power managements by achieving\nhigh temporal resolution [1]–[3], [10]–[12]. They select the\nmost power-correlated RTL signals, named power proxies, as\nthe po",
  "elated RTL signals, named power proxies, as\nthe power model input. Notably, PowerProbe [2] can construct\nthe on-chip power meters with a temporal resolution of 100-1K\nclock cycles, while maintaining the resource overhead of less\nthan 8%. Simmani [3] achieves per-cycle temporal resolution\nfor the Rocket RISC-V microprocessor but utilizes over 500\nproxies, resulting in significant overhead. State-of-the-art solu-\ntions like APOLLO [1] can construct the OPM with per-cycle\ntemporal resolution and le",
  " the OPM with per-cycle\ntemporal resolution and less than 1% overhead. However, these\nmethods are primarily designed for architectures with complex\ncontrol flow and may not be suitable for data-streaming-based\nDNN accelerators. Moreover, their response time may still be\ninsufficient for advanced power management techniques.\nB. Voltage Management\nOver the past two decades, many techniques have been\nproposed for managing supply voltage to maintain the system\nstability, including voltage emergency ",
  "the system\nstability, including voltage emergency mitigation [4], [13] and\nvoltage noise smoothing [5], [14]. There is a strong connection\nbetween power fluctuations and inductive voltage fluctuations.\nThe works of [4], [15] proposed to predict the voltage emer-\ngency based on the micro-architecture events and take actions\nDNN\nAccelerator\n RTL Design\nSynthesis and\nLayout\nGate-level\nSimulation\nPower\nSimulation\nPower TraceArch Info\nExtract\nPatternsDefine Patterns\nDatasetLinear\nRegression\nDNN DataT",
  "Define Patterns\nDatasetLinear\nRegression\nDNN DataTrained Power \nModel\nOn-chip Power\nMeterFig. 2: The overall development framework of PROPHET.\nproactively. The works of [5], [14] applied a fine-grained OPM\nto monitor the power fluctuation to smooth the voltage noise.\nHowever, due to the processing delay of OPM and feedback\nloop delay from OPM to the power management unit, the\ninsufficient responding time left for the voltage management\nis still the bottleneck to be addressed.\nTo provide accurate",
  "he bottleneck to be addressed.\nTo provide accurate and timely power trace for fast power\nmanagement, we propose PROPHET, the first predictive and\nfine-grained on-chip power meter for DNN accelerators. As\nFig. 1 shows, PROPHET can be implemented either on the\nfeature buffers’ write ports or read ports, where the sampling\nlogic trees consisting of comparators and AND gates can\nextract pre-defined data patterns during memory access. These\ndata patterns are then accumulated using a small sliding pow",
  "rns are then accumulated using a small sliding power\nmeasurement window to construct the input vector, enabling\nhigh temporal resolution. Finally, the power consumption\nvalue can be calculated using the off-line trained parameters\nand the sampled input vector. Since these sampling and\ncalculation processes require only a few clock cycles, predict-\ning power consumption in advance is possible by observing\nmemory access patterns. Table I summarizes recent power\nmodeling methods. Compared to previo",
  " recent power\nmodeling methods. Compared to previous power modeling\nmethods, PROPHET can predict the power waveform dozens\nof clock cycles before it happens in an accurate, fine-grained,\nand low-cost way.\nMethod Type Resolution Cost Predictive\n[6]–[9] event counters ≥1Kcycles low %\n[2], [10], [11]\nsignal proxies≥100 cycles 5∼15%\n% [3] per-cycle >100%\n[1], [12] per-cycle <1%\nPROTHET data patterns ≥4cycles 1∼2% ✓\nTABLE I: Comparison among various power models\nIII. M ETHODOLOGY\nA. Overall Framework",
  "ower models\nIII. M ETHODOLOGY\nA. Overall Framework of PROPHET\nFig. 2 shows the PROPHET development framework. It first\nconducts accurate power simulations with input data of DNN\nmodel to gather ground-truth power traces. Simultaneously,\nour proposed data patterns are extracted from these input\ndata. These patterns have a strong correlation with the power\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "Symbol Description\nN Rows of PE array(feature input)\nM Columns of PE array(weight input)\nK Input (feature, weight) pairs in each PE\nL Length of local PE pipeline\nG The number of combinational logics in multiplier or adder\nW Number of cycles in sliding window\nNsw The number of sub-windows in sliding window\nR Temporal resolution of power model\nTABLE II: Description of frequently used symbols.\nconsumption of the specific systolic array architecture. The\ndetails of our data pattern definitions will ",
  " The\ndetails of our data pattern definitions will be presented in\nSubsection III-B. Next, the extracted data patterns are com-\nbined with the power traces to create the dataset for the\nsubsequent stage of power model training. The details of this\ndataset extraction and sampling procedure will be discussed in\nSubsection III-C. Finally, since our on-chip power meter uses\ndata patterns as input, it can be implemented at the write ports\nof the input buffer to predict the power consumption of the\nDNN",
  "buffer to predict the power consumption of the\nDNN accelerator dozens of cycles in advance. Alternatively,\nthe power meter at the read ports of the input buffer can\nestimate the run-time power. The difference between the two\ndesigns is discussed in detail in Subsection III-D.\nB. Define Data Patterns\nData patterns refer to the various combinations of input\ndata into logic gates that result in different dynamic power\nconsumption. In DNN algorithms, the activation functions,\nparticularly the common",
  " the activation functions,\nparticularly the commonly-used ReLU function, tend to intro-\nduce sparse data into the feature map of intermediate layers by\ngenerating zero outputs. We have discovered that the zeros in\nthe PE arrays’ inputs significantly impact the entire system’s\ndynamic power consumption. The work of [16] has demon-\nstrated that the sparsity of input feature maps can be a pattern\nto predict the workload for several milliseconds. However,\nsuch sparsity level can only reflect the pow",
  "ever,\nsuch sparsity level can only reflect the power consumption\nin coarse-grained temporal resolution. Therefore, it is still\nnecessary to identify more data patterns for fine-grained power\nmodeling to construct the power model for timely voltage\nmanagement.\nAs illustrated in Fig. 3, systolic array-based DNN accel-\nerators comprise an N×M×Karray with homogeneous\nPEs. In each PE, Kmultipliers and Kadders can perform\nmultiple-accumulate operations for matrix multiplications. The\narithmetic units ",
  " for matrix multiplications. The\narithmetic units in the PE can be abstracted as a combination\nof combinational logic gates Gand registers. The dynamic\npower consumption of the entire DNN accelerator during the\ntime window Tcan be represented by Eq. 1. Pdynmand\nPdynaare the dynamic power of multipliers and adders,\nrespectively. Pother is the power consumption generated by\nother components, such as memory, register, and local control\nlogic, which are less affected by the input data. Thus, Pother\n",
  "are less affected by the input data. Thus, Pother\ncan be approximated as a constant. The two input data\nof adders/multipliers are denoted as aandb. The average\ndynamic power consumption of multipliers and adders over\nthe time window Tcan be formulated as Eq. 2 and Eq. 3,\nrespectively. Here, αg(a, b)represents the toggle rate of the\n......Weight (M)\n...\nPEK\nInternal structure...a b\nlogic gates\nlogic gates...Input Feature\n(N)\n...PE\nPE\nPE\nPEPE\nPE\nPE\nPEPE\nPE\nPE\nPEPE\nPE\nPE\nPE...\n...\n...\nPE array.....",
  "\nPEPE\nPE\nPE\nPEPE\nPE\nPE\nPE...\n...\n...\nPE array......Fig. 3: The hierarchical structure of a typical PE array.\ncombinational logic gwith respect to the input data (a, b)of\nthe multiplier or adder, and Cgdenotes the average capacitance\nof the combinational logic g. The toggle rate αg(a, b)follows\na certain distribution αg(a, b)∈ Dg(a, b).\nPdyn=Pdynm+Pdyna+Pother (1)\nPdynm∝TX\n(NXMXKXG1X\ng11\n2V2αg1(a, b)Cg1)/T (2)\nPdyna∝TX\n(NXMXKXG2X\ng21\n2V2αg2(a, b)Cg2)/T (3)\nPremises : Since zero values in input ha",
  "g2)/T (3)\nPremises : Since zero values in input have a deterministic\nimpact on power consumption, we can categorize the data\npatterns for multipliers and adders as below. For multipliers,\n1) When both a̸= 0 andb̸= 0, the toggle rate will be high,\nDg(a, b) =Dmhigh, resulting in high power consumption. 2)\nWhen either aorbequals zero, the toggle rate will be low, and\nDg(a, b) =Dmlow. For adders, similarly, 1) When both a̸= 0\nandb̸= 0, the toggle rate will be high, Dg(a, b) =Dahigh.\n2) When either a",
  "e will be high, Dg(a, b) =Dahigh.\n2) When either aorbequals zero, αg(a, b)will follow the\nother distribution Damedium with a medium toggle rate, and\nthe power consumption will be medium. 3) When both aandb\nequal zero, the average toggle rate will be low, and the power\nconsumption will be low, Dg(a, b) =Dalow.\nApplying the law of large numbers in statistics, if the\nT×N×M×Kis sufficiently large, the average toggle\nrates of all combinational gates with the same structure in the\nPE array will conver",
  "ith the same structure in the\nPE array will converge to the expectations of the distribution.\nTherefore, the total dynamic power consumption of multipliers\nand adders in PE array with different input situations can be\napproximated as constants. For multipliers, we can statistically\nrecord two different input situations: a, b̸= 0 and any one of\naorbis equal to zero, denoted as m11andm01, respectively.\nSimilarly, for adders in the PE array, we can record the\nfollowing three input situations: a, b̸",
  "record the\nfollowing three input situations: a, b̸= 0, any one of aorbis\nequal to zero, and both aandbare zero, denoted as a11,a10,\nanda00, respectively. In this way, we can formulate the data-\ndriven power model as Eq. 4, where the learnable power model\nparameter Iis the average dynamic power corresponding to\neach input data situation, and αis the ratio of different input\nunder the time window and can sample during memory access.\nAuthorized licensed use limited to: Zhejiang University. Download",
  "nsed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "We can construct the power model based on these data patterns\nand employ a regression model to fit these parameters in Eq. 4.\nPdyn=Pother +NXMXKX\n(αm11Im11+αm01Im01)+\nNXMXKX\n(αa11Ia11+αa01Ia01+αa00Ia00)\n(4)\nC. Extraction and Sampling Data Patterns\nWe have discussed the data patterns related to zeros in\ninput data of adders/multipliers aandb. For the multipliers,\nas shown in Fig. 3, their two inputs aandbcorrespond\nto input features and weights, respectively. Currently, due to\nthe irregular memor",
  "espectively. Currently, due to\nthe irregular memory access enabled for DNN algorithms\nafter weight pruning, dense DNN algorithms are still the\npredominant workload for most general DNN accelerators.\nExecuting a dense DNN algorithm on an accelerator generates\na considerable amount of dynamic power consumption com-\npared to the sparse DNN algorithm since the sparsity of weight\ndata in sparse DNN can be more than 80% [17]. Therefore,\nour data-pattern-based power modeling focuses on the DNN\naccelera",
  "n-based power modeling focuses on the DNN\naccelerator when running dense DNN applications. Based on\nthis, our data patterns can be extracted directly from the feature\ndata (i.e., input aof multipliers), and all weights (i.e., input\nbof multipliers) are assumed to be non-zero by default. To\nachieve fine-grained power prediction in advance, we need to\nconstruct the dataset for power model training based on our\npredefined data patterns.\nFor DNN accelerators, the PE typically comprises multiple\nmult",
  "lerators, the PE typically comprises multiple\nmultipliers and adders. As Fig. 4 shows, since different zeros\ninaandbserve as data patterns for multipliers and adders,\nwe can directly count these patterns on the PE input through\nthe logic tree. The logic tree is constructed by abstracting the\nmultiplier as a comparator and the adder as an AND gate. The\nstates of signals S∈ {S0, S1, . . . , S 2k−2}in the logic tree can\nreflect the data patterns of multipliers and adders. Therefore,\nwe can record t",
  "multipliers and adders. Therefore,\nwe can record the states of signals in the logic tree as the\ndata patterns of PEs to construct the input vector of the power\nmodel.\nAnother crucial factor is the pipeline length, which includes\nthe arithmetic unit’s pipeline within a PE and the pipeline\nbetween PEs in the PE array. As long as the valid input data\nremains in the pipeline structure, it will continue to affect\nthe total power consumption. Thus, a sliding window that can\nrecord all data patterns du",
  "liding window that can\nrecord all data patterns during a certain period is necessary\nfor fine-grained temporal resolution. Considering that the input\nfeature data can transfer along the column dimension Mof\nthe PE array, the number of clock cycles in the sliding window\nWdepends on the average pipeline length of the PE internal\npipeline Land data transfer pipeline M, as shown in Eq. 5.\nRegarding the on-chip power meter, considering the tem-\nporal resolution Ras the stride of the sliding window, w",
  "resolution Ras the stride of the sliding window, when\nR < W , there will be multiple sub-windows to record the\npatterns from each sampling. The number of sub-windows\n Logic Tree0.150.1 S0\nS1\nS2S4\nS6S7\nS5\nS31=0?\n=0?\n=0?\n=0?0.200123\n0.100.20\n0.150.100\n0.20000.250.50.50\n0.500.750.25\n0.50.750.51\n0.750.250.50.5\n0.2500.50\n0.50.250.250.5\n0.2500.250cycle\nS0\nS1\nS2\nS3\nS4\nS5\nS6\nSliding Window3 2 1 0\naverage0.3125\n0.375\n0.6875\n0.5\n0.1875\n0.375\n0.125\nInput Vector Input Feature Datasub-windowXFig. 4: The exam",
  "ctor Input Feature Datasub-windowXFig. 4: The example of input vector generation for input feature\ndata is when K= 4,Nsw= 4andR= 4(S7can be pruned as\nit is same as S6). The logic tree corresponds to the PE structure\nin Fig. 3, with comparators and AND gates corresponding to\nmultipliers and adders, respectively. When assuming weights\nare non-zero, compactors only need to check input features.\nNswin the sliding window can be calculated as shown in\nEq. 6.\nW=L+ [M−1\n2] (5)\nNsw= [W/R ] (6)\nAssuming a",
  "Eq. 6.\nW=L+ [M−1\n2] (5)\nNsw= [W/R ] (6)\nAssuming an 8×8×4systolic array, feature data is\ntransferred between PEs along the Mdimension and the\nmultiply-accumulate (MAC) pipeline length structure in each\nPE is 12. Therefore, based on Eq. 5, the sliding window size\nWis 16. Fig. 4 shows an example of input vector generation\nwhen K= 4,Nsw= 4, and R= 4. The sampling logic tree\nconsists of 4 comparators and 4 AND gates corresponding to\n4 multipliers and 4 adders in the PE. The signal states ( s) in\nthe",
  " 4 adders in the PE. The signal states ( s) in\nthe logic tree are recorded. Every cycle, four input feature data\nare input to the PE array. After 4 cycles, the sliding window\nshifts, and the signal states Sduring the latest 4 cycles are\nupdated to sub-window 0 in Fig. 4. The values in the sliding\nwindow are then averaged to generate the new input vector.\nTo train the power model, we can construct a dataset based\non the defined data patterns, sliding window, and stimulated\npower traces. The per-c",
  "ing window, and stimulated\npower traces. The per-cycle pattern trace can be obtained by\nextracting the stimulation data using the equivalent logic tree.\nFor power model training, a linear regression model can be\nemployed. As shown in Eq. 7, the input vector X, which\ncorresponds to each S∈ {S0, S1, . . . , S 2k−2}, is constructed\naccording to the resolution R, and Pis the power consumption\nobtained from gate-level power simulation. After constructing\nthe dataset, the data-pattern-based power mode",
  "ing\nthe dataset, the data-pattern-based power model can be trained\nwith a regression model. The trained parameter Wand bias\nitembcan then be implemented into the hardware system.\nP=2K−2X\ni=0(Xi×wi) +b (7)\nD. Hardware Implementation and Power Prediction\nSince the proposed power model is based on the data pat-\nterns of input feature data, after the hardware implementation,\nit samples data patterns during memory access. Fig. 1 has\nshown the general architecture of DNN accelerators, which\ncomprises ",
  "architecture of DNN accelerators, which\ncomprises multi-level memories. The L1 buffer typically has a\nlarge capacity to store uniform data. The data reshape module\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "SA1 SA2\nsymbols value symbols value\nM 8 M 4\nN 4 N 4\nK 4 K 8\nL 22 L 28\nW 32 W 32\nTABLE III: Hardware Architecture Info\ncan convert the operations in convolution layers and fully\nconnected layers into the general matrix multiplication format.\nThe weight and feature data are stored in the weight (WEI)\nbuffer and input-feature (IF) buffer, respectively, for PE array\ncomputing. For fine-grained tracing of power consumption,\nthe sampling of data patterns should ensure the same order\nof input data flow",
  "ns should ensure the same order\nof input data flow as in the PE array. Therefore, the feasible\nlocations for integrating our proposed power meter are at the\nwrite or read ports of the feature buffer.\n1) Power Estimation on Read Ports: The power meter at\nthe read ports of the feature buffer can estimate the run-\ntime power of the DNN accelerators since the data input\nflow sampled by the power meter is the same flow as that\nto the DNN accelerators. Data pattern samplers will monitor\nthe signals’ s",
  " Data pattern samplers will monitor\nthe signals’ state in the sampling logic tree every clock cycle\nand then construct the input vector of the power model. The\npower consumption will be calculated based on the trained\nmodel. However, the power meter at the read ports of the\nfeature buffer can only achieve run-time estimation because\nthe response time left will be too short for any proactive power\nmitigation. Still, it will not be influenced by the control flow\nof the system.\n2) Power Prediction ",
  "e control flow\nof the system.\n2) Power Prediction on Write Ports: The power meter at\nthe write ports of the feature buffer can predict the power\nconsumption for a batch of data written in the buffer. The\nnumber of clock cycles predicted in advance is the time\ninterval between the read and write operations of this batch of\ndata. Unlike the samplers on the read ports, the controller can\naffect the data flow of the DNN accelerators. In our current\nexperiments, we assume that the sampled data still ",
  "xperiments, we assume that the sampled data still follow the\nsame flow as the read data, and these data will be read out\nto the accelerator without interruption. To further take a more\ncomplex control flow into consideration, our solution could be\nextended to address this impact by developing an additional\nsub-model for the controller, which will be studied in our\nfuture work.\nIV. E XPERIMENT AND DISCUSSION\nA. Experiment Setup\nThe simulation platform for experiments is implemented\naccording to F",
  "form for experiments is implemented\naccording to Fig. 1. The L1 buffer and data reshape modules\nare implemented in software for flexible simulation, while the\nrest of the modules are synthesized, placed, and routed with\nTSMC 40nm process. The ground-truth power simulation is\nperformed with Synopsys PTPX. The DNN accelerators are\nimplemented based on the widely-adopted output-stationary\nsystolic array (SA) with various shapes and PE structures, as\nsummarized in Table III. The data type used is fl",
  "\nsummarized in Table III. The data type used is floating-point\nnumbers, which generate a large amount of dynamic power.\n(a) NMAE\n (b)R2score\nFig. 5: Accuracy vs. temporal resolution for SA1.(sliding\nwindow W: 32 cycles)\n(a) NMAE\n (b)R2score\nFig. 6: Accuracy vs. temporal resolution for SA2. (Sliding\nwindow W: 32 cycles)\nTwo systolic array accelerators are constructed using the same\nmultiplier, adder, and accumulator, with pipeline lengths of 6,\n6, and 4, respectively. The sliding window size is f",
  " and 4, respectively. The sliding window size is fixed to\n32 clock cycles, which enables the shift operation to replace\nthe divider and reduce overhead. The power modeling dataset\ncomprises a 100,000-cycle power waveform obtained from\ngate-level power simulation. The stimuli are data from 10\nconvolution layers in VGG and ResNet.\nWe evaluate the performance of the proposed data-pattern-\nbased power model with two different DNN accelerators in\nTable III. The per-cycle pattern trace is generated by",
  "e III. The per-cycle pattern trace is generated by analyzing\nthe input feature data based on the defined data patterns.\nPower and data patterns with different temporal resolutions\nand sliding window sizes are then constructed by averaging\nthe per-cycle waveform during a time window for experiments\nwith different resolutions. We evaluate our power model using\nthe normalized mean absolute error (NMAE), coefficient of\ndetermination R2, and the overhead of power and area. We\ncompare our model with t",
  "ead of power and area. We\ncompare our model with two baselines: the one using sparsity\nas the data pattern [16] and minimax concave penalty (MCP)\nregression [1]. The method in [16] uses sparsity as the common\ndata pattern for workload prediction for DNN accelerators.\nMeanwhile, the MCP technique adopted in APOLLO [1] is\nconsidered one of the most advanced methods for selecting the\nminimum proxies from the RTL signals to construct accurate,\nper-cycle, and low-cost OPM.\nB. Evaluate Data-Pattern-Ba",
  "cle, and low-cost OPM.\nB. Evaluate Data-Pattern-Based Power Model\nWe evaluated the accuracy, temporal resolution, and over-\nhead of the three models. The MCP baseline selected about\n200 proxies from RTL signals to construct the power model\nfor SA1 and SA2, respectively. Fig. 5 and Fig. 6 show the\nR2score and the NMAE of power models under different\ntemporal resolutions. Our data-pattern-based power model can\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 a",
  "ejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "SA1 SA2\nSPA MCP OURS SPA MCP OURS\nArea (%) 0.21 1.15 0.96 0.31 1.17 1.96\nPower (%) 0.18 0.90 0.91 0.33 0.96 1.96\nTABLE IV: Hardware overhead of our power model. (SPA:\nonly sparsity of input data as model input)\ncompete with the state-of-the-art MCP-based power model\nwhen the resolution is more than 4 clock cycles, while\nproviding several additional benefits. First, PROPHET can\npredict power waveform dozens of clock cycles in advance\nduring memory access. Second, PROPHET only requires 7\nand 15 in",
  " access. Second, PROPHET only requires 7\nand 15 input patterns in the power model, while the MCP-\nbased OPM requries 100-200 input proxies. For the MCP-\nbased OPM, transitions of each proxy should be collected\nevery clock cycle from various locations of the entire chip,\nwhich may increase the difficulty of layout for large-scale\nsystems. Instead, PROPHET samples data patterns only on\nthe input buffers, and the layout of the power model is more\nconcentrated, reducing the overhead of the power met",
  "ncentrated, reducing the overhead of the power meter.\nIn our experiment, we implemented four samplers for four\ninput columns of the PE array, and the bit-width of parameters\nin the power model is 16 bits. Table IV shows the area and\npower overhead. For SA1 and SA2, the overhead of PROPHET\nis lower than 2%. And for the MCP-based model, the overhead\nis only around 1% beacuse multi-bit multipliers are replaced\nwith AND gates. However, as the size of the DNN accelerator\nincreases in M,N, and Kdimens",
  " the DNN accelerator\nincreases in M,N, and Kdimensions, the area and power\noverhead of PROPHET will be lower than 1%, since the\noverhead and power are only linear with MandK. Although\nMCP and SPA can achieve lower overhead than our method,\nas seen in Fig. 5 and Fig. 6, SPA incurs a much larger power\nestimation error. As for MCP, since it is based on runtime\ncounting of the signal switching activities inside the SA, it\ncannot be used for predictive power prediction.\nC. Predict Power in Advance\nPR",
  "e power prediction.\nC. Predict Power in Advance\nPROPHET at the write ports of the input feature buffer\nis implemented to demonstrate the feasibility of the power\nprediction. The input buffer is a ping-pong FIFO that enables\none batch of data to be written into the buffer first and then\nstarts the computing pipeline. The depth of the ping-pong\nFIFO is 64, leading to a 64-cycle time interval between write\nand read. When the input feature buffer is full, the DNN\naccelerator will start reading data ",
  "full, the DNN\naccelerator will start reading data for computing, and the next\nbatch of data will be put into the input buffer at the same\ntime. Fig. 7 illustrates the predictive and ground-truth power\nwaveform when the DNN accelerator is working. PROPHET\ncan achieve power prediction of 56 clock cycles in advance\nfor this buffer setting.\nV. C ONCLUSION AND FUTURE WORK\nIn this paper, we proposed PROPHET, a fine-grained predic-\ntive on-chip power meter for DNN accelerators that can predict\nthe powe",
  "ter for DNN accelerators that can predict\nthe power waveform dozens of cycles in advance. Our analysis\nand experimental results showed that PROPHET can achieve\na temporal resolution of 4 clock cycles with R2>0.92and\nNMAE <7%within 2% area/power overhead for two different\nFig. 7: PROPHET predicts the power waveform.\nsystolic array accelerators. Moreover, we demonstrated that\nPROPHET can predict the power waveform 56 cycles in\nadvance when the input buffer is a 64-depth ping-pong FIFO.\nOur future ",
  "t buffer is a 64-depth ping-pong FIFO.\nOur future research will focus on two directions. First,\nwe will conduct a comprehensive evaluation and exploration\nof our data patterns using different architectures and data\ntypes. Second, we will explore high-performance fast power\nmanagement techniques based on our power prediction model.\nVI. ACKNOWLEDGMENTS\nThis work is partially funded by Hong Kong RGC GRF\n16213521 and Huawei Hong Kong Research Center (HKRC).\nWe thank Chun Hang Lee and Jingbo Jiang in",
  "(HKRC).\nWe thank Chun Hang Lee and Jingbo Jiang in HKRC for their\nexcellent critique and feedback.\nREFERENCES\n[1] Z. Xie et al. , “APOLLO: An automated power modeling framework for\nruntime power introspection in high-volume commercial microproces-\nsors,” in MICRO , 2021.\n[2] D. Zoni et al. , “Powerprobe: Run-time power modeling through auto-\nmatic rtl instrumentation,” in DATE . IEEE, 2018.\n[3] D. Kim et al. , “Simmani: Runtime power modeling for arbitrary rtl with\nautomatic signal selection,” i",
  " arbitrary rtl with\nautomatic signal selection,” in MICRO , 2019.\n[4] V . J. Reddi et al. , “V oltage emergency prediction: Using signatures to\nreduce operating margins,” in HPCA . IEEE, 2009.\n[5] V . K. Kalyanam et al. , “A proactive voltage-droop-mitigation system in\na 7nm hexagon™ processor,” in VLSI . IEEE, 2020.\n[6] C. Gilberto et al. , “Power prediction for intel xscale processors using\nperformance monitoring unit events power prediction for intel xscale\nprocessors using performance monito",
  "r intel xscale\nprocessors using performance monitoring unit events,” in ISLPED , 2005.\n[7] F. Oboril et al. , “High-resolution online power monitoring for modern\nmicroprocessors,” in DATE . IEEE, 2015.\n[8] Y . Zhang et al. , “On-the-fly power-aware rendering,” in Computer\nGraphics Forum . Wiley Online Library, 2018.\n[9] M. Sagi et al. , “A lightweight nonlinear methodology to accurately\nmodel multicore processor power,” TCAD , 2020.\n[10] M. Najem et al. , “A design-time method for building cost-",
  " et al. , “A design-time method for building cost-effective run-\ntime power monitoring,” TCAD , 2016.\n[11] D. J. Pagliari et al. , “All-digital embedded meters for on-line power\nestimation,” in DATE . IEEE, 2018.\n[12] Z. Xie et al. , “DEEP: Developing extremely efficient runtime on-chip\npower meters,” in ICCAD , 2022.\n[13] M. S. Gupta et al. , “Understanding voltage variations in chip multipro-\ncessors using a distributed power-delivery network,” in DATE . IEEE,\n2007.\n[14] J. Leng et al. , “GPU ",
  " in DATE . IEEE,\n2007.\n[14] J. Leng et al. , “GPU voltage noise: Characterization and hierarchical\nsmoothing of spatial and temporal voltage noise interference in gpu\narchitectures,” in HPCA . IEEE, 2015.\n[15] G. Papadimitriou et al. , “Harnessing voltage margins for energy effi-\nciency in multicore cpus,” in MICRO , 2017.\n[16] S. Liu et al. , “Dynamic voltage and frequency scaling to improve energy-\nefficiency of hardware accelerators,” in HiPC . IEEE, 2021.\n[17] S. Han et al. , “Eie: Efficient",
  ". IEEE, 2021.\n[17] S. Han et al. , “Eie: Efficient inference engine on compressed deep neural\nnetwork,” ACM SIGARCH Computer Architecture News , 2016.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:49 UTC from IEEE Xplore.  Restrictions apply.",
  "Late Breaking Results: RL-LPO: Reinforcement\nLearning Based Leakage Power Optimization\nFramework with Graph Neural Network\nPeng Cao, Jiahao Wang\nNational ASIC System Engineering Technology Research Center, Southeast University, Nanjing, China\n{caopeng, wjiahao }@seu.edu.cn\nAbstract —Leakage power optimization based on threshold\nvoltage ( Vth) assignment poses great challenge in circuit design\ndue to its tremendous solution space. In this paper, a Reinforce-\nment Learning-based Leakage Power Opti",
  " Reinforce-\nment Learning-based Leakage Power Optimization framework\n(RL-LPO ) is first-ever proposed to formulate Vthassignment\nas a reinforcement learning (RL) process by learning timing\nand physical characteristics of each circuit instance with Graph\nNeural Networks (GNN). The proposed RL-LPO was validated by\nthe IWLS2005 and Opencores benchmark circuits with TSMC\n28nm technology and experimental results demonstrate that our\nwork achieves better leakage power optimization by additional\n3% red",
  "er leakage power optimization by additional\n3% reduction on average than the commercial tool PrimeTime\nwith 6.7 ×speed up when being transferred to unseen circuits\nwith negligible timing degradation.\nIndex Terms —leakage power, threshold voltage, reinforcement\nlearning\nI. I NTRODUCTION\nLeakage power tends to be an urgent issue especially in\nlow power scenarios with increasing portion of total power\nconsumption. In order to reduce leakage power under specific\ntiming constraints, voltage threshold",
  "der specific\ntiming constraints, voltage threshold ( Vth) assignment and gate\nsize adjustment approaches are commonly applied where Vth\nassignment is preferable due to the exponential influence to\nleakage power and the convenience with the same footprint\nduring cell swapping [1]. However, due to the iterative cell\nswap and subsequent timing check to avoid timing violation,\nthe leakage optimization problem has been proven to be an\nNP-hard problem [2] which poses significant challenges to\nthe opti",
  "[2] which poses significant challenges to\nthe optimization time due to exponentially increased solution\nspace with respect to the size of netlist.\nDifferent from the traditional heuristic and analytical algo-\nrithms which may lead to globally sub-optimal solution with\nunacceptable runtime, Graph Neural Network (GNN) [3] based\nmethods have attracted high attention recently to perform\nefficient and high-quality leakage optimization by learning\nthe optimization results from commercial tools. Nevert",
  "optimization results from commercial tools. Nevertheless,\nsince the optimization solutions from commercial tools are\nconsidered as the golden results during training, GNN-based\napproaches are hard to achieve lower leakage consumption\nwhen transferring to unseen circuits and more over, new timing\nviolations are inevitable during leakage reduction.\nIn this work, a R einforcement L earning-based L eakage\nPower O ptimization framework, namely RL-LPO , is first-ever\nproposed to efficiently explore th",
  ", is first-ever\nproposed to efficiently explore the design space of the Vthas-\nSubc ircuit Topologyb\nTopology GraphGNN    \nPolicy -Value \nNetworkReward \nFunction\nRL Agenta\nbc\nd\nef\ng\nhc\na\nd\nehf\ngTrain\nDesign #1b egfca\ndTiming Violation \nRemovalTransfer\nDesign #2\n  Fig. 1. Overview of RL-LPO framework\nsignment. The proposed framework was trained by IWIS2005\nand Opencores benchmark circuits and transferred to other\nunseen circuits for leakage optimization with TSMC 28nm\nmulti- Vthstandard cell libr",
  "zation with TSMC 28nm\nmulti- Vthstandard cell libraries. Compared with the GNN-\nbased solution [3] which consumes 7% higher leakage power\nthan commercial tool Synopsys PrimeTime with nonnegligi-\nble Total Negative Slack ( TNS) penalty, this work achieves\nbetter optimization results than PrimeTime with 4% leakage\nreduction and 26% less TNS than [3]. By removing timing\nviolations, the proposed framework outperforms PrimeTime\nwith 3% leakage reduction and 6.7 ×speedup.\nII. P ROPOSED METHOD\nA. Overv",
  "on and 6.7 ×speedup.\nII. P ROPOSED METHOD\nA. Overview\nThe overview of the proposed RL-LPO is illustrated in\nFig. 1, where the policy-value networks of RL agent is updated\nin the train stage by exploring the Vthassignment space\nand applied for unseen circuits in the transfer stage. Since\nthe sequential decisions made iteratively to swap Vthof all\ncircuit instances can be formulated as a Markov Decision\nProcess (MDP), the RL algorithm can be intuitively applied\nto minimize the leakage power as muc",
  "ively applied\nto minimize the leakage power as much as possible with Vth\nassignment while satisfying timing constraints.\nB. Graph Representation Learning\nIn order to get more meaningful embeddings for each\ninstance and improve the transferable capability of leakage\noptimization framework, we leverage GraphSAGE [4] to2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247707\nAuthorized licensed use limited to: Zhejiang Univ",
  "\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:54 UTC from IEEE Xplore.  Restrictions apply.",
  "TABLE I\nCOMPARISON OF LEAKAGE POWER , TNS, AND RUNTIME BY PrimeTime , GNN [3] AND RL-LPO ON UNSEEN CIRCUITS .\nDesignLeakage (µw) TNS(ps) Runtime (sec)\nPrimeTime GNN [3]RL-LPO\nw/ormv.1 RL-LPO PrimeTime GNN [3]RL-LPO\nw/ormv.1 RL-LPO PrimeTime GNN [3]RL-LPO\nw/ormv.1 RL-LPO\nwbconmax 328.4 347.3 316.6 318.8 -130.9 -222.2 -175.3 -131.1 24.15 4.45 4.46 5.76\ntate 532.7 569.5 519.4 521.6 -834.2 -938.8 -926.8 -834.2 175.24 18.10 18.15 19.35\nethernet 270.0 289.4 260.1 261.3 -220.1 -319.7 -295.9 -220.2 99.1",
  "289.4 260.1 261.3 -220.1 -319.7 -295.9 -220.2 99.16 12.43 12.48 13.48\nvga 846.3 916.5 821.4 826.9 -165.1 -304.4 -277.9 -165.1 61.53 6.47 8.69 10.39\nnova 309.4 329.7 300.8 301.9 -382.5 -429.3 -410.9 -382.6 78.27 11.50 11.52 12.42\nNorm. 1.00× 1.07× 0.96× 0.97× 0.0 -96.3 -70.8 -0.1 1.0× 7.8× 7.3× 6.7×\n1RL-LPO without timing violation removal.\naggregate the information of neighboring instances and to\nencode features for all instances, which can get node feature\ninformation efficiently in an inductiv",
  "ode feature\ninformation efficiently in an inductive way.\nGiven a typical design, we first transform it into a netlist\nG, and for each instance v∈G, we can get its representation\nvector hk\nvby aggregating its previous representation at level\nkas:\nhk−1\nNk(v)=meanpool\u0000\b\nWagg\nkhk−1\nu,∀u∈Nk(v)\t\u0001\n,(1)\nhk\nv=sigmoid\u0010\nWproj\nk·concath\nhk−1\nv, hk−1\nNk(v)i\u0011\n,(2)\nwhere Nk(v)represents the neighbors of current instance at\nlevel k;Wagg\nkandWproj\nkmean the aggregation matrices and\nprojection matrices respective",
  "gation matrices and\nprojection matrices respectively. Besides, inspired by classical\ndeep learning methods which combines shallow and deep\nlayers [5], we concatenate all layers’ output embedding in\nthe final stto make more accurate assignments and converge\nmore quickly because each embedding of the layers includes\ninformation from different depths.\nC. Reinforcement Learning Formulation\nSince assigning new Vthcan be treated as a MDP and has\nbeen proved to be an NP-hard problem [2], Double Deep Q\n",
  "roved to be an NP-hard problem [2], Double Deep Q\nNetwork (DDQN) [6] is used as the RL agent in this work\nto perform Vthassignment by thoroughly exploring the action\nspace to get optimal solution.\nDuring each action iteration, the corresponding reward is\ncalculated according to the update of leakage and TNS. Note\nthat increasing one instance’s Vthwill bring lower leakage\npower but higher cell delay which may cause slack violation,\nthe reward Rt(st,at)for state stwith action atis defined\nas the n",
  "t,at)for state stwith action atis defined\nas the normalized change of leakage power with its original\nleakage when TNS does not increase and otherwise, the reward\nis set to be -1 as a penalty, as formulated as follows:\nRt(st,at) =(\nPt−1−Pt\nPt−1TNSt>=TNSt−1\n−1 otherwise, (3)\nwhere PtandPt−1mean the leakage power of the current\ninstance (current state st) after and before being assigned to\na new Vthrespectively while TNStandTNSt−1denote the\ncorresponding TNS.\nNote that when transferring the traine",
  "onding TNS.\nNote that when transferring the trained RL agent to unseen\ncircuits, timing violations may occur due to over aggressive\nVthprediction, which could be removed by assigning lower\nVthto the instances with negative worst slacks while mini-\nmizing the total leakage power.III. E XPERIMENTAL RESULTS\nThe proposed RL-LPO was trained by IWIS 2005 and\nOpencores benchmark circuits with TSMC 28nm multi- Vth\nstandard cell libraries and transferred to unseen circuits to\nperform leakage optimization",
  "to unseen circuits to\nperform leakage optimization, which is compared with the\nGNN-based method [3] and commercial tool PrimeTime by\nleakage power, TNS, and runtime, as shown in I.\nIt can be seen that rather than the additional 7% leakage cost\nby the GNN-based method [3] with over -96.3 ps TNS penalty\nin average compared with PrimeTime , the proposed RL-LPO\nachieves better Vthassignment solution by 3% leakage re-\nduction with negligible timing degradation. Even without the\naid of timing violatio",
  "gradation. Even without the\naid of timing violation removal, the TNS loss of RL-LPO is\nstill 26% less (-70.8 ps) than the GNN-based solution [3] with\n4% lower leakage than PrimeTime , indicating that RL-based\nframework enables more powerful leakage optimization ability\nthan the GNN model which learns Vthassignment solution\nfrom PrimeTime . Due to the runtime overhead in RL-based\nprediction and timing violation removal, this work performs\nslower than [3] but still outperforms PrimeTime with 6.7 ×",
  "han [3] but still outperforms PrimeTime with 6.7 ×\nspeedup.\nACKNOWLEDGMENT\nThis work was supported in part by the National Key\nResearch and Development Program of China (Grant No.\n2019YFB2205004), and in part by the National Natural Sci-\nence Foundation of China under Grant (62174031) and in\npart by the Jiangsu Natural Science Foundation (Grant No.\nBK20201233).\nREFERENCES\n[1] Daboul, Siad, et al. ”An approximation algorithm for threshold voltage\noptimization.” ACM Transactions on Design Automati",
  "optimization.” ACM Transactions on Design Automation of Electronic\nSystems (TODAES) 23.6 (2018): 1-16.\n[2] Ning, Wing. ”Strongly NP-hard discrete gate-sizing problems.” IEEE\ntransactions on computer-aided design of integrated circuits and systems\n13.8 (1994): 1045-1051.\n[3] Lu, Yi-Chen, et al. ”A fast learning-driven signoff power optimiza-\ntion framework.” Proceedings of the 39th International Conference on\nComputer-Aided Design. 2020.\n[4] Hamilton, Will, Zhitao Ying, and Jure Leskovec. ”Induct",
  "ton, Will, Zhitao Ying, and Jure Leskovec. ”Inductive repre-\nsentation learning on large graphs.” Advances in neural information\nprocessing systems 30 (2017).\n[5] He, Kaiming, et al. ”Deep residual learning for image recognition.”\nProceedings of the IEEE conference on computer vision and pattern\nrecognition. 2016.\n[6] Van Hasselt, Hado, Arthur Guez, and David Silver. ”Deep reinforcement\nlearning with double q-learning.” Proceedings of the AAAI conference\non artificial intelligence. V ol. 30. No.",
  "nference\non artificial intelligence. V ol. 30. No. 1. 2016.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:54 UTC from IEEE Xplore.  Restrictions apply.",
  "HH-PIM: Dynamic Optimization of Power and Performance\nwith Heterogeneous-Hybrid PIM for Edge AI Devices\nSangmin Jeon†, Kangju Lee†, Kyeongwon Lee, and Woojoo Lee∗\nSchool of Intelligent Semiconductor Engineering, Chung-Ang University, Seoul, Korea\n{jademin96, agl0312, since69se, space}@cau.ac.kr\nAbstract—Processing-in-Memory (PIM) architectures offer promising\nsolutions for efficiently handling AI applications in energy-constrained\nedge environments. While traditional PIM designs enhance performa",
  "ts. While traditional PIM designs enhance performance\nand energy efficiency by reducing data movement between memory and\nprocessing units, they are limited in edge devices due to continuous power\ndemands and the storage requirements of large neural network weights\nin SRAM and DRAM. Hybrid PIM architectures, incorporating non-\nvolatile memories like MRAM and ReRAM, mitigate these limitations\nbut struggle with a mismatch between fixed computing resources and\ndynamically changing inference workload",
  "ources and\ndynamically changing inference workloads. To address these challenges,\nthis study introduces a Heterogeneous-Hybrid PIM (HH-PIM ) architec-\nture, comprising high-performance MRAM-SRAM PIM modules and\nlow-power MRAM-SRAM PIM modules. We further propose a data\nplacement optimization algorithm that dynamically allocates data based\non computational demand, maximizing energy efficiency. FPGA prototyp-\ning and power simulations with processors featuring HH-PIM and other\nPIM types demonstrat",
  "rs featuring HH-PIM and other\nPIM types demonstrate that the proposed HH-PIM achieves up to 60.43%\naverage energy savings over conventional PIMs while meeting application\nlatency requirements. These results confirm HH-PIM’s suitability for\nadaptive, energy-efficient AI processing in edge devices.\nI. Introduction\nWith the advent of artificial intelligence (AI), real-world applica-\ntions are rapidly expanding, fueling a trend to embed AI capabilities\ninto IoT devices across diverse fields. However",
  "es\ninto IoT devices across diverse fields. However, traditional server-\ncentric data processing, such as cloud computing, faces significant\nenergy and latency challenges due to processing and communication\noverloads. Consequently, distributing AI workloads to edge devices\nhasbecomeapromisingsolution,withrecentresearchfocusingonen-\nabling on-device AI through TinyAI models that support lightweight,\nlocal computations [1], [2].\nIn energy-constrained edge environments, integrating Processing-\nin-Me",
  "d edge environments, integrating Processing-\nin-Memory (PIM ) architectures has emerged as a promising ap-\nproachforexecutingAIapplicationsefficiently[3]–[8].PIMenhances\nperformance and energy efficiency in memory-intensive tasks, such\nas AI applications, by minimizing the overhead of data movement\nbetween processing and memory units. Early PIM designs primarily\nemployed volatile memories like SRAM [9] and DRAM [10], but\nthese designs faced challenges in storing large neural network weights\ndue ",
  "enges in storing large neural network weights\ndue to the continuous power demands of volatile memory and\nissues such as SRAM leakage power and the periodic refresh cycles\nrequired by DRAM. To address these limitations, PIM architectures\nbased on non-volatile memories ( NVMs), such as MRAM [11],\n[12] and ReRAM [13], [14], were proposed. These designs achieve\nhigh energy efficiency in weight storage, especially when combined\nwith power-gating techniques. However, NVMs may introduce addi-\ntional re",
  "iques. However, NVMs may introduce addi-\ntional read/write latency, potentially impacting overall neural network\nperformance. Consequently, recent advances in PIM design have\nintroduced hybrid architectures combining SRAM and NVM, known\nas Hybrid-PIM (H-PIM ). These architectures use NVM to store\nThis work was supported in part by Institute of Information & communi-\ncations Technology Planning & Evaluation (IITP) grants funded by the Korea\ngovernment(MSIT) (No.2022-0-00971, Logic Synthesisfor NV",
  "ment(MSIT) (No.2022-0-00971, Logic Synthesisfor NVM-basedPIM\nComputing Architecture and No. RS-2023-00277060, Development of open\nedge AI SoC hardware and software platform), and in part by the National\nResearch Foundation of Korea (NRF) grant funded by the Korea government\n(MSIT) (No. RS-2024-00345668)\n†Sangmin Jeon and Kangju Lee contributed equally to this work.\n∗Woojoo Lee is the corresponding author.weight data and SRAM as a buffer for input and output data, thereby\nenhancing both performan",
  " and output data, thereby\nenhancing both performance and energy efficiency [15]–[17].\nHowever, H-PIM faces distinct limitations in achieving optimal\nenergy efficiency during dynamic scenarios where inference loads\nfluctuate in real time on edge devices. For instance, an edge device\nrunning a YOLO model for real-time object detection experiences\nsubstantial variations in processing demand depending on the number\nof objects detected per video frame. Operating at a fixed performance\nlevel across al",
  ". Operating at a fixed performance\nlevel across all time intervals—typically set for peak computational\nload—inevitably leads to inefficient energy consumption. To address\nthis,weobservethatthemismatchbetweenfixedcomputingresources\nand dynamically changing workloads has long been a challenge in\ntraditional CPU-centric architectures. Established solutions, such as\nDynamic Voltage and Frequency Scaling (DVFS ) [18]–[21] and het-\nerogeneous multi-processor architectures with high-performance and\nlo",
  "ocessor architectures with high-performance and\nlow-power cores [22]–[24], have been extensively researched for this\npurpose. Our focus centers on heterogeneous architectures, as DVFS\ncontinues to face significant challenges in edge devices due to added\ndesign complexities, such as DC-DC converters and real-time power\nmonitoring. In contrast, heterogeneous architectures, exemplified by\nARM’s big.LITTLE architecture [25], are widely implemented in\ncommercial processors, effectively improving ener",
  "\ncommercial processors, effectively improving energy efficiency by\nadapting to dynamic computational loads. Building on this insight,\nwe propose configuring PIM modules, which integrate memory and\nProcessing Elements (PEs) for independent computation, into high-\nperformance and low-power configurations. This approach allows\nPIM architectures to dynamically balance performance and energy\nconsumption throughout the application runtime.\nTo further elaborate this idea, we introduce Heterogeneous-Hyb",
  "laborate this idea, we introduce Heterogeneous-Hybrid\nPIM (HH-PIM ), an architecture designed to dynamically optimize\nperformance and energy efficiency for AI applications on edge\ndevices. HH-PIM integrates two distinct PIM modules: a High-\nPerformance (HP) PIM module and a Low-Power (LP) PIM module.\nEach module’s memory consists of a hybrid configuration of MRAM\nand SRAM banks, resulting in four types of memory: HP-MRAM,\nHP-SRAM, LP-MRAM, and LP-SRAM. Unlike conventional hybrid\nPIM architecture",
  "-SRAM. Unlike conventional hybrid\nPIM architectures—where NVM is primarily allocated for weight\nstorage and SRAM is reserved as an input-output buffer—HH-PIM\nadopts an adaptive approach. During periods of high computational\ndemand, HH-PIM actively utilizes SRAM for weight storage as well,\nmaximizing responsiveness to fluctuating inference loads. Addition-\nally, to capitalize on HH-PIM’s architecture, we propose an optimal\ndata distribution algorithm that minimizes energy consumption by\ndynamical",
  "thm that minimizes energy consumption by\ndynamically adjusting data allocation across the four memory types.\nThis combinatorial optimization algorithm allocates weight data\nacross HP-MRAM, HP-SRAM, LP-MRAM, and LP-SRAM to reduce\nenergy use while balancing workload between HP-PIM and LP-\nPIM. With this design, HH-PIM efficiently adapts to the changing\ncomputational demands of AI applications, achieving significant\nenergy savings without compromising performance.\nToverifythefunctionalityandevaluat",
  "ng performance.\nToverifythefunctionalityandevaluatetheeffectivenessofourpro-\nposed technique, we modeled the memory and PEs and performed an\nRTL-level design of the entire PIM processor, incorporating the HH-\nPIM architecture. We then conducted FPGA prototyping to confirm\ncorrect operation and measure performance of the developed PIM2025 62nd ACM/IEEE Design Automation Conference (DAC) | 979-8-3315-0304-8/25/$31.00 ©2025 IEEE | DOI: 10.1109/DAC63849.2025.11132829\nAuthorized licensed use limited ",
  "849.2025.11132829\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "processor, while power measurements were obtained through synthe-\nsis using 45nm process technology and memory model simulations.\nThrough various benchmark scenarios, we experimentally validated\nthe energy-saving potential of the proposed HH-PIM architecture\nand data distribution algorithm. Results showed that our approach\nmaximizesenergyefficiencywhilesatisfyingthelatencyrequirements\nof AI applications in edge computing environments. Specifically, the\ndeveloped PIM processor demonstrated superi",
  "y, the\ndeveloped PIM processor demonstrated superior adaptability to real-\ntime inference load variations compared to conventional PIM-based\nprocessors, achieving up to 60.43% average energy savings across\ndifferent benchmark scenarios relative to the baseline processor.\nThese outcomes validate the suitability of the HH-PIM architecture\nfor maximizing efficiency in edge processors running AI applications.\nII. HH-PIM Architecture for AI Edge Processors\nFig. 1 illustrates the proposed HH-PIM archi",
  "ssors\nFig. 1 illustrates the proposed HH-PIM architecture, designed as\na solution to dynamically maximize energy efficiency in response\nto changes in inference workload, while meeting the performance\ndemands of AI applications on edge devices. Basically, HH-PIM\nadopts the general structure of near-memory computing architectures,\nas suggested in previous research [10], [26]. It comprises multiple\nPIM modules, each with PEs and memory banks, a controller that\nmanages these modules, and an interfac",
  "roller that\nmanages these modules, and an interface for external communication.\nOperating based on dedicated PIM instructions, commands received\nfrom the processor core are sequentially stored in the PIM Instruc-\ntion Queue . Unlike conventional PIM architectures, where a single\ncontroller manages all PIM modules based on PIM instructions, HH-\nPIM incorporates two distinct controllers: HP-PIM Controller and\nLP-PIMController , as shown in the figure. This dual-controller setup\nis designed specifi",
  "re. This dual-controller setup\nis designed specifically to support HH-PIM’s unique heterogeneous\narchitecture, which consists of two types of clusters: HP-PIM module\ncluster operating at high performance with higher power consump-\ntion, and LP-PIM module cluster operating at low power with reduced\nperformance. The HP-PIM and LP-PIM Controllers are responsible\nfor controlling and synchronizing their respective module types.\nAnother notable feature of HH-PIM is its hybrid memory architec-\nture,whe",
  " of HH-PIM is its hybrid memory architec-\nture,wherethePIMmodule’smemoryconsistsofMRAMandSRAM\nwith distinct characteristics and read/write latencies. The previous H-\nPIM architectures leverage the data characteristics of neural networks\nby storing large weight data in NVM to enhance energy efficiency,\nwhile using SRAM or DRAM for input/output data to improve\ncomputationalperformancebyprovidinglow-latencyaccess.HH-PIM\nbuilds upon this approach, exploiting the performance and energy\nefficiency ben",
  "ploiting the performance and energy\nefficiency benefits of hybrid memory. However, to overcome the\nlimitations of fixed memory allocation based solely on data character-\nistics, HH-PIM adds flexibility. When computational demand spikes\nand maximum PIM computing power is required, HH-PIM allows\nweight data to be stored in SRAM, thus enabling stable operation\neven in cases where traditional H-PIM architectures cannot meet the\nmaximum latency requirements of certain applications. To realize this\nca",
  "ements of certain applications. To realize this\ncapability, the PIM module is designed to support variable operand\ncounts retrieved from MRAM and SRAM during computation. Each\nPIM module’s internal interface is controlled by the controller, which\ndynamically adjusts the load process based on data storage status.\nThis design ensures synchronization of differing memory read cycles\nand access speeds between MRAM and SRAM in the LOAD state,\nguaranteeing reliable operation.\nThis heterogeneous and hyb",
  "ing reliable operation.\nThis heterogeneous and hybrid architecture of HH-PIM enables\nflexible adaptation to real-time variations in inference workloads\nfor AI applications. When computational demand is high, the HP-\nPIM module and SRAM can be actively utilized to boost processor\nperformance. Conversely, in low-demand scenarios, computations can\nbe maximally allocated to the LP-PIM module to meet application\nlatency requirements and maximize energy efficiency. To implement\nthis strategy effective",
  "y efficiency. To implement\nthis strategy effectively, it is essential to design the PIM Controllers\nwith precision, as they play a crucial role by synchronizing compo-\nLP-PIM ControllerHP-PIM ControllerHP-PIM Module Cluster\nHP-PIM InterfaceHP-PIM Module 1\nHP\nMRAMInterfaceHP\nSRAMPE\nHP-PIM Module 2\nHP\nMRAMInterfaceHP\nSRAMPE\nHP-PIM Module 3\nHP\nMRAMInterfaceHP\nSRAMPE\nHP-PIM Module n\nHP\nMRAMInterfaceHP\nSRAMPELP-PIM Module Cluster\nLP-PIM Module 1\nLP\nMRAMInterfaceLP\nSRAMPE\nLP-PIM Module 2\nLP\nMRAMInterf",
  "AMInterfaceLP\nSRAMPE\nLP-PIM Module 2\nLP\nMRAMInterfaceLP\nSRAMPE\nLP-PIM Module 3\nLP\nMRAMInterfaceLP\nSRAMPE\nLP-PIM Module n\nLP\nMRAMInterfaceLP\nSRAM\nPELP-PIM InterfaceHH-PIM Interface PIM Instruction QueueHH-PIM\nInterface \nPeri.HH-PIM ArchitectureFig. 1: Block diagram of the proposed HH-PIM architecture.\nnents that operate at different speeds and minimizing data movement\noverhead between PIM operations.\nThe architecture of HP-PIM and LP-PIM Controllers is fundamen-\ntally identical. Fig. 2 shows the ",
  "rs is fundamen-\ntally identical. Fig. 2 shows the architecture block diagram of the HP-\nPIM Controller, which consists of a State Machine ,Instruction De-\ncoder,Command Encoder ,Data Allocator , andInterface Logic . The\ncontroller operates through the basic PIM instruction cycle, which\nincludes the FETCH-DECODE-LOAD-EXECUTE-STORE phases,\nmanaged internally by the State Machine. The Instruction Decoder\ndecodes the fetched instruction into components such as the instruc-\ntion type ( Category), spe",
  "ts such as the instruc-\ntion type ( Category), specific operation or data movement details to\nbe executed by the PIM module ( Instruction Field ), and the target\nmodule for the operation ( Module Select Signal ). The Command\nEncoder then generates command signals for each PIM module based\non the decoded instruction details.\nTo minimize overhead from frequent data movement between\nHP-PIM and LP-PIM modules, as well as between MRAM and\nSRAM within each PIM module—resulting from the dynamic energy\n",
  "each PIM module—resulting from the dynamic energy\noptimization of the HH-PIM architecture—the controller design of\nHH-PIM incorporates a Data Allocator and separates the Interface\nLogic as seen in Fig. 2. The Data Allocator manages data placement\nto minimize external data movement during PIM operations for\nvarious computations, such as convolution and Multiply-Accumulate\n(MAC) operations within AI applications. Additionally, the Interface\nLogicisdividedintoaCMDInterfaceLogicfordeliveringcommands",
  "dividedintoaCMDInterfaceLogicfordeliveringcommands\nto the PIM modules and a MEM Interface Logic for data movement\nbetweenPIMmodules.Whenoperanddataisappropriatelypositioned\nin each PIM module’s memory, computations can proceed efficiently\nthrough the CMD Interface Logic alone. Furthermore, the bandwidth\noftheMEMInterfaceLogicisscaledaccordingtothenumberofPIM\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions appl",
  " 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "HP-PIM Controller\nInstruction Field (Opcode, Operands, Addr.) Module Select SignalInstruction Decoder\nCategory\nModule Index, AddressPIM\nCommand Command \nEncoder\nEnable\nCMD Interface \nLogicState MachinePIM \nInstruction\nData,\nAddrEnablePIM\nCommandState Control \nLogicState \nRegister\nAddress Generator\nAddress\nCalculation LogicAddress \nRegisterData Rearrange Buffer\nHP-PIM Module Buffer\nMEM Interface \nLogicData AllocatorHP-PIM\nData, Addr.\nLP-PIM\nData, Addr.Fig. 2: Block diagram of the HP-PIM Controlle",
  "Addr.Fig. 2: Block diagram of the HP-PIM Controller architecture.\nmodules within each cluster, enabling parallel data transfers across\nclusters from multiple PIM modules simultaneously.\nThe controller’s operation sequence for data movement between\nPIM modules is as follows. First, when a data placement instruction\nfrom the core prompts data transfer between HP-PIM and LP-PIM\nmodules, the controller decodes the instruction and provides the\nrelevant operation and address information to the Data Al",
  "t operation and address information to the Data Allocator.\nBased on the provided addresses, the Data Allocator accesses the\nmemory in each PIM module via the MEM Interface Logic and\nstores the data to be transferred in the Data Rearrange Buffer . The\nData Rearrange Buffer retains the data until the destination PIM\nmodule in the opposite cluster is ready for data writing, preventing\ndata conflicts caused by the speed discrepancy between HP-PIM\nand LP-PIM modules. Next, the Address Generator withi",
  " LP-PIM modules. Next, the Address Generator within the Data\nAllocator generates the destination PIM module index and memory\nbank addresses in the opposite cluster to facilitate data movement.\nThe generated addresses are then provided to the Data Rearrange\nBuffer, enabling it to place the buffered data into the memory banks\nof the designated PIM modules.\nIn summary, the PIM controllers in HH-PIM are meticulously\ndesigned to support data transfers between HP-PIM and LP-PIM\nmodules and to effectiv",
  " between HP-PIM and LP-PIM\nmodules and to effectively manage operations within each PIM mod-\nule. This design maintains operational consistency and prevents data\nconflicts, supporting the HH-PIM architecture in achieving its design\ngoals of real-time performance and energy efficiency optimization.\nIII. Dynamic Data Placement Strategy for Optimizing Energy\nEfficiency in HH-PIM\nOptimal data placement in the HH-PIM architecture is essential\nfor achieving maximum energy efficiency while ensuring smo",
  "eving maximum energy efficiency while ensuring smooth\napplication execution without latency. In HH-PIM, each layer of a\nneural network is distributed across HP-PIM and LP-PIM modules\nforparallelcomputation,withthefinaloutputobtainedbyaggregating\nresults from each module. When high performance is prioritized, it is\ncritical to minimize idle times where the HP-PIM module completes\nits tasks quickly but must wait for the slower LP-PIM module. To\nachieve low output latency, a balanced distribution o",
  "ieve low output latency, a balanced distribution of data between\nHP-PIM and LP-PIM modules, as well as an optimal utilization\nratio of MRAM and SRAM—each with different read/write laten-\ncies—must be carefully managed.\nOn the other hand, energy efficiency can be prioritized when\nAI applications do not always require the lowest possible inference\nlatency. In such cases, HH-PIM can operate below peak performance\nby allocating more data to the energy-efficient LP-PIM module\nand increasing the usage",
  "y-efficient LP-PIM module\nand increasing the usage of MRAM, thus improving overall energy\nefficiency while maintaining acceptable application latency. Based\non this approach, we propose a dynamic data placement strategy that\nperiodically adjusts the distribution of weight data during runtime.\nThis strategy optimizes energy efficiency by shifting computationalloadsaccordingtoreal-timedemands,focusingonmaximizingenergy\nsavings while meeting the minimum latency requirements of the\napplication.\nA. P",
  "imum latency requirements of the\napplication.\nA. Problem Definition\nIn the proposed weight data placement strategy, data redistribution\noccurs every predefined interval 𝑇, referred to as the time slice . We\ndefine a set of PIM operations generated by a single inference process\nin an AI application as a task. These tasks, generated by inference, are\nstored in a task buffer, and HH-PIM sequentially processes the tasks\nstored from the previous time slice within the current time slice . This\napproac",
  "slice within the current time slice . This\napproach ensures that the operational latency of HH-PIM does not\nexceed 2𝑇, thereby maintaining the inference latency required by AI\napplications. In this framework, the number of tasks stored in the task\nbuffer determines the minimum time 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 needed for each task\nto be processed within the time slice without delay. Specifically, each\ntask’s processing time 𝑡𝑡𝑎𝑠𝑘must not exceed 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 to guarantee\nthat all tasks are completed within the all",
  "rantee\nthat all tasks are completed within the allotted time slice , ensuring\nthe system adheres to the required maximum latency.\nThe proposed method aims to achieve the most energy-efficient\ndata distribution for each task within the time constraint 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 .\nThe HH-PIM architecture includes four distinct storage spaces: HP-\nMRAM, HP-SRAM, LP-MRAM, and LP-SRAM, each with different\npower consumption and latency characteristics. Consequently, we\nare faced with a discrete combinatorial optimiz",
  "we\nare faced with a discrete combinatorial optimization problem of\ndetermining the optimal placement of each weight data in order to\nminimize energy consumption. This problem can be reduced to a\nwell-known knapsack problem [27], with necessary adaptations for\nourspecificcontext.Here,eachstoragespacecorrespondstoanitem 𝑖\nin the knapsack problem, and the time constraint 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 , task ex-\necution time 𝑡𝑡𝑎𝑠𝑘, and energy consumption per task 𝐸𝑡𝑎𝑠𝑘represent\nthe knapsack’s capacity, item weight, a",
  "𝑘represent\nthe knapsack’s capacity, item weight, and item value, respectively.\nHowever, this problem is more complex than the standard knapsack\nproblem, as it exhibits the characteristics of both an unbounded\nknapsack (due to the multiple selections of a particular storage space)\nandamulti-choiceknapsack(duetothefixedtotalnumberofselected\nstorage spaces, equivalent to the number of weight data).\nThe mathematical formulation of this problem is as follows:\nMinimize𝐸𝑡𝑎𝑠𝑘=𝑛∑︁\n𝑖=1𝑒𝑖·𝑥𝑖 (1)\nsubject to",
  "follows:\nMinimize𝐸𝑡𝑎𝑠𝑘=𝑛∑︁\n𝑖=1𝑒𝑖·𝑥𝑖 (1)\nsubject to:𝑛∑︁\n𝑖=1𝑡𝑖·𝑥𝑖≤𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡,𝑛∑︁\n𝑖=1𝑥𝑖=𝑘,\n𝑥𝑖∈Z≥0,∀𝑖=1,2,···,𝑛.\nwhere𝑛denotes the number of storage spaces, 𝑥𝑖represents the\nnumber of data assigned to storage space 𝑖, and𝑘is the total\nnumber of weight data to be stored. Additionally, 𝑡𝑖and𝑒𝑖denote the\ncomputation time per weight and the energy consumption per weight\nin storage space 𝑖, respectively. This fomula defines the objective\nfunction to minimize energy consumption per task, 𝐸𝑡𝑎𝑠𝑘, along wit",
  "mize energy consumption per task, 𝐸𝑡𝑎𝑠𝑘, along with\nconstraints on execution time and the total count of selected items.\nB. Proposed Solution for Optimized Data Placement\nSince this problem is inherently more complex than the standard\nknapsack problem, it is naturally NP-hard. To address the opti-\nmization problem defined in (1), we propose a bottom-up dynamic\nprogramming (DP) approach. This method uses a DP table to store\nintermediate results, avoiding redundant calculations and solving the\npro",
  "voiding redundant calculations and solving the\nproblem incrementally. The recurrence relation is defined as follows:\n𝑑𝑝[𝑖][𝑡][𝑘]= \n𝑑𝑝[𝑖−1][𝑡][𝑘],(𝑡𝑖·𝑘 >𝑡)\nmin(𝑑𝑝[𝑖−1][𝑡][𝑘],\n𝑑𝑝[𝑖][𝑡−𝑡𝑖][𝑘−1]+𝑒𝑖),(𝑡𝑖·𝑘≤𝑡)(2)\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "Algorithm 1 Finding Optimal Data Placement with DP.\n1:function KNAPSACK_MIN_ENERGY\n2: Set dp[i][t][k] =∞andcount[i][t][k] = 0 for all i,tandk.\n3: Set dp[i][t][0] = 0 for all iandt.\n4:forifrom 1 to𝑛/2:\n5:forkfrom 1 to K:\n6: fortfrom 1 to T:\n7: ifti≤tthen\n8: Calculate the possible previous state time index t−ti\n9: dp[i][t][k] =min(dp[i−1][t][k],dp[i][t−ti][k−1]+ei)\n10: Update count[i][t][k] based on the chosen minimum path.\n11: else\n12: dp[i][t][k] =dp[i-1][t][k]\n13: count[i][t][k] =count[i-1][t][",
  " =dp[i-1][t][k]\n13: count[i][t][k] =count[i-1][t][k]\n14: end if\n15: end for\n16: end for\n17:end for\n18:end function\nwhere𝑖,𝑡, and𝑘denote the current storage space being considered,\nthe time constraint, and the number of weights, respectively. The\nvalue𝑑𝑝[𝑖][𝑡][𝑘]represents the minimum energy consumption re-\nquired to store exactly 𝑘weight data across the first 𝑖storage spaces\nwhile satisfying the time constraint 𝑡. In this equation, when 𝑡𝑖·𝑘 >𝑡,\nstoring𝑘weights in the current storage space excee",
  "storing𝑘weights in the current storage space exceeds the time\nconstraint. Therefore, in this case, the value from the previous storage\nspace𝑑𝑝[𝑖−1][𝑡][𝑘]is carried forward. Conversely, if 𝑡𝑖·𝑘≤𝑡,\nthe recurrence considers the minimum between 𝑑𝑝[𝑖−1][𝑡][𝑘]and\n𝑑𝑝[𝑖][𝑡−𝑡𝑖][𝑘−1]+𝑒𝑖, where the latter term represents the energy\nconsumption if one additional weight is stored in the current storage\nspace𝑖, based on the previous results.\nMeanwhile, in implementing the solution based on (2), it is\nessentia",
  "ementing the solution based on (2), it is\nessential to consider the architectural constraints of HH-PIM. HH-\nPIM allows for parallel execution of weight data distributed between\nHP-PIM and LP-PIM clusters; however, parallel processing is not\nfeasible for weights stored across MRAM and SRAM within each\nmodule. To address this, we propose a partitioned approach where the\nDP algorithm is applied separately to HP-PIM and LP-PIM clusters,\ngenerating individual DP tables for each cluster. By combining",
  "ndividual DP tables for each cluster. By combining these\ntables, we identify the optimal allocation of weights that minimizes\nenergy consumption within the time constraint, yielding the best\ncombination of 𝑘values, (𝑘hp,𝑘lp), where𝑘hpand𝑘lprepresent\nthe number of weights assigned to HP-PIM and LP-PIM clusters,\nrespectively. This approach effectively accounts for both inter-cluster\nparallelism and intra-module serialization, optimizing energy effi-\nciency while adhering to the operational constra",
  "-\nciency while adhering to the operational constraints of HH-PIM.\nFirst, Algorithm 1 presents the pseudo-code of the algorithm based\non (2). Here, 𝐾denotes the total number of weight data to be stored\nin HH-PIM, and count[i][t][k] is a variable used to trace the path to\nthe optimal energy state. Lines 2 and 3 set the base conditions for\nthe recurrence relation, while lines 7-13 correspond to the recurrence\nrelation defined in (2). Notably, because Algorithm 1 is performed\nseparately for both HP-",
  "e Algorithm 1 is performed\nseparately for both HP-PIM and LP-PIM, the iteration for 𝑖in line 4\nonly proceeds up to 𝑛/2.\nNext, Algorithm 2 finds the optimal combination of 𝑘values\nthat minimizes energy consumption using the results of the DP\ntablesdphpanddplpgenerated for HP-PIM and LP-PIM, respectively.\nHere,min_energy denotes the minimum energy consumption, while\nk_opt_hp andk_opt_lp represent the number of weights assigned to\nHP-PIM and LP-PIM at this minimum. As a result, Algorithm 2\nyieldsal",
  "at this minimum. As a result, Algorithm 2\nyieldsallocation_state [n][t], representing the distribution state of\nweight data for each time constraint. In the proposed method, both\nAlgorithm 1 and Algorithm 2 are performed only once during the\napplication initialization phase to construct a Look-up Table ( LUT)\nfor the final output, allocation_state . This LUT allows rapid determi-\nnation of the optimal weight placement state for varying 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡Algorithm 2 Finding Optimal ( 𝑘hp,𝑘lp).\n1:functio",
  "𝑡Algorithm 2 Finding Optimal ( 𝑘hp,𝑘lp).\n1:function SET_ALLOCATION_STATE\n2:fortfrom 0 to T:\n3: Set min_energy =∞,k_opt_hp =0,k_opt_lp =0\n4:forkhpfrom 1 to K:\n5: klp=𝐾−khp\n6: ifdphp[𝑛/2][t][khp]+dplp[𝑛/2][t][klp]<∞then\n7: ifmin_energy>dphp[𝑛/2][t][khp]+dplp[𝑛/2][t][klp]then\n8: min_energy =dphp[𝑛/2][t][khp]+dplp[𝑛/2][t][klp]\n9: k_opt_lp =klp\n10: end if\n11: end if\n12: ifmin_energy remains∞,continue to nextt\n13: k_opt_hp =K−k_opt_lp\n14: Set allocation_state [n][t] with current k_opt_hp andk_opt_lp\n1",
  "n_state [n][t] with current k_opt_hp andk_opt_lp\n15: end for\n16:end for\n17:end function\nTABLE I: Developed specifications for HH-PIM and other PIM\narchitectures.\nArchitecture PIM Module Configuration Memory Types (per module)\nBaseline-PIM 8 HP-PIM 128kB SRAM\nHeterogeneous-PIM 4 HP-PIM + 4 LP-PIM 128kB SRAM\nHybrid-PIM 8 HP-PIM 64kB MRAM + 64kB SRAM\nHH-PIM 4 HP-PIM + 4 LP-PIM 64kB MRAM + 64kB SRAM\nvalues required at each time slice during application runtime. Further-\nmore, the calculation of 𝑡𝑐𝑜𝑛",
  "on runtime. Further-\nmore, the calculation of 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 at runtime incorporates the data\nmovement overhead time needed for transitioning from the previous\ntime slice data allocation to the new one, ensuring no inference delay\narises due to data movement overhead.\nThe time complexities of Algorithm 1 and Algorithm 2 are\n𝑂(𝑛·𝑇·𝐾)and𝑂(𝑇·𝐾), respectively. Although these algorithms\nare executed only once during application initialization, the full\ntime slice range for𝑇and the large number of weight",
  "ime slice range for𝑇and the large number of weight data 𝐾,\noften tens to hundreds of thousands in lightweight neural network\nmodels, may impose considerable overhead for energy optimization\nacross all points in an edge processor. To mitigate this, we limit the\nresolution of optimization to ensure that the total computation time\ndoes not exceed 1% of each time slice , thereby avoiding excessive\nfine-grained calculations. The resulting optimal data distribution and\nimprovements in energy efficienc",
  " distribution and\nimprovements in energy efficiency are presented in the following\nsection, along with detailed experimental settings.\nIV. Experimental Work\nA. Development and Evaluation of PIM Processors for Power and\nPerformance Assessment\nTo evaluate the effectiveness of the proposed HH-PIM architecture\nand its data placement optimization strategy, we first implemented\nthe HH-PIM architecture from Fig. 1 at the RTL level. The HP\nand LP clusters were each configured with four HP-PIM and four\nL",
  "s were each configured with four HP-PIM and four\nLP-PIM modules, with each PIM module equipped with 64kB of\nMRAMandSRAM.Forafairandprecisecomparison,weestablished\nBaseline-PIM, Heterogeneous-PIM (Hetero.-PIM), and Hybrid-PIM\nas comparison groups, each also implemented at the RTL level.\nTable I presents the specifications of each comparison group.\nSubsequently,wedevelopedprocessorsequippedwiththeproposed\nHH-PIM and each of the comparative PIM architectures. RTL design\nand simulation were conducte",
  "itectures. RTL design\nand simulation were conducted using RISC-V eXpress (RVX) [28],\n[29].ThediagramontherightsideofFig.3illustratesthearchitecture\nof the processor equipped with HH-PIM, which is built with a single\nRISC-V Rocket [30] core. To facilitate efficient transmission and re-\nceptionoflarge-scaleAIapplicationdata,theHH-PIMcommunicates\nwith the core through the AXI protocol, offering high bandwidth and\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026",
  "Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "HH-PIM Processor\n(Prototyped on Genesys 2 FPGA Board)\nSystem InterconnectJTAG\nFLASHIROMAPBRocket\nRISC-V\nCoreHH-PIMSRAM\nSystem \nMemoryI2C\nUARTSPIAPB\nPeripheralsAXI AXIHH-PIM Processor Architecture\nAXIFig. 3: Design and FPGA prototyping of the HH-PIM processor.\nTABLE II: FPGA prototype resource utilization.\nIPs LUTs FFs BRAMs DSPs\nRISC-V Rocket Core 14,998 9,762 12 4\nPeripherals 4,704 7,159 - -\nSystem Interconnect 5,237 7,720 - -\nHP-PIM Module 968 1,055 32 2\nHP-PIM Module Controller 2,823 875 - -\n",
  "1,055 32 2\nHP-PIM Module Controller 2,823 875 - -\nTotal (HP-PIM module cluster) 6,951 5,460 128 8\nLP-PIM Module 1,074 1,094 32 2\nLP-PIM Module Controller 2,149 875 - -\nTotal (LP-PIM module cluster) 6,680 5,616 128 8\nlow latency. For the system interconnect, we utilized 𝜇𝑁𝑜𝐶[31], a\nlightweight Network-on-Chip optimized for edge devices.\nNext, we performed functional verification and performance eval-\nuation through FPGA prototyping of the developed processors. First,\nwe modeled the operation and ",
  "d processors. First,\nwe modeled the operation and latency of SRAM at the RTL level,\nwith Table III reporting the read/write latencies used in the model.\nThese results were obtained using the NVSim simulation tool [32] at\na 45nm process, with different operating voltages of 1.2V for HP-PIM\nand 0.8V for LP-PIM. The 0.8V operating voltage of LP-MRAM, in\nparticular, is based on recent specifications of fabricated STT-MRAM\nchips [11], [33]. We then programmed the developed processors on a\nGenesys2 Ki",
  "ogrammed the developed processors on a\nGenesys2 Kintex-7 FPGA board [34]. The operating clock frequency\nof the prototypes was set to 50 MHz, and memory latencies were\nscaled according to Table III. The photograph on the right side of\nFig. 3 shows the prototype of the HH-PIM processor, while Table II\nreports the resource consumption of the prototype.\nWe verified the correct operation of the FPGA prototypes and\nmeasured performance metrics by running benchmark applications,\nexpecting similar resul",
  "ng benchmark applications,\nexpecting similar results to those observed on an SoC operating at\n50 MHz [28]. For the AI benchmark applications, we used TinyML\nmodels based on CNN deep learning backbones such as EfficientNet-\nB0 [35], MobileNetV2 [36], and ResNet-18 [37], which are suitable\nfor edge AI devices. We extracted the characteristics and operations\nof these models, calculated the proportion of PIM operations relative\nto the total operations, and incorporated these ratios into the bench-\nm",
  "s, and incorporated these ratios into the bench-\nmark applications. The time slice for performing the data placement\nalgorithm in each TinyML model was set to allow up to 10 inferences\npertime slice , representing the scenario in which HH-PIM operates\nat maximum performance. Table IV reports the characteristics of the\nmodels incorporated into the benchmark applications. Additionally, to\nreflectdynamiccomputationalloadvariationsencounteredatruntime,\nwe configured six workload scenarios as shown i",
  "e,\nwe configured six workload scenarios as shown in Fig. 4. These\nscenarios include a consistently low workload pattern (Case 1), a\nconsistently high workload pattern (Case 2), periodic spike patterns\n(Case 3 and 4), a pulsing pattern with alternating high and low\nTABLE III: Latency comparison of HP-PIM and LP-PIM modules.\nLatency\n(𝑛𝑠)STT-MRAM SRAMPE\nRead Write Read Write\nHP-PIM (𝑉𝑑𝑑=1.2𝑉)2.62 11.81 1.12 1.12 5.52\nLP-PIM (𝑉𝑑𝑑=0.8𝑉)2.96 14.65 1.41 1.41 10.68TABLE IV: TinyML model specs and PIM op",
  " 1.41 10.68TABLE IV: TinyML model specs and PIM operation ratios.\nModel # Param # MAC PIM Operation –\nEfficientNet-B0 95k 3.245M 85%INT8 Quantized\n& PrunedMobileNetV2 101k 2.528M 80%\nResNet-18 256k 29.580M 75%\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni0000002f/uni00000052/u",
  "/uni00000044/uni00000047/uni0000002f/uni00000052/uni0000005a/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057\n(a) Case 1\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni0000",
  "000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni0000002b/uni0000004c/uni0000004a/uni0000004b/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057 (b) Case 2\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/u",
  "/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni00000033/uni00000048/uni00000055/uni0000004c/uni00000052/uni00000047/uni0000004c/uni00000046/uni00000003/uni00000036/uni00000053/uni0000004c/uni0000004e/uni00000048/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051\n(c) Case 3\n/uni00000037/uni0000004c/uni0000",
  "000051\n(c) Case 3\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni00000033/uni00000048/uni00000055/uni0000004c/uni00000052/uni00000047/uni0000004c/uni00000046/uni00000003/uni00000036/uni00000053/uni0000004c/uni0000004e/uni00000048/uni00000003/uni00000033/uni00000044/uni00000057/u",
  "/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051/uni00000003/uni0000000b/uni00000049/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000057/uni0000000c (d) Case 4\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni0000",
  "000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni0000002b/uni0000004c/uni0000004a/uni0000004b/uni00000010/uni0000002f/uni00000052/uni0000005a/uni00000003/uni00000033/uni00000058/uni0000004f/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051\n(e) Case 5\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/u",
  "/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000044/uni00000047/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047 (f) Case 6\nFig. 4: Various workload scenarios of the AI benchmark app.\nTABLE V: Power consumption ( 𝑚𝑊) across memory types in HP-\nPIM (1.2V) and LP-PI",
  "𝑊) across memory types in HP-\nPIM (1.2V) and LP-PIM (0.8V).\nPowerSTT-MRAM SRAM PE\nDynamic\n(Read/Write)StaticDynamic\n(Read/Write)Static Dynamic Static\nHP-PIM 428.48 / 133.78 2.98 508.93 / 500 23.29 0.9 0.48\nLP-PIM 179.05 / 47.78 0.84 177.3 / 177.3 5.45 0.51 0.25\nworkloads (Case 5), and a random workload pattern (Case 6). The\nspikeandpulsepatternssimulaterealisticscenariosinAIapplications\non edge devices, where computational demands periodically surge.\nTo measure the power consumption of the devel",
  "rge.\nTo measure the power consumption of the developed processors,\nwe then synthesized them at a 45nm process technology. The RTL\ndesign, excluding memory, was synthesized using Synopsys Design\nCompiler with the 45nm Nangate PDK [38], while the memory\npower values were obtained through NVSim simulations at 45nm\ntechnology, as previously described. Table V details the power\nconsumption of each memory type. These values were subsequently\ncombinedwithbenchmark-specificexecutiontimes,asmeasuredfrom\n",
  "thbenchmark-specificexecutiontimes,asmeasuredfrom\nFPGA prototypes, to calculate total energy consumption. A detailed\nanalysis of energy consumption, along with energy-saving results\nacross various benchmark scenarios shown in Fig. 5, is discussed\nfurther in the following section.\nB. Comprehensive Analysis of Energy Savings\nFig. 6 reports the results of applying the proposed data placement\noptimization algorithm to benchmark applications executed on HH-\nPIM. The x-axis represents the 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 ",
  "on HH-\nPIM. The x-axis represents the 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 within a time slice (𝑇),\nwhile the left y-axis indicates the memory utilization determined\nby the optimal data placement under the given 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 , and\nthe right y-axis represents the corresponding energy consumption\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "/uni0000002f/uni00000052/uni0000005a/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057 /uni0000002b/uni0000004c/uni0000004a/uni0000004b/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000044/uni000",
  "0000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000057 /uni00000033/uni00000048/uni00000055/uni0000004c/uni00000052/uni00000047/uni0000004c/uni00000046/uni00000003/uni00000036/uni00000053/uni0000004c/uni0000004e/uni00000048/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051 /uni00000033/uni00000048/uni00000055/uni0000004c/uni00000052/uni00000047/uni0000004c/uni00000046/uni00000003/uni00000036/uni00000053/uni0000004c/uni0000004e/uni0000004",
  "036/uni00000053/uni0000004c/uni0000004e/uni00000048/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051/uni00000003/uni0000000b/uni00000049/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000057/uni0000000c /uni0000002b/uni0000004c/uni0000004a/uni0000004b/uni00000010/uni0000002f/uni00000052/uni0000005a/uni00000003/uni00000033/uni00000058/uni0000004f/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni",
  "ni00000051/uni0000004a/uni00000003/uni00000033/uni00000044/uni00000057/uni00000057/uni00000048/uni00000055/uni00000051 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004e/uni0000004f/uni00000052/uni00000044/uni00000047/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000028/uni00000051/uni00000048/uni00000055/uni0000004a/uni0000005c/uni00000003/uni00000",
  "00055/uni0000004a/uni0000005c/uni00000003/uni00000036/uni00000044/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/",
  "8/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni",
  "ni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni000",
  "0000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni000000",
  "0057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/",
  "b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047\n/uni00000059/uni00000056/uni00000011/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni00000048/uni00000057/uni",
  "ni00000003/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056\n/uni00000059/uni00000056/uni00000011/uni00000003/uni0000002b/uni0000005c/uni00000045/uni00000055/uni0000004c/uni00000047/uni00000037/uni0000004c/uni00000051/uni0000005c/uni00000030/uni0000002f/uni00000003/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000028/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000",
  "000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000031/uni00000048/uni00000057/uni00000010/uni00000025/uni00000013\n/uni00000050/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000031/uni00000048/uni00000057/uni00000039/uni00000015\n/uni00000035/uni00000048/uni00000056/uni00000031/uni00000048/uni00000057/uni00000010/uni00000014/uni0000001bFig. 5: Energy savings of the HH-PIM over the Baseline-, Heterogeneous-, and Hybrid-PIM across b",
  "Baseline-, Heterogeneous-, and Hybrid-PIM across benchmark scenarios.\nPeak performance point of HH-PIM\n𝒕𝒂𝒔𝒌(Normalized)Memory Utilization Ratio (%)\nTHH-PIM peak with weights in \nMRAM only (as in H-PIM)\nNot Possible\nHP-SRAM\nHP-MRAM\nLP-SRAM\nLP-MRAM\n௧௔௦௞(Normalized)\nFig. 6: Memory utilization and energy consumption across\n𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 withintime slice (𝑇) based on HH-PIM’s optimized data\nplacement.\n𝐸𝑡𝑎𝑠𝑘of HH-PIM. The gray region in the graph denotes cases\nwhere𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 is too small, i.e., the re",
  " cases\nwhere𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 is too small, i.e., the required performance is too\nhigh for HH-PIM to handle. The green dot marks the point where\nHH-PIM operates at peak performance, satisfying the application’s\nperformance requirements. At this point, HH-PIM actively utilizes\nSRAMs for data storage, with the total neural network data stored\nin a 16:9 ratio between HP-SRAM and LP-SRAM, minimizing PIM\nmodule idle times. The inference times at this point are measured\nas 31.06𝑚𝑠, 25.71𝑚𝑠, and 320.87 𝑚𝑠for",
  "are measured\nas 31.06𝑚𝑠, 25.71𝑚𝑠, and 320.87 𝑚𝑠for the EfficientNet-B0, Mo-\nbileNetV2, and ResNet-18 benchmarks, respectively. As expected,\nenergy consumption peaks at this point, and 𝐸𝑡𝑎𝑠𝑘is normalized\nto this value in the graph. Additionally, the purple dot indicates the\npeak performance point of HH-PIM when weights are stored only in\nMRAM(asinpreviousH-PIMs).Atthispoint,theinferencetimesfor\nthe EfficientNet-B0, MobileNetV2, and ResNet-18 benchmarks are\n44.5𝑚𝑠, 36.84𝑚𝑠, and 459.74 𝑚𝑠, respecti",
  "marks are\n44.5𝑚𝑠, 36.84𝑚𝑠, and 459.74 𝑚𝑠, respectively. This demonstrates that\nutilizing both SRAM and MRAM for weight storage, as proposed,\noutperforms the traditional method in terms of performance.\nFig. 6 demonstrates the sequential allocation of weight data across\nmemory types as 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 increases. Specifically, the distribution\nprogresses through combinations of HP-SRAM and LP-MRAM, HP-\nMRAM and LP-SRAM, LP-SRAM alone, and finally LP-MRAM\nalone. This sequence reflects the balance betwee",
  "M\nalone. This sequence reflects the balance between performance\nand energy efficiency across memory types. In the range where\n𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 is longest, all weight data are stored exclusively in LP-\nMRAM, which operates at minimal power. In this configuration,\nothermemorytypesaredeactivatedthroughpower-gating,eliminating\nstandby power and maximizing the processor’s energy efficiency.\nConsequently,inthishighlyefficientregion,HH-PIMachievesuptoa\n43.17% reduction in 𝐸𝑡𝑎𝑠𝑘compared to unoptimized data a",
  "% reduction in 𝐸𝑡𝑎𝑠𝑘compared to unoptimized data allocation.\nThis significant reduction underscores the practical value of the\nproposed algorithm in real-world edge AI applications. Additionally,\nin the mid-range of 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 ,𝐸𝑡𝑎𝑠𝑘exhibits a quasi-linear decline\nwith intermittent plateaus as 𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 increases. This pattern\nreflects the algorithm’s capability to shift data progressively to lower-TABLE VI: Energy Savings ( 𝐸𝑆) by HH-PIM for Cases 3–6.\n𝑬𝑺(%)\noverBaseline- Hetero.-H-PIMPIM PI",
  "Cases 3–6.\n𝑬𝑺(%)\noverBaseline- Hetero.-H-PIMPIM PIM\nCase 3: Periodic Spike 72.01 55.78 54.09\nCase 4: Periodic Spike (frequent) 61.46 38.38 47.60\nCase 5: High-Low Pulsing 48.94 16.89 42.10\nCase 6: Random 59.28 34.14 50.52\npower memory while meeting inference latency constraints. These\nresults demonstrate that the proposed solution achieves optimal en-\nergy efficiency within the constraints of application inference latency\nfor each𝑡𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡 , ensuring that HH-PIM can dynamically allocate\ndata to",
  "uring that HH-PIM can dynamically allocate\ndata to balance energy savings and performance effectively.\nFig. 5 presents the energy savings achieved by HH-PIM compared\nto Baseline-PIM, Hetero.-PIM, and H-PIM (Hybrid-PIM) when exe-\ncuting the benchmark application across 50 time slices under various\ninference pattern scenarios from Fig. 4. The best-case scenario for\nHH-PIM is Case 1 (low workload constant), where the inference\npattern remains low across all intervals. In this scenario, HH-PIM\nachie",
  "ross all intervals. In this scenario, HH-PIM\nachieved energy savings of up to 86.23%, 78.7%, and 66.5% com-\npared to Baseline-, Hetero.-, and H-PIM, respectively. Conversely,\nthe worst-case scenario occurs in Case 2 (high workload constant),\nwhere the inference pattern remains consistently high. Here, HH-\nPIM still delivered notable energy savings of up to 41.46% and\n39.69% compared to Baseline- and H-PIM, respectively. However,\nsavings against Hetero.-PIM were limited to 3.72%, as both HH-PIM\na",
  "etero.-PIM were limited to 3.72%, as both HH-PIM\nand Hetero.-PIM primarily utilize HP-SRAM and LP-SRAM in this\nscenario, resulting in minimal differences. Detailed energy savings\nfor the other cases are reported in Table VI. HH-PIM achieved the\nhighest energy savings over the baseline in ResNet-18, with up to\n8.59% difference across cases. On average, HH-PIM achieved energy\nsavings of up to 60.43%, 36.3%, and 48.58% compared to Baseline-\nPIM, Hetero.-PIM, and H-PIM, respectively.\nV. Conclusion\nI",
  "ero.-PIM, and H-PIM, respectively.\nV. Conclusion\nIn this study, we proposed the HH-PIM architecture designed to\nefficiently execute AI applications on edge devices. The architec-\nture integrates HP MRAM-SRAM PIM modules and LP MRAM-\nSRAM PIM modules to achieve a balance between performance\nand energy efficiency. Additionally, we introduced a data placement\noptimization algorithm that dynamically allocates data based on\ncomputationaldemand,maximizingenergysavingswhilemaintaining\napplication perfo",
  "ingenergysavingswhilemaintaining\napplication performance requirements. To validate the effectiveness\nof the proposed architecture and algorithm, we designed processors\nincorporating HH-PIM and established comparison baselines with\nprocessors equipped with Baseline-, Heterogeneous-, and Hybrid-\nPIM architectures. These designs were evaluated through FPGA\nprototyping and power simulations. By running various edge AI\napplication benchmarks, the proposed HH-PIM demonstrated its\nsuperiority, achievin",
  "osed HH-PIM demonstrated its\nsuperiority, achieving up to 60.43% average energy savings com-\npared to conventional PIM architectures, while meeting application\nperformance requirements.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "References\n[1] M. Shafique, T. Theocharides, V. J. Reddy, and B. Murmann, “TinyML:\nCurrent progress, research challenges, and future roadmap,” in 2021 58th\nACM/IEEE Design Automation Conference (DAC) . IEEE, 2021, pp.\n1303–1306.\n[2] Y. Dong, T. Jia, K. Du, Y. Jing, Q. Wang, P. Zhan, Y. Zhang, F. Yan,\nY. Ma, Y. Liang et al., “A model-specific end-to-end design methodol-\nogy for resource-constrained TinyML hardware,” in ACM/IEEE Design\nAutomation Conference (DAC) . IEEE, 2023, pp. 1–6.\n[3] Q. Zhen",
  "onference (DAC) . IEEE, 2023, pp. 1–6.\n[3] Q. Zheng, S. Li, Y. Wang, Z. Li, Y. Chen, and H. H. Li, “Accelerating\nsparseattentionwithareconfigurablenon-volatileprocessing-in-memory\narchitecture,” in 2023 60th ACM/IEEE Design Automation Conference\n(DAC). IEEE, 2023, pp. 1–6.\n[4] X. Yang, K. Zhu, X. Tang, M. Wang, M. Zhan, N. Lu, J. P. Kulkarni,\nD.Z.Pan,Y.Liu,andN.Sun,“Anin-memory-computingcharge-domain\nternary CNN classifier,” IEEE Journal of Solid-State Circuits , vol. 58,\nno. 5, pp. 1450–1461, 2",
  "-State Circuits , vol. 58,\nno. 5, pp. 1450–1461, 2023.\n[5] Y.-D. Chih, P.-H. Lee, H. Fujiwara, Y.-C. Shih, C.-F. Lee, R. Naous,\nY.-L. Chen, C.-P. Lo, C.-H. Lu, H. Mori et al., “16.4 an 89TOPS/W\nand 16.3 TOPS/mm2 all-digital SRAM-based full-precision compute-\nin memory macro in 22nm for machine-learning edge applications,” in\nIEEE International Solid-State Circuits Conference (ISSCC) , vol. 64.\nIEEE, 2021, pp. 252–254.\n[6] J. Heo, J. Kim, S. Lim, W. Han, and J.-Y. Kim, “T-PIM: An energy-\nefficien",
  "W. Han, and J.-Y. Kim, “T-PIM: An energy-\nefficient processing-in-memory accelerator for end-to-end on-device\ntraining,” IEEE Journal of Solid-State Circuits , vol. 58, no. 3, pp. 600–\n613, 2022.\n[7] K. Park, H. Jeong, S. Kim, J. Shin, M. Kim, and K. J. Lee, “A 701.7\nTOPS/W Compute-in-Memory processor with time-domain computing\nfor spiking neural network,” IEEE Transactions on Circuits and Systems\nI: Regular Papers , pp. 1–11, 2024.\n[8] K. Lee, S. Jeon, K. Lee, W. Lee, and M. Pedram, “Radar-PIM:",
  ". Jeon, K. Lee, W. Lee, and M. Pedram, “Radar-PIM:\nDeveloping IoT processors utilizing Processing-in-Memory architecture\nfor ultra-wideband radar-based respiration detection,” IEEE Internet of\nThings Journal , 2024.\n[9] J.-W. Su, X. Si, Y.-C. Chou, T.-W. Chang, W.-H. Huang, Y.-N. Tu,\nR. Liu, P.-J. Lu, T.-W. Liu, J.-H. Wang et al., “15.2 A 28nm 64Kb\ninference-training two-way transpose multibit 6T SRAM compute-in-\nmemory macro for AI edge chips,” in IEEE International Solid-State\nCircuits Confere",
  "in IEEE International Solid-State\nCircuits Conference-(ISSCC) . IEEE, 2020, pp. 240–242.\n[10] S. Lee, S.-h. Kang, J. Lee, H. Kim, E. Lee, S. Seo, H. Yoon, S. Lee,\nK. Lim, H. Shin et al., “Hardware architecture and software stack for\nPIM based on commercial DRAM technology: Industrial product,” in\nACM/IEEE 48th Annual International Symposium on Computer Archi-\ntecture (ISCA) . IEEE, 2021, pp. 43–56.\n[11] Y.-C. Chiu, W.-S. Khwa, C.-Y. Li, F.-L. Hsieh, Y.-A. Chien, G.-Y. Lin,\nP.-J. Chen, T.-H. Pan,",
  "eh, Y.-A. Chien, G.-Y. Lin,\nP.-J. Chen, T.-H. Pan, D.-Q. You, F.-Y. Chen et al., “A 22nm 8Mb STT-\nMRAM near-memory-computing macro with 8b-precision and 46.4-\n160.1 TOPS/W for edge-ai devices,” in 2023 IEEE International Solid-\nState Circuits Conference (ISSCC) . IEEE, 2023, pp. 496–498.\n[12] H. Cai, Z. Bian, Y. Hou, Y. Zhou, Y. Guo, X. Tian, B. Liu, X. Si,\nZ. Wang, J. Yang et al., “33.4 A 28nm 2Mb STT-MRAM computing-\nin-memory macro with a refined bit-cell and 22.4-41.5 TOPS/W for ai\ninference,",
  "ed bit-cell and 22.4-41.5 TOPS/W for ai\ninference,” in 2023 IEEE International Solid-State Circuits Conference\n(ISSCC). IEEE, 2023, pp. 500–502.\n[13] C.-X. Xue, Y.-C. Chiu, T.-W. Liu, T.-Y. Huang, J.-S. Liu, T.-W. Chang,\nH.-Y. Kao, J.-H. Wang, S.-Y. Wei, C.-Y. Lee et al., “A CMOS-integrated\ncompute-in-memory macro based on resistive random-access memory\nfor AI edge devices,” Nature Electronics , vol. 4, no. 1, pp. 81–90, 2021.\n[14] T. Yang, D. Li, Y. Han, Y. Zhao, F. Liu, X. Liang, Z. He, and L.",
  ", Y. Han, Y. Zhao, F. Liu, X. Liang, Z. He, and L. Jiang,\n“PIMGCN: A ReRAM-based PIM design for graph convolutional net-\nworkacceleration,”in ACM/IEEEDesignAutomationConference(DAC) .\nIEEE, 2021, pp. 583–588.\n[15] Y. Zhu, Z. Zhu, G. Dai, F. Tu, H. Sun, K.-T. Cheng, H. Yang, and\nY. Wang, “Pim-hls: An automatic hardware generation tool for hetero-\ngeneous processing-in-memory-based neural network accelerators,” in\nACM/IEEE Design Automation Conference (DAC) . IEEE, 2023, pp.\n1–6.\n[16] H. Kim, H. Y",
  "nce (DAC) . IEEE, 2023, pp.\n1–6.\n[16] H. Kim, H. Ye, T. Mudge, R. Dreslinski, and N. Talati, “RecPIM: A\nPIM-enabled DRAM-RRAM hybrid memory system for recommenda-\ntion models,” in IEEE/ACM International Symposium on Low Power\nElectronics and Design (ISLPED) . IEEE, 2023, pp. 1–6.\n[17] F. Zhang, A. Sridharan, W. Tsai, Y. Chen, S. X. Wang, and D. Fan,\n“Efficient memory integration: MRAM-SRAM hybrid accelerator for\nsparse on-device learning,” in ACM/IEEE Design Automation Confer-\nence, 2024, pp. 1–",
  "/IEEE Design Automation Confer-\nence, 2024, pp. 1–6.\n[18] W. Lee, Y. Wang, and M. Pedram, “VRCon: Dynamic reconfiguration\nof voltage regulators in a multicore platform,” in Design, Automation &\nTest in Europe Conference & Exhibition (DATE) , 2014, pp. 1–6.[19] S. K. Panda, M. Lin, and T. Zhou, “Energy-efficient computation\noffloading with DVFS using deep reinforcement learning for time-critical\niot applications in edge computing,” IEEE Internet of Things Journal ,\nvol. 10, no. 8, pp. 6611–6621, ",
  "f Things Journal ,\nvol. 10, no. 8, pp. 6611–6621, 2022.\n[20] W. Lee, Y. Wang, and M. Pedram, “Optimizing a reconfigurable power\ndistribution network in a multicore platform,” IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems , vol. 34,\nno. 7, pp. 1110–1123, 2015.\n[21] H. Bouzidi, M. Odema, H. Ouarnoughi, M. A. Al Faruque, and S. Niar,\n“HADAS: Hardware-aware dynamic neural architecture search for edge\nperformance scaling,” in Design, Automation & Test in Europe Confer-",
  "g,” in Design, Automation & Test in Europe Confer-\nence & Exhibition (DATE) . IEEE, 2023, pp. 1–6.\n[22] K. Lee, K. Bong, C. Kim, J. Park, and H.-J. Yoo, “An energy-efficient\nparallel multi-core ADAS processor with robust visual attention and\nworkload-prediction DVFS for real-time HD stereo stream,” in IEEE\nSymposium in Low-Power and High-Speed Chips (COOL CHIPS XIX) ,\n2016, pp. 1–3.\n[23] J. Park, E. Choi, K. Lee, J.-J. Lee, K. Han, and W. Lee, “Developing an\nultra-low power RISC-V processor for ",
  "eveloping an\nultra-low power RISC-V processor for anomaly detection,” in Design,\nAutomation & Test in Europe Conference & Exhibition (DATE) , 2023,\npp. 1–2.\n[24] E. Choi, J. Park, K. Lee, J.-J. Lee, K. Han, and W. Lee, “Day–Night\narchitecture: Development of an ultra-low power RISC-V processor for\nwearable anomaly detection,” Journal of Systems Architecture , vol. 152,\np. 103161, 2024.\n[25] ARM, https://www.arm.com/technologies/big-little, Accessed 19 11.\n2024.\n[26] J. H. Kim, S.-H. Kang, S. Lee",
  "ed 19 11.\n2024.\n[26] J. H. Kim, S.-H. Kang, S. Lee, H. Kim, Y. Ro, S. Lee, D. Wang, J. Choi,\nJ. So, Y. Cho et al., “Aquabolt-XL HBM2-PIM, LPDDR5-PIM with\nin-memory processing, and AXDIMM with acceleration buffer,” IEEE\nMicro, vol. 42, no. 3, pp. 20–30, 2022.\n[27] H. Kellerer, U. Pferschy, and D. Pisinger, Knapsack Problems . Springer,\n2004.\n[28] K. Han, S. Lee, K.-I. Oh, Y. Bae, H. Jang, J.-J. Lee, W. Lee, and\nM. Pedram, “Developing TEI-aware ultralow-power SoC platforms for\nIoT end nodes,” IEEE",
  "ralow-power SoC platforms for\nIoT end nodes,” IEEE Internet of Things Journal , vol. 8, no. 6, pp.\n4642–4656, 2020.\n[29] H. Jang, K. Han, S. Lee, J.-J. Lee, S.-Y. Lee, J.-H. Lee, and W. Lee,\n“Developing a multicore platform utilizing open RISC-V cores,” IEEE\nAccess, vol. 9, pp. 120010–120023, 2021.\n[30] SiFIVE, https://github.com/chipsalliance/rocket-chip, Accessed 19 11.\n2024.\n[31] K. Han, S. Lee, J.-J. Lee, W. Lee, and M. Pedram, “TIP: A temperature\neffect inversion-aware ultra-low power syste",
  "ature\neffect inversion-aware ultra-low power system-on-chip platform,” in\nIEEE/ACM International Symposium on Low Power Electronics and\nDesign (ISLPED) , 2019, pp. 1–6.\n[32] X. Dong, C. Xu, Y. Xie, and N. P. Jouppi, “Nvsim: A circuit-level\nperformance, energy, and area model for emerging nonvolatile memory,”\nIEEETransactionsonComputer-AidedDesignofIntegratedCircuitsand\nSystems, vol. 31, no. 7, pp. 994–1007, 2012.\n[33] P.-H. Lee, C.-F. Lee, Y.-C. Shih, H.-J. Lin, Y.-A. Chang, C.-H. Lu,\nY.-L. Chen",
  "Shih, H.-J. Lin, Y.-A. Chang, C.-H. Lu,\nY.-L. Chen, C.-P. Lo, C.-C. Chen, C.-H. Kuo et al., “33.1 A 16nm\n32Mb embedded STT-MRAM with a 6ns read-access time, a 1M-cycle\nwrite endurance, 20-year retention at 150 °c and MTJ-OTP solutions\nfor magnetic immunity,” in IEEE International Solid-State Circuits\nConference (ISSCC) . IEEE, 2023, pp. 494–496.\n[34] Genesys2, https://digilent.com/shop/genesys-2-kintex-7-fpga-\ndevelopment-board, Accessed 19 11. 2024.\n[35] M. Tan and Q. Le, “Efficientnet: Rethink",
  "024.\n[35] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for con-\nvolutional neural networks,” in International conference on machine\nlearning. PMLR, 2019, pp. 6105–6114.\n[36] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\n2018, pp. 4510–4520.\n[37] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” ",
  ", “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[38] NCSU, https://eda.ncsu.edu/freepdk/freepdk45, Accessed 19 11. 2024.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:19 UTC from IEEE Xplore.  Restrictions apply.",
  "EENet: Energy Efficient Neural Networks with Run-time\nPower Management\nXiangjie Li*, Yingtao Shen*, An Zou, Yehan Ma\nShanghai Jiao Tong University\nAbstract —Deep learning approaches, such as convolution neural net-\nworks (CNNs), have achieved tremendous success in versatile applications.\nHowever, one of the challenges to deploy the deep learning models\non resource-constrained systems is its huge energy cost. As a dynamic\ninference approach, early exit adds exiting layers to the networks, which\nc",
  " exit adds exiting layers to the networks, which\ncan terminate the inference earlier with accurate results to save energy.\nThe current passive decision-making for energy regulation of early exit\ncannot adapt to ongoing inference status, varying inference workloads,\nand timing constraints, let alone guide the reasonable configuration of\nthe computing platforms alongside the inference proceeds for potential\nenergy saving. In this paper, we propose an Energy Efficient Neural\nNetworks (EENet) , whic",
  "an Energy Efficient Neural\nNetworks (EENet) , which introduces a plug-in module to the state-of-\nthe-art networks by incorporating run-time power management. Within\neach inference, we establish prediction of where the network will exit and\nadjust computing configurations (i.e., frequency and voltage) accordingly\nover a small timescale. Considering multiple inferences over a large\ntimescale, we provide frequency and voltage calibration advice, given\ninference workloads and timing constraints. Fin",
  "en\ninference workloads and timing constraints. Finally, the dynamic voltage\nand frequency scaling (DVFS) governor configures voltage and frequency\nto execute the network according to the prediction and calibration.\nExtensive experimental results demonstrate that EENet achieves up to\n63.8% energy-saving compared with classic deep learning networks and\n21.5% energy-saving compared with the early exit under state-of-the-art\nexiting strategies, together with improved timing performance.\nIndex Terms ",
  "her with improved timing performance.\nIndex Terms —Neural Networks, Early Exit, Energy Efficiency, Inference\nTime, Feedback Control\nI. I NTRODUCTION\nNeural networks, such as convolution neural networks (CNNs),\nhave achieved tremendous success in versatile applications, such as\naudio perception and object detection [1], [2]. It is indispensable to\ndeploy trained neural networks to resource-constrained systems with\nlower costs, which should reduce the energy cost while satisfying\nthe computation l",
  "the energy cost while satisfying\nthe computation latency [3]. The computation latency determines\nthe quality of service (QoS) of neural network applications. At the\nsame time, energy consumption directly impacts the duration of the\nresource-constrained systems or the economic cost of computing [4].\nDiving into the performance of network inference cases, Wang et\nal. [5]–[7] found that the significant growth in model complexity is\nonly helpful in classifying a handful of complicated inputs correct",
  "lassifying a handful of complicated inputs correctly,\nand they might become “wasteful” for simple inputs. Motivated by\nthis observation, early exit [7]–[10] includes additional side branch\nclassifiers (exiting layers) to some of the network layers. Compared\nwith the classic deep learning network, the additional exiting layer\nallows inference results for a large portion of test samples to exit the\nnetwork early when samples have been inferred with high confidence.\nDespite being employed in burgeo",
  " high confidence.\nDespite being employed in burgeoning efforts to reduce computation\ncost and energy consumption in inference, the approaches mentioned\nabove are not capable of addressing the challenges listed below.\nFirst, the current passive decision-making for energy regulation of\nearly exit is triggered by passively checking the pre-placed exiting\nlayers, which can only reduce the computing voltage and frequency\nafter exiting [11]. During the inference process, run-time computing\n*Authors co",
  " inference process, run-time computing\n*Authors contributed equally to this research. This research project is\nsupported by NSFC 62103268, NSFC 62202287, and Shanghai Chenguang\nProgram 21CGA11. Yehan Ma is the corresponding author.configuration adjustment can not be applied ahead of time for potential\nenergy saving. Actually, adjusting computing configurations during\nthe inference is more effective than after, which will be explained in\nSec. IV-B. Second, due to the complex relationships among c",
  ". Second, due to the complex relationships among computing\ntime, inference accuracy, and computing cost, it is nontrivial to\nconfigure the computation platform to satisfy both computation time\nand inference accuracy to save energy during inference. Third, it\nis challenging to coordinate the requirements of inference accuracy\nand energy cost of inference at the small timescale, and the varying\ninference workloads and timing constraints over a longer time interval\nsimultaneously.\nIn this work, we ",
  "er time interval\nsimultaneously.\nIn this work, we propose EENet :Energy- Efficient Inference of\nNeural Networks, which addresses the above challenges. The main\ncontributions of EENet are summarized as follows.\n•Targeting energy-efficient neural networks, an EENet framework\nis presented, which dynamically adjusts the computing frequency\nand voltage according to the intra-inference exiting prediction,\ncross-inference workloads, and timing constraints;\n•An intra-inference prediction is proposed to ",
  "ts;\n•An intra-inference prediction is proposed to forecasts where an\ninference network will exit (i.e., establish the number of remain-\ning layers to finish this inference) and calculate the computing\nfrequency accordingly;\n•A cross-inference calibration mechanism is adopted after each\ninference to count the number of finished and remaining infer-\nences and the timing constraints to further calibrate the computing\nfrequency.\nExtensive experiments with different network models and datasets are\nco",
  " with different network models and datasets are\nconducted to demonstrate the significant energy saving under soft real-\ntime constraints.\nII. R ELATED WORK\nTargeting computing power/energy efficiency, power management\ntechniques, such as dynamic voltage and frequency scaling (DVFS),\nhave been recently explored on neural network inferences. For exam-\nple, Yao et al. [12] propose energy-aware adaptive scheduling for CNN\ninference on high-performance GPUs. By adaptively coordinating\nbatching and DV",
  "e GPUs. By adaptively coordinating\nbatching and DVFS according to fluctuating workloads, it minimizes\nenergy consumption and meets service-level latency. Nabavinejad et al.\n[13] present a BatchSizer, which controls the size of input batches fed\nto the GPU accelerator in DNN inference applications in the presence\nof the power cap. They develop power-inference accuracy trading,\nwhich combines DVFS and switches between low and high precision\nmodels to balance the performance, power, and accuracy. H",
  "to balance the performance, power, and accuracy. However,\nthese techniques are built upon completing the whole inference task,\ni.e., all the pre-defined neural network layers, in every inference,\nignoring the potential energy savings within each inference.\nThe early exits can terminate the inference early with accurate\nresults to reduce computation and energy costs [7]–[10]. Baccarelli\net al. [14], Panda et al. [15], and Ghodrati et al. [16] proposed\nan optimal exiting placement and an on-the-fl",
  "osed\nan optimal exiting placement and an on-the-fly supervision training\nmechanism to reach a dynamic tradeoff between accuracy and power.2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247701\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  "Execution time\n ………Coordination period     , DeadlineTask Release Task Release\nM sequential  inferences\n,\nit\niT\n1j\n2j\n3j\nMj\n1j\n2j\ncT\ncD\n,Fig. 1. The inference task model.\nTo further reduce the power and energy consumption, Tambe et al.\n[11] lessen the frequency and voltage of the process after the network\nexits and until the start of the next inference. Together with theoretical\nmodels, early exits have started being deployed in hardware and IoT\nsystems [17], [18]. Although early exits in these ",
  "systems [17], [18]. Although early exits in these studies can reduce\npower and energy costs, they suffer from a passive decision mode\nfor energy regulation. The corresponding adjustment of computation\nconfiguration could only be performed after the exiting happens.\nIn this work, we propose an active power management framework\ntailored for neural networks, EENet, which can dynamically adjust the\nprocessor voltage and frequency based on the active prediction of the\nearly exit points, and the calib",
  "prediction of the\nearly exit points, and the calibration based on inference workloads and\ntiming constraints.\nIII. S YSTEM MODELING\nA. Inference Task Model\nIn this paper, we consider inference tasks that are released and\nexecuted on a computing platform in a general burst manner [19],\nwhere Msequential network inferences ji, i∈ {1,2, ..., M }are\nreleased at one time, as shown in Fig. 1. This model could capture\nmost inference applications. For instance, it can describe only one\nnetwork inference",
  "stance, it can describe only one\nnetwork inference released each time by setting M= 1 , and 10\nsequential inferences released and executed at one time by setting\nM= 10 . The period and deadline for releasing and finishing these\nsequential inference tasks are defined as the coordination period Tc\nand coordination deadline Dc(which is equal to Tc). As the sequential\ntasks are under soft real-time constraints Tc, the occasional missing\ndeadlines will degrade the quality of service without breaking ",
  "l degrade the quality of service without breaking down\nthe system. Besides, the unfinished inference tasks will be delayed\nto the beginning of the next coordination period and become the\ninference tasks among Mtasks over the next Tc. Therefore, Mcould\nbe a variable, which indicates the varying inference workloads as\ntime passes. For each inference ji, it has a default execution time,\nTi, which is the execution time of the network with all layers. The\nactual execution time of inference jiis defin",
  " The\nactual execution time of inference jiis defined as ti. Ideally, we havePM\ni=1ti≤PM\ni=1Ti≤Tc. In this work, we set Tc=λPM\ni=1Ti,\n0< λ≤1. Smaller λindicates a tighter deadline. Different values of\nλwill be discussed and evaluated in Sec. V.\nB. Power and Energy Model\nWhen inference tasks are executed, the power consumption in\ncomputing, called active power Pactive , is the summation of dynamic\npower PD, static power PS, and constant power PC. While the\ncomputing platform is idle, the power con",
  "hile the\ncomputing platform is idle, the power consumption, also called idle\npower Pidle, mainly comprises PSandPC.PDoriginates from the\nactivity of logic gates inside a processor, which can be derived by\nPD=CV2f, (1)\nwhere Cis the capacitance of switched logic gates. PSoriginates from\ntransistor static current (including leakage and short circuit current)\nwhen the processor is powered on, which is described by\nPS=V NtrIS. (2)\nNtris the number of logic gates, and ISis the normalized static curre",
  " logic gates, and ISis the normalized static current\nfor each logic gate. PCis the power consumption by the auxiliary\ndevices, such as board fans and peripheral circuitry.\nAfter each \ninference………Coordination Period    , DeadlineInput \nImage\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nClassificati\non. LayerNet. Layer\nNet. LayerInput \nImage\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nClassificati\non. LayerNet. Layer\nNet. Layer\nExit\nLayer\nCross -inference\nCalibrationIntra -in",
  "r\nExit\nLayer\nCross -inference\nCalibrationIntra -inference\nPrediction\nDVFS \nGovernorfi  Prediction\nfi CalibrationProcessor \nVi / fi\nPlug -in ModuleInside each \ninference\nEENetExit \nprediction\nΔfi Inference layersInference layersInference layers\ncT\ncD\n1j\n2j\n3j\nMj\n1j\n2jFig. 2. The EENet framework.\nNormally, the classic neural networks are executed by running\nall network layers at the default high frequency fHand voltage\nVHwithout early exit or DVFS. The energy consumption for each\ninference within ",
  " The energy consumption for each\ninference within Tiis\nE=MX\ni=1Ei=MX\ni=1ZTi\n0Pactive dt=MX\ni=1ZTi\n0(PD+PS+PC)dt\n=MX\ni=1ZTi\n0(CV2\nHfH+VHNtrIS+PC)dt.\nAs the deployment of early exists to neural networks, network layers\nlocated ahead of the early exit run at (VH, fH). After the network exits\natti, the computing platform can reduce the voltage and frequency\nto the lowest level (VL, fL)by DVFS until the time reaches Ti[11].\nTherefore, the energy consumption of jican be described by\nEi=Zti\n0Pactive dt",
  "mption of jican be described by\nEi=Zti\n0Pactive dt+ZTi\ntiPidledt\n=Zti\n0(PD+PS+PC)dt+ZTi\nti(PS+PC)dt\n=Zti\n0(CV2\nHfH+VHNtrIS+PC)dt+ZTi\nti(VLNtrIS+PC)dt.\nSince (V, f)settings are cubic to PDbut linear to PSandti\naccording to Eqs. (1) and (2), it is highly demanded to further reduce\nPD. An intuition to achieve this is to adjust (V, f)to proper middle-\nlevel (VM, fM)for two potential improvements. First, each inference\njobjicould run the network at a proper lower (V, f)pair to prolong\nthe execution t",
  "proper lower (V, f)pair to prolong\nthe execution time titoTiat a lower energy cost. Second, among\nthe operations of multiple inference jobs within Tc, proper (V, f)\ncould be established to adjust to the remaining workloads and timing\nconstraints. However, when and how to adjust (V, f)during the\ninference progress is not trivial. In the next section, we propose an\nEENet solution to adjust (V, f)at run-time based on the prediction of\nwhere the network will exit and the remaining inferences to be d",
  "ork will exit and the remaining inferences to be done\nbefore the deadline Tc.\nIV. F RAMEWORK OF EEN ET\nA. Overview of the Framework\nThe proposed EENet will work as a plug-in power management\nmodule for existing networks without any model modification, as\nshown in Fig. 2. To properly adjust (V, f)for lowering energy cost of\njiand satisfying multiple inference workloads and timing constraints\nat the same time. EENet includes an intra-inference prediction, a cross-\ninference calibration, and a DVFS",
  "iction, a cross-\ninference calibration, and a DVFS governor .\nWithin an inference task ji, the intra-inference prediction first\nexecutes an early exit prediction, which forecasts where the inference\nnetwork will exit (i.e., establish the number of remaining layers\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  "to finish this inference). Then it establishes a proper prediction of\n(V, f)according to the remaining layers and worst-case execution\ntime Tiuntil the end of the current inference for lowering energy\ncost over a small timescale. Considering the remaining amount of\ninferences over Tc, the cross-inference calibrator, which is enabled\nby a feedback control policy, calibrates the (V, f)according to\nthe multi-inference progress, total inference workloads, and timing\nconstraints for balancing workloa",
  "oads, and timing\nconstraints for balancing workloads, energy, and time cost over a\nlarge timescale. According to the prediction and calibration, a DVFS\ngovernor will update the processor to proper computing configurations\n(i.e., frequency and voltage) at run-time to save energy while meeting\nthe deadlines.\nB. Intra-inference Prediction\n1) Backgrounds of Early Exit: Looking into the inference task ji,\nin this work, the design of exiting layer is based on the existing work\n[10], [20]. All the exit",
  "ased on the existing work\n[10], [20]. All the exiting layers are further modified to share the\nsame topology. It contains a Bag-of-Features (BoF) pooling layer and\na Fully-Connected (FC) layer. Let yibe the intermediate results at\ntheithlayer, and Ncbe the number of object classes in the dataset.\nThe BoF pooling layer functions as a feature aggregation to extract\nfrom yi. In the BoF pooling, a set of feature vectors called codebook\nare used to describe yi. The weight of each codebook is generate",
  "scribe yi. The weight of each codebook is generated by\nmeasuring the similarity between the codebook and yi. Since the size\nof codebook weight is usually larger than Nc, the FC layer further\nworks as a classifier that adjusts the result of BoF pooling to RNC,\nwhich estimates the final output of the exiting layer. Details can be\nfound in [10]. The function of the exiting layer is denoted as\ng(i)\nWi(yi) =g(i)\nWi(fW(x, i))∈RNC, (3)\nwhere fW(x, i)stands for the intermediate result ofithlayer with\nin",
  "nds for the intermediate result ofithlayer with\ninitial network inputs xand overall network parameters W.Wistands\nfor the parameters of ithexiting (BoF and FC) layer. And g(i)\nWi(yi)\nis the estimated result of the classifier.\nThe parameters in exiting layers need offline training. Since the\nexiting layer functions through estimating final network outputs, the\nsame cross-entropy loss function is chosen in training Wi.\nJ(p,q) =−NCX\ni=1p(i) log (q( i)), (4)\nwhere p(i)andq(i)are the actual and the p",
  "i)), (4)\nwhere p(i)andq(i)are the actual and the predicted class distributions\nfor each object, respectively.\nDuring training exiting layers, the training set data\n{x1,x2, . . . , xN}with size Nare fed into the model to optimize\nthe exiting layer parameter through batch gradient descent. Let\nLtotalbe the layer number of the network, and a target vector set\n{r1, r2, . . . , r N} ∈RN×Ncbe the correct result. At first, the model\nis trained without any exiting layer by the following equation,\nW′=W−η",
  "ny exiting layer by the following equation,\nW′=W−η\nNNX\nj=1∇WJ(fW(xj, Ltotal), rj), (5)\nwhere W′is the updated overall network parameters, and ηis the\nlearning rate. If the accuracy fails to meet the requirement, ηcan\nbe adjusted. Afterward, with Wof the original model fixed and\narmed with exiting layers, the model is trained again to optimize the\nparameters in the BoF pooling and FC layer,\nW′\ni=Wi−η\nNNX\nj=1∇WiJ\u0010\ng(i)\nWi(fW(xj, i)), rj\u0011\n, i∈[1, ..., L total].\n(6)\nAfter we’ve trained the exiting l",
  "., L total].\n(6)\nAfter we’ve trained the exiting layers, the average feature weight µi\nis calculated as a parameter for exit decision making,\nµi=1\nN1\nNCNcX\nk=1NX\nj=1h\ng(i)\nWi(fW(xj, i))i\nk. (7)\nFrequency \nPredictionInput \nImage\nNet. Layer\nNet. Layer\nClassificati\non. LayerNet. Layer\nNet. Layer\ntotalL\nNetwork Layers \nof Inference\nEarly Exit  \nPREDICT\n0,,L\nIntra -inference Prediction\n1L\n2LNet. Layer\nNet. Layer\nNet. Layer\nL\nExit\nLayer\nij\n0L\n,Mif\n0\n0 0()\nLL\nL gyW\n0,,i totalf L L\nFig. 3. The diagram f",
  "0()\nLL\nL gyW\n0,,i totalf L L\nFig. 3. The diagram for intra-inference prediction\nDuring the inference of initial network inputs xj, at each early exiting\nlayer, the weight ratio αWis calculated in Eq. (8) as the maximum\nfeature weight over the µimultiplied by a hyperparameter βspecified\nby the user. The larger the maximum feature weight is, the more\nconfident the classification is regarded. Once αWis larger than 1, the\ninference is terminated, and the result at this early exiting layer is\napplied",
  " the result at this early exiting layer is\napplied as the final result.\nαW=maxkh\ng(i)\nWi(fW(xj, i))i\nk\nβµi(8)\n2) Early Exit PREDICT :The prediction of the early exit and\nfrequency is shown in Fig. 3. It starts to forecast the exiting point\nsince the L0network layer of the inference. Given the intermediate\nestimated result g(L0)\nWL0(yL0)calculated based on the intermediate\ninference result yL0and the early exiting parameters pre-trained by\nEqs. (4)-(6), the P REDICT module establishes ζ, informin",
  ")-(6), the P REDICT module establishes ζ, informing the optimal\nlayer L0+ζto exit, based on Eqs. (9)-(12) at the run-time.\nTo perform the convolution, we first extend the g(L0)\nWL0(yL0)from\nRNctoRNc+K−1by zero padding. The extension allows more space\nfor the filter to cover the intermediate result,\nJL0=ZEROPAD(Nc+K−1)\u0010\ng(L0)\nWL0(yL0)\u0011\n=(\ng(L0)\nWL0(yL0) [l],K−1\n2≤l≤K−3\n2+Nc\n0, otherwise..(9)\nThen, a vector h∈RKis generated (with all 1 in our work) as the filter\nof the convolution. Based on JL0and",
  " as the filter\nof the convolution. Based on JL0andh, a new set of feature weight\nGL0+1\nL0∈RNcis generated through one-dimensional convolution,\nwhich estimates the results of exiting layer placed at L0+ 1,\nGL0+1\nL0[l] =K−1X\nk=0h[k]×JL0[l−k−1\n2]. (10)\nBy recursively replacing the intermediate estimated result g(L0)\nWL0(yL0)\nin Eq. (9) with GL0+1\nL0[l]and repeating the computation of Eqs. (9)\nand (10), the predicted results of the exiting layer placed at L0+ 2\ncan be obtained and noted as GL0+2\nL0+",
  "ed at L0+ 2\ncan be obtained and noted as GL0+2\nL0+1. Following the above steps, the\npredicted results of any exiting layer placed after L0can be calculated.\nWith the predicted results of exiting layer placed at any layer after\nL0, we will have P REDICT function described as follows:\nPREDICT\u0010\ng(L0)\nWL0(yL0), β\u0011\n=ζ, (11)\nwhere ζ∈Zrepresents the predicted exiting point after L0, which is\nthe smallest number that satisfies h\nGL0+ζ\nL0+ζ−1i\nk\nβµL0+ζ>1, k= arg max GL0+ζ\nL0, (12)\nwhereh\nGL0+ζ\nL0+ζ−1i\nk\n",
  " k= arg max GL0+ζ\nL0, (12)\nwhereh\nGL0+ζ\nL0+ζ−1i\nk\nβµL0+ζindicates the predicted exiting confidence at layer\nL0+ζ. Therefore, L0+ζis the predicted exiting point from the\npredict function. In case ζcannot be found, a hyperparameter τis\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  "further introduced, that is ζ∈[1, τ],τ≤Ltotal−L0.τshould be no\nmore than Ltotal−L0.The upper bound of τmeans that prediction\nresult beyond the last layer is forbidden. If no integer in [1, τ]satisfies\nEq. (12), we assume that ζ=τ. The time complexity of the prediction\nisO(Nc×K×τ), which is 1/106to1/107of the entire EENet’s\ncomputation cost represented by the number of operations . Meanwhile,\nhyperparameters are introduced in the prediction. L0,β, and τcan be\ntuned by advanced users, which can ba",
  " and τcan be\ntuned by advanced users, which can balance the prediction accuracy\nand computation cost for different application scenarios.\n3) Frequency Prediction: Inspired by the intuition described in the\nend of Sec. III-B, this module is designed to transform the exiting\nprediction result ζof P REDICT to a proper “middle-level” frequency,\nfM,i.For inference task ji, during the time interval between the\nbeginning of the inference and the prediction at L0, a relatively “high-\nlevel” ˆfH,iis cons",
  "ion at L0, a relatively “high-\nlevel” ˆfH,iis conservatively applied to finish Ltotalof the model in\ntime without exit,\nˆfH,i= min( fH+ ∆fi, fH), (13)\nwhere and fHis the default highest frequency of the computing\nplatform that can finish the inference with Ltotallayers without early\nexit by time Ti. The frequency ˆfH,iis calibrated by ∆fi, which is\nderived from the cross-inference feedback DVFS mechanism described\nin Sec. IV-C.\nAt the time of prediction at L0, based on the predicted exiting poin",
  "diction at L0, based on the predicted exiting point,\nfis adjusted to proper “middle-level” fM,iand runs the network until\nit exits at Ti. Given a network with Ltotallayers and the early exiting\nprediction ζ, the computing frequency is reduced to\nfM,i=ˆfH,i\nLtotal−L0ζ, (14)\nwhere fM,iis the prediction of the lowest frequency that can finish\nthe inference by time Ti.\nC. Cross-inference Calibration\nSince we are targeting executing inference tasks in a burst manner,\nfor inference task ji+1, the cros",
  " a burst manner,\nfor inference task ji+1, the cross-inference DVFS policy provides\ncalibration advise ∆fi+1after finishing all previous tasks including\nji, given the workloads and timing constraints over a larger time\nscale Tc. The calibration is implemented with a discrete incremental\nproportional-integral (PI) regulator,\n∆fi+1=∆fi+\u0000\nKP+KI(ti−ti−1)\u0001\ne(ti)−KPe(ti−1),(15)\nwhere KPandKIare the proportional and integral coefficients,\nrespectively. Index iindicates the ithinference task in current c",
  "ndex iindicates the ithinference task in current co-\nordination period. Notation tidenotes the relative finishing time of\ninference jisince the beginning of the coordination period. The term\nti−ti−1is the time interval between the finishing time of inference\ntasks jiandji−1.\nThe input deviation of the PI regulator, e(ti), is the evaluation\nof inference progress over Tcaccording to total inference workloads\nand the execution progress of inferences since the beginning of the\ncoordination period. I",
  " since the beginning of the\ncoordination period. In this section, we define e(ti), leaning on the\ncoordination goal, e(ti) =M\nαTc−i\nti, of which the first term is the\nreference pace for inference workload balancing, and the second term\nis the processing pace until now.\nIn practice, we use a smaller time period αTc,0< α≤1. The\ninterval between αTcandTcis the margin reserved to eliminate the\nimpacts of execution time overshoots by feedback control. A proper\nchoice of αcould balance the tradeoff be",
  " A proper\nchoice of αcould balance the tradeoff between saving energy and\nmeeting deadlines, which will be evaluated in Sec. V. The output\nof the cross-inference calibrator is the frequency calibration for the\nnext inference ji+1for DVFS governor to configure the computation\nplatform, ∆fi+1.\nInput \nImage\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nNet. Layer\nClassificati\non. LayerNet. Layer\nNet. LayerClassificati\non. Layer\n1L\n2L\ntotalL\n0L\nInput \nImage\nf\nˆ\nHf\nˆ\nMf\nvalue for ji \ndeployedvalue for ",
  "Image\nf\nˆ\nHf\nˆ\nMf\nvalue for ji \ndeployedvalue for ji updated but \nnot deployedcalculation overhead \n(exaggerated)wait for input\nij\nt\n1ij\n1ij\nInference Fig. 4. The diagram for DVFS governor\nD. DVFS Governor\nFinally, the DVFS governor configures frequency and its corre-\nsponding voltage [21] to execute the network according to the predic-\ntion and calibration over the small and large timescales, respectively,\nto save energy further and meet the network inference deadlines. The\nDVFS governor operat",
  "work inference deadlines. The\nDVFS governor operates (V, f)scaling with a smaller granularity\nthan the inference. As shown in Fig. 4, at the beginning of each\ninference, ∆fiis derived by the cross-inference calibration based on\nEq. (15). And the DVFS governor sets the computation configuration\nto(ˆVH,ˆfH)based on Eq. (13) to finish the inference without early\nexit, conservatively. At the time the early exit prediction is made at the\ninference layer L0, the DVFS governor establishes proper freque",
  "er L0, the DVFS governor establishes proper frequency\nconfiguration ˆfM,i, according to fM,iderived by the intra-inference\npredictor based on Eq. (14) and ∆fiderived by the cross-inference\ncalibration based on Eq. (15) ,\nˆfM,i=fM,i+ ∆fi. (16)\nHence, the resultant energy consumption would be\nE=MX\ni=1\u0010ZtL0,i\n0(CˆV2\nH,iˆfH,i+ˆVH,iNtrIS+PC)dt\n+Zti\ntL0,i(CˆV2\nM,iˆfM,i+ˆVM,iNtrIS+PC)dt\u0011\n+ZTc\nPM\ni=1ti(VLNtrIS+PC)dt,(17)\nwhere tL0,iis the time of finishing the prediction layer L0and its\nprevious network",
  "ng the prediction layer L0and its\nprevious network layers of ji.ˆVM,i is the corresponding voltage\ngiven ˆfM,i. As the selection of fis based on the predicted remaining\nworkloads in the inference, the intra-inference prediction can make ji\nfinished by Ti. And the cross-inference calibration targets making M\nsequential inferences finished by Tc. Since Vandfare linear scales\nto the computing performance (inversely proportional to inference\nlatency), but the cubic scale and linear scale to PDandPS,",
  ", but the cubic scale and linear scale to PDandPS, the double-\ntimescale power management effectively reduces E.\nThe scaling of processor voltage and frequency can be implemented\nat the application level through a system call (syscall). The transient\ntime for each (V, f)change is approximately 1 ms to 3 ms based\non our observation. The transient processes in changing the processor\n(V, f)are incorporated in our realsystem evaluations.\nNote that for each inference at run-time, the computation comp",
  "r each inference at run-time, the computation complexity\nof intra-inference exit prediction is O(Nc×K×τ)as described in\nSec. IV-B. And that for both cross-inference calibration and DVFS\ngovernor are O(1). Therefore, the overhead of the EENet plug-in\nmodule, which combines the three, is lightweight with the computation\ncomplexity of O(Nc×K×τ), which we will evaluate in Sec. V-E.\nV. E VALUATION\nA. Experimental Setup\nWe compare EENet with state-of-the-art approaches: (1) Classic :\nthe classic CNN [",
  "he-art approaches: (1) Classic :\nthe classic CNN [22], [23]; (2) Hier. : the hierarchical early exit for\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  "(a) M=10, Tc= 100 ms.\n (b) M=20, Tc= 200 ms.\n (c) M=50, Tc= 500 ms.\nFig. 5. Statistical results for latency of different M(Each boxplot contains 5000 inferences of images in Cifar10 dataset.)\n100 100 100\n89.8\n84.6\n0100 99.6 99.4\n93.4\n66.6\n0100 100 99.2\n90.2\n39.6\n0.04100 99.4 98.896\n68.6\n10100 99.6 9997\n77\n8.2100 100 99.4 98.4\n84.8\n41.8100 100 100 99.6 99.6\n92.35100 99.2 99.6 98.496.4\n82.1\n0102030405060708090100\n1 0.9 0.8 0.7 0.6 0.5Deadline Meet Rate (%)CNN-Vgg19\nCNN-Resnet18\nHierachical-Vgg19\nH",
  "Rate (%)CNN-Vgg19\nCNN-Resnet18\nHierachical-Vgg19\nHierachical-Resnet18\nPlacement-Vgg19\nPlacement-Resnet18\nEENet-Vgg19\nEENet-Resnet18\nValue of  \nFig. 6. The ratio of deadline meetings under various λ(Ti= 20 ms for Vgg-19, Ti= 18 ms for Resnet-18, M= 10.)\nCNN proposed in [10] by placing exit layers at roughly1\n4,1\n2,3\n4of\nthe network; (3) Place : the placement of early exit improved [14] by\nidentifying the best place , exhaustively searching the placements of\nexiting layers with the local minimum c",
  "cements of\nexiting layers with the local minimum computational costs . We eval-\nuate EENet using VGG-19 and ResNets-18 as the backbone models\non the commonly used CIFAR-10, CIFAR-100 [24], SVHN [25], and\nCINIC [26] datasets. The inference accuracy and timing performance\nare evaluated on NVIDIA Jetson TX2, with GPU fH= 1.30050 GHz\nandfL= 0.11475 GHz. The serial and I/O operations are performed\non CPUs, and the massive parallel and computation-intensive segments\nare offloaded to the GPUs, which ar",
  "nsive segments\nare offloaded to the GPUs, which are automatically managed by\nthe PyTorch libraries [27]. Energy costs are evaluated by Tektronix\nMDO32 oscilloscope and TCP2020 current probe, with a sampling\nfrequency of 50 kHz. Please note that the overheads of all the above\napproaches are incorporated in the real-system evaluation for fair\ncomparisons. In addition, we list the parameters for prediction and\ncalibration in different benchmarks in Table I for readers’ reference.\nTABLE I\nPARAMETERS",
  "Table I for readers’ reference.\nTABLE I\nPARAMETERS IN DIFFERENT BENCHMARKS\nModel VGG-19 ResNet-18\nBenchmark CIFAR-10 CIFAR-100 SVHN CINIC-10 CIFAR-10 CIFAR-100 SVHN CINIC-10\nL0 6 6 6 6 7 7 7 7\nβ 4.5 20 7.5 3.5 27 17 15 20\nKP 100fL 200fL 200fL 200fL 200fL 200fL 50fL 200fL\nKI 70fL 150fL 150fL 500fL 150fL 150fL 50fL 150fL\nB. Prediction and Inference Accuracy\nWithin the inference process, the frequency is calculated based\non the prediction of the early exit points. The accuracy of the\nprediction dir",
  "ly exit points. The accuracy of the\nprediction directly determines the task timing and energy consumption.\nTherefore, we tested the prediction accuracy across the datasets and\nmodels with different L0. According to the prediction and inference\naccuracy results, we set L0= 6 and7in Vgg-19 and Resnet-18 in\nthe rest sections, respectively.\nThen we evaluate the inference accuracy, the results of which are\nshown in Table II. EENet achieves the same accuracy as other early\nexit approaches on both mode",
  "curacy as other early\nexit approaches on both models, which is 1% - 3% lower than Classic\nCNN. Because in the early exit prediction, the network will continue\nfor the next prediction instead of forcing the inference to exit even\nif the prediction is wrong, and no extra inference accuracy loss is\nintroduced. Please note that the rest of the power management settingsin EENet have no impact on the inference accuracy since the exiting\nlayer prediction of ζis determined first in order to achieve high",
  "n of ζis determined first in order to achieve high\ninference accuracy, which the power management is built upon.\nTABLE II\nINFERENCE ACCURACY IN THE VGG-19 AND RESNET-18 NETWORK\nModel VGG-19 ResNet-18\nApproach Classic Hier. Place EENet Classic Hier. Place EENet\nCIFAR-10 91% 89% 89% 89% 87% 87% 87% 87%\nCIFAR-100 64% 61% 61% 61% 61% 59% 59% 59%\nSVHN 94% 92% 92% 92% 94% 92% 92% 92%\nCINIC-10 80% 79% 79% 79% 81% 80% 80% 80%\nC. Timing Performance\nWe evaluate the timing performance of EENet. Fig. 5 pres",
  "luate the timing performance of EENet. Fig. 5 presents\nthe distribution of tasks execution time (PM\ni=1ti) for VGG-19 and\nResNet-18 models on the Cifar10 dataset under different Mand\nTc. Similar results are observed on other datasets. Not surprisingly,\nclassic CNN has a much longer and broader distribution of execution\ntime. EENet shows more concentrated and lower execution time than\nother approaches because of the effective double-timescale DVFS\nmanagement. As we mentioned in Sec. IV-C, αcould ",
  "\nmanagement. As we mentioned in Sec. IV-C, αcould balance the\ntradeoff between saving energy and meeting the deadline. α= 0.75\nresults in a slightly larger total execution time than α= 0.5, whereas\nit saves more energy, as indicated in next section.\nFinally, we evaluate the deadline Tcmeeting rate under different\nvalues of λ. As the stress test, we compare the timing performance\nunder tighter deadlines, i.e., Tc=λPM\ni=1∗Ti,0< λ≤1. Fig. 6\nshows the meeting rate of different deadlines. When λ= 1, ",
  "e meeting rate of different deadlines. When λ= 1, all the\napproaches can meet the deadline. However, when the deadline gets\ntighter, EENet shows outstanding performance in meeting deadlines\nbecause of the lightweight predictive exit and the high-efficiency cross-\ninference feedback power management.\nD. Energy Consumption\nTo measure the run-time power consumption, the Jetson TX2 board\nis directly connected to the Keysight E36231A power supply with a 19\nV voltage. The current probe and oscilloscop",
  " a 19\nV voltage. The current probe and oscilloscope monitor the current fed\nto the board. Table III summarizes the average energy consumption\nfor each task. The energy measurements on real platforms show that\nEENet achieves up to 63.8% energy-saving compared with classic\ndeep learning networks and 21.5% energy-saving compared with the\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  "Xplore.  Restrictions apply.",
  "TABLE III\nAVERAGE ENERGY CONSUMPTION IN NETWORKS (JOULE )\nVGG-19 Classic Hier. PlaceEENet\n(α= 0.5)EENet\n(α= 0.75)\nCIFAR-10 959.93 505.62 415.55 397.32 369.09\nCIFAR-100 1137.71 523.11 579.91 468.75 460.26\nSVHN 1131.20 567.67 521.57 419.75 409.21\nCINIC-10 1191.35 623.71 564.79 460.05 473.39\nResNet-18 Classic Hier. PlaceEENet\n(α= 0.5)EENet\n(α= 0.75)\nCIFAR-10 817.13 485.95 425.17 399.80 414.67\nCIFAR-100 817.25 498.06 523.31 451.98 422.47\nSVHN 790.54 520.29 468.29 419.21 418.61\nCINIC-10 850.22 475.02",
  "520.29 468.29 419.21 418.61\nCINIC-10 850.22 475.02 498.75 456.83 450.25\nCIFAR-10 CIFAR-100 SVHN CINIC-1050100150200250300 Vgg19\nResnet18\nFig. 7. The time costs for early exit P REDICT\nearly exit under state-of-the-art exiting strategies. α= 0.75results in\nless energy consumption than α= 0.5in most cases.\nE.Overhead of EENet\nAs discussed in the end of Sec. IV-C, the main source of the\noverhead of the EENet plug-in module is the early exit P REDICT .\nTherefore, we evaluate the time costs of 1750 e",
  " .\nTherefore, we evaluate the time costs of 1750 early exit P REDICT for\neach benchmark, as shown in Fig. 7. The maximum time costs of all\nbenchmarks are below 300 µs. The box of ResNets-18 is broader than\nthat of VGG-19 since ResNets-18 is predicted to exit with relatively\nlarger ζ. The time costs indicate the applicability and scalability for\nthe online power management of EENet.\nVI. C ONCLUSION\nIn this paper, we propose EENet, which provides double-timescale\npower management tailored for neur",
  "ouble-timescale\npower management tailored for neural networks. EENet integrates an\nintra-inference prediction, which configures the computation based on\nforecasts of early exits within an inference, cross-inference calibration\nbased on inference workloads and progress over the coordination\nperiod, and a fine-grained DVFS Governor configures the voltage and\nfrequency of a processor. Extensive experimental results demonstrate\nthat EENet achieves reasonable accuracy, up to 63.8% energy-saving\ncompa",
  "asonable accuracy, up to 63.8% energy-saving\ncompared with classic deep learning networks and 21.5% energy-\nsaving compared with the early exit under state-of-the-art exiting\nstrategies, together with improved timing performance, according to\nthe testing and measurements on real platforms. In the future, we will\nexplore more power management approaches for other networks.\nREFERENCES\n[1]A. Xygkis, L. Papadopoulos, D. Moloney, D. Soudris, and S. Yous,\n“Efficient winograd-based convolution kernel i",
  "us,\n“Efficient winograd-based convolution kernel implementation on edge\ndevices,” in Proceedings of the Design Automation Conference , 2018.\n[2]A. Raha and V . Raghunathan, “Towards full-system energy-accuracy\ntradeoffs: A case study of an approximate smart camera system,” in\nProceedings of the Design Automation Conference , 2017.\n[3]C. Gao, D. Neil, E. Ceolini, S.-C. Liu, and T. Delbruck, “Deltarnn: A\npower-efficient recurrent neural network accelerator,” in the ACM/SIGDA\nInternational Symposiu",
  "elerator,” in the ACM/SIGDA\nInternational Symposium on Field-Programmable Gate Arrays , 2018.\n[4]R. Liu, J. Yang, Y . Chen, and W. Zhao, “eslam: An energy-efficient\naccelerator for real-time orb-slam on fpga platform,” in Proceedings of\nthe Design Automation Conference , 2019.[5]X. Wang, F. Yu, Z.-Y . Dou, T. Darrell, and J. E. Gonzalez, “Skipnet:\nLearning dynamic routing in convolutional networks,” in Proceedings of\nthe European Conference on Computer Vision , 2018.\n[6]M. Figurnov, M. D. Collin",
  "mputer Vision , 2018.\n[6]M. Figurnov, M. D. Collins, Y . Zhu, L. Zhang, J. Huang, D. Vetrov,\nand R. Salakhutdinov, “Spatially adaptive computation time for residual\nnetworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , 2017.\n[7]S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Branchynet: Fast infer-\nence via early exiting from deep neural networks,” in 23rd International\nConference on Pattern Recognition . IEEE, 2016.\n[8]P. Panda, A. Sengupta, and K. Roy, ",
  "IEEE, 2016.\n[8]P. Panda, A. Sengupta, and K. Roy, “Conditional deep learning for energy-\nefficient and enhanced pattern recognition,” in Design, Automation & Test\nin Europe Conference & Exhibition . IEEE, 2016.\n[9]Y . Wu, Z. Wang, Z. Jia, Y . Shi, and J. Hu, “Intermittent inference with\nnonuniformly compressed multi-exit neural network for energy harvesting\npowered devices,” in Design Automation Conference . IEEE, 2020.\n[10]N. Passalis, J. Raitoharju, A. Tefas, and M. Gabbouj, “Efficient adaptiv",
  "arju, A. Tefas, and M. Gabbouj, “Efficient adaptive\ninference for deep convolutional neural networks using hierarchical early\nexits,” Pattern Recognition , vol. 105, p. 107346, 2020.\n[11]T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y . Yang, M. Donato, V . Sanh,\nP. Whatmough, A. M. Rush, D. Brooks et al. , “Edgebert: Sentence-\nlevel energy optimizations for latency-aware multi-task nlp inference,”\ninIEEE/ACM International Symposium on Microarchitecture , 2021.\n[12]C. Yao, W. Liu, Z. Liu, L. Yan",
  "tecture , 2021.\n[12]C. Yao, W. Liu, Z. Liu, L. Yan, S. Hu, and W. Tang, “Eali: Energy-aware\nlayer-level scheduling for convolutional neural network inference services\non gpus,” Neurocomputing , vol. 507, pp. 265–281, 2022.\n[13]S. M. Nabavinejad, S. Reda, and M. Ebrahimi, “Batchsizer: Power-\nperformance trade-off for dnn inference,” in Proceedings of the Asia and\nSouth Pacific Design Automation Conference , 2021.\n[14]E. Baccarelli, S. Scardapane, M. Scarpiniti, A. Momenzadeh, and\nA. Uncini, “Opti",
  "M. Scarpiniti, A. Momenzadeh, and\nA. Uncini, “Optimized training and scalable implementation of con-\nditional deep neural networks with early exits for fog-supported iot\napplications,” Information Sciences , vol. 521, pp. 107–143, 2020.\n[15]P. Panda, A. Sengupta, and K. Roy, “Energy-efficient and improved image\nrecognition with conditional deep learning,” ACM Journal on Emerging\nTechnologies in Computing Systems , 2017.\n[16]A. Ghodrati, B. E. Bejnordi, and A. Habibian, “Frameexit: Conditional\nea",
  "nordi, and A. Habibian, “Frameexit: Conditional\nearly exiting for efficient video recognition,” in the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2021.\n[17]M. Odema, N. Rashid, and M. A. A. Faruque, “Eexnas: Early-exit\nneural architecture search solutions for low-power wearable devices,”\ninIEEE/ACM International Symposium on Low Power Electronics and\nDesign , 2021.\n[18]E. Samikwa, A. Di Maio, and T. Braun, “Adaptive early exit of com-\nputation for energy-efficient and low-l",
  "it of com-\nputation for energy-efficient and low-latency machine learning over iot\nnetworks,” in IEEE Consumer Communications Networking Conference ,\n2022.\n[19]C. Yao, W. Liu, W. Tang, and S. Hu, “Eais: Energy-aware adaptive\nscheduling for cnn inference on high-performance gpus,” Future Gen-\neration Computer Systems , vol. 130, pp. 253–268, 2022.\n[20]Y . Bai, S. S. Bhattacharyya, A. P. Happonen, and H. Huttunen, “Elastic\nneural networks: A scalable framework for embedded computer vision,”\ninEuro",
  "le framework for embedded computer vision,”\ninEuropean Signal Processing Conference . IEEE, 2018.\n[21]A. Zou, Y . Ma, K. Garimella, B. Lee, C. D. Gill, and X. Zhang, “F-lemma:\nFast learning-based energy management for multi-/many-core processors,”\nIEEE Transactions on Computer-Aided Design of Integrated Circuits and\nSystems , 2022.\n[22]K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv , 2014.\n[23]K. He, X. Zhang, S. Ren, and ",
  "nt arXiv , 2014.\n[23]K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016.\n[24]A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features\nfrom tiny images,” 2009.\n[25]Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng,\n“Reading digits in natural images with unsupervised feature learning,”\n2011.\n[26]L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. ",
  " N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, “Cinic-10\nis not imagenet or cifar-10,” arXiv preprint arXiv , 2018.\n[27]A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneural information processing systems , vol. 32, 2019.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:52 UTC from IE",
  "nloaded on January 05,2026 at 07:47:52 UTC from IEEE Xplore.  Restrictions apply.",
  ".\n.\nLatest updates: hps://dl.acm.org/doi/10.1145/3649329.3655956\n.\n.\nRESEARCH-ARTICLE\nPowerLens: An Adaptive DVFS Framework for Optimizing Energy\nEﬀiciency in Deep Neural Networks\nJIAWEI GENG, University of Science and Technology of China, Hefei, Anhui, China\n.\nZONGWEI ZHU, University of Science and Technology of China, Hefei, Anhui, China\n.\nWEIHONG LIU, University of Science and Technology of China, Hefei, Anhui, China\n.\nXUEHAI ZHOU, University of Science and Technology of China, Hefei, Anhui,",
  " of Science and Technology of China, Hefei, Anhui, China\n.\nBOYU LI, University of Science and Technology of China, Hefei, Anhui, China\n.\n.\n.\nOpen Access Support provided by:\n.\nUniversity of Science and Technology of China\n.\nPDF Download\n3649329.3655956.pdf\n04 January 2026\nTotal Citations: 1\nTotal Downloads: 2152\n.\n.Published: 23 June 2024\n.\n.\nCitation in BibTeX format\n.\n.\nDAC '24: 61st ACM/IEEE Design\nAutomation Conference\nJune 23 - 27, 2024\nCA, San Francisco, USA\n.\n.\nConference Sponsors:\nSIGDA\n",
  "San Francisco, USA\n.\n.\nConference Sponsors:\nSIGDA\nDAC '24: Proceedings of the 61st ACM/IEEE Design Automation Conference (June 2024)\nhps://doi.org/10.1145/3649329.3655956\nISBN: 9798400706011\n.",
  "PowerLens: An Adaptive DVFS Framework for Optimizing\nEnergy Efficiency in Deep Neural Networks\nJiawei Geng𝑎𝑏, Zongwei Zhu∗𝑏, Weihong Liu𝑎𝑏, Xuehai Zhou𝑎𝑏, Boyu Li𝑏\n𝑎School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China\n𝑏Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou 215123, China\n{gjw1998,lwh2017,llbbyy}@mail.ustc.edu.cn,{zzw1988*,xhzhou}@ustc.edu.cn\nABSTRACT\nTo address the power management cha",
  "du.cn\nABSTRACT\nTo address the power management challenges in deep neural net-\nworks (DNNs), dynamic voltage and frequency scaling (DVFS) tech-\nnology is garnering attention for its ability to enhance energy\nefficiency without modifying the structure of DNNs. However, cur-\nrent DVFS methods, which depend on historical information such\nas processor utilization and task computational load, face issues\nlike frequency ping-pong, response lag, and poor generalizability.\nTherefore, this paper introduce",
  " generalizability.\nTherefore, this paper introduces PowerLens, an adaptive DVFS\nframework. Initially, we develop a power-sensitive feature extrac-\ntion method for DNNs and identify critical power blocks through\nclustering based on power behavior similarity, thereby achieving\nadaptive DVFS instrumentation point settings. Then, the frame-\nwork adaptively presets the target frequency for each power block\nthrough a decision model. Finally, through a refined training and\ndeployment process, we ensure",
  "refined training and\ndeployment process, we ensure the framework’s effective adapt-\nability across different platforms. Experimental results confirm the\neffectiveness of the framework in energy efficiency optimization.\nKEYWORDS\nDNNs, energy efficiency, adaptive DVFS, power characterization\n∗Corresp\nonding author: Zongwei Zhu1 INTRODUCTION\nThe capabilities of deep learning and deep neural networks (DNNs)\nin solving complex challenging problems have been widely recog-\nnized. With the increasing co",
  "e been widely recog-\nnized. With the increasing complexity of these models, effectively\nmanaging their power consumption has emerged as a critical issue\nin both research and practical applications [11].\nTo address this issue, dynamic voltage and frequency scaling\n(DVFS) has attracted widespread attention in research for its ability\nto optimize energy efficiency without modifying the structure of\nDNNs (e.g., pruning that alters DNN structure [ 17]). While effective\nin traditional domains, DVFS fa",
  "). While effective\nin traditional domains, DVFS faces challenges in deep learning,\nhighlighting the need for tailored solutions.\nCurrent research on DVFS in deep learning often extends tra-\nditional strategies, relying on hardware states and historical data\nfor runtime frequency adjustments. Although recent methods have\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pr",
  "ded that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nDAC ’24, June 23–27, 2024, San Francisco, ",
  "cm.org.\nDAC ’24, June 23–27, 2024, San Francisco, CA, USA\n©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0601-1/24/06. . .$15.00\nhttps://doi.org/10.1145/3649329.3655956incorporated features of DNNs, such as considering the computa-\ntions in convolutional layers [ 21], they essentially just expand the\nuse of historical data without breaking out of this paradigm. This\nessentially heuristic approach encounters significant limitations:\nFigure 1: ",
  "ach encounters significant limitations:\nFigure 1: An illustration of the two DVFS methods.\n①Frequency Ping-Pong and Lag Issue. As shown in Figure\n1(A), the reliance on historical information [ 5,12] (e.g., processor\nutilization) and heuristic rules [ 10], often leads to lag in response,\ncreating a misalignment between computation needs and frequency\nadjustments. Moreover, rapid workload changes can cause DVFS\nsystems to oscillate between settings, diminishing performance.\n②Frequency Tuning Accur",
  ", diminishing performance.\n②Frequency Tuning Accuracy Issue. DVFS methods that rely\non historical information may struggle to accurately predict the\nactual workload and performance demands of DNNs [12].\n③Platform Generalizability Issue. While existing methods may\nbe effective in certain scenarios, they encounter challenges when\ntransitioning to new hardware platforms. For example, transferring\nthe method in [ 5] to different devices necessitates recalibrating the\ngovernor’s utilization represent",
  "recalibrating the\ngovernor’s utilization representations and thresholds.\nDeep learning tasks possess structures suitable for pre-analysis\n[13], distinguishing them from general tasks. Therefore, we in-\ntroduce the adaptive DVFS framework, PowerLens, emphasizing\nadaptability in three key aspects to address previous limitations:\nAdaptive DVFS Instrumentation Points Granularity. To\naddress frequency ping-pong and lag issues, we propose a power-\nsensitive feature extraction method and power behavior",
  "itive feature extraction method and power behavior similarity\nclustering. As shown in Figure 1(B), this approach accurately iden-\ntifies key power blocks, serving as the fundamental units for DVFS,\nthereby preventing frequency ping-pong. Moreover, this method\nalleviates lag issues by presetting DVFS instrumentation points\nbefore each block, enabling more precise frequency tuning.\nAdaptive Target Frequency Setting. To address the issue of\naccuracy in frequency tuning, the framework employs a deci",
  " in frequency tuning, the framework employs a decision",
  "model to predict and adaptively preset the target frequency for\neach block. This ensures that the frequency settings align with the\nvarying performance and power requirements of each power block.\nAdaptability to Hardware Platforms. To achieve platform\ngeneralizability, PowerLens provides a complete model training\nand framework deployment workflow without human intervention,\nensuring stable performance across different hardware platforms.\nThe contributions of this work are summarized below.\n•We p",
  "ributions of this work are summarized below.\n•We propose a power-sensitive feature extraction method,\npaired with clustering based on power behavior similarity\nto identify critical power blocks in DNNs.\n•An offline adaptive DVFS framework is developed. It opti-\nmizes the energy efficiency by accurately determining DVFS\ninstrumentation points and their target frequencies through\ntwo deep learning-based prediction models.\n•The framework has been successfully deployed on two hard-\nware platforms, c",
  "ccessfully deployed on two hard-\nware platforms, confirming its adaptability and effectiveness.\n2 POWERLENS FRAMEWORK\nThis section delves into the adaptive DVFS framework, PowerLens,\ncovering two core aspects as illustrated in Figure 2. Firstly, we ex-\nplore the workflow and essential modules of PowerLens, which in-\nclude power-sensitive feature extraction methods, clustering based\non power behavior similarity, and adaptive DVFS decisions. These\nmethods, connected through the intermediate repres",
  "methods, connected through the intermediate representation. Sub-\nsequently, the model training stage encompasses a unified process\nfrom dataset generation to model training. This process yields two\nkey prediction models: ①A model for predicting clustering hyperpa-\nrameters, enabling the selection of appropriate clustering schemes\nfor different DNNs. ②A decision model used to determine target\nfrequency of each power block. Finally, we systematically analyze\nthe offline and runtime overhead introd",
  "ly analyze\nthe offline and runtime overhead introduced by the framework.\nFigure 2: The adaptive DVFS framework PowerLens.\n2.1 Workflow and Essential Module\nThe PowerLens integrates modules including power-sensitive fea-\nture extraction, power behavior similarity clustering, and adaptive\ntarget frequency decision-making. The interconnection of variousmodules is facilitated through intermediate representations and\nprediction modules.\n2.1.1 Workflow. As illustrated in figure 2, the framework operat",
  ". As illustrated in figure 2, the framework operates\nas follows. ①Global features of the DNNs are extracted by a global\nfeature extractor and are then used as inputs for the clustering\nhyperparameter prediction model. This model predicts clustering\nhyperparameters for the current DNNs. ②Subsequently, these\nclustering hyperparameters are combined with the network’s fine-\ngrained deepwise features for power behavior similarity clustering.\n③The clustering process maps the \"power distance\" between o",
  "tering process maps the \"power distance\" between opera-\ntors by calculating the Mahalanobis distance [3] and regularization,\nthus forming a power view. ④The block information extracted\nfrom the power view is further processed by the global feature\nextractor to extract each block’s global feature, serving as input for\nthe decision model. This model outputs target frequency decisions\nfor each block. ⑤Based on these optimization decisions, target fre-\nquencies are preset in advance at each power bl",
  "e-\nquencies are preset in advance at each power block within the DNN.\nThe entire framework achieves a closed-loop control from input to\noptimization decision, ensuring that the neural network achieves\nenergy efficiency optimization while maintaining performance.\n2.1.2 Power-Sensitive Feature Extraction. We delve into power-\nsensitive feature extraction methods of mixed granularity. Our goal\nis to construct a comprehensive representation that accurately re-\nflects the power characteristics of DNN",
  "rately re-\nflects the power characteristics of DNNs through feature extraction\nand integration. Therefore, we have adopted two complementary\nmethods: one is the Depthwise Feature Extractor, which delves into\neach layer of the network to extract fine-grained layer-level fea-\ntures; the other is the Global Feature Extractor, which operates from\na global perspective to extract coarse-grained macroscopic features\nof the network. Subsequently, these features will be effectively ex-\npressed and fused ",
  "eatures will be effectively ex-\npressed and fused to form intermediate representation, laying a\nsolid foundation for subsequent clustering and prediction.\nDepthwise Feature Extractor. The complexity and diversity\nof component types in DNNs, along with the heterogeneity of their\nfunctions, necessitate the extraction of fine-grained, layer-by-layer\ndeepwise features. Specifically, it involves extracting crucial at-\ntributes such as computational load, number of parameters, volume\nof memory access,",
  "ad, number of parameters, volume\nof memory access, operator type, the number of input and out-\nput channels, and the dimensions of the feature map. For operator\ntypes that have a significant impact on power consumption, fur-\nther extraction of their deep features are carried out to enhance\nthe representational capability of these features. For instance, in\nthe case of convolutional layers, we pay close attention to key\nparameters such as the dimensions of the convolutional kernels,\nthe quantity ",
  "nsions of the convolutional kernels,\nthe quantity of filters, and the stride length. As for Transformer\nmodules, their inherent complexity necessitates a comprehensive\nexamination. This includes delving into aspects like the number of\nheads in the attention mechanism, the dimensions of the matrices\ninvolved, as well as the parameters governing the fully connected\nand normalization layers. These attributes collectively reflect the\ncomputational and storage requirements of the layers, establishing",
  "d storage requirements of the layers, establishing\na direct correlation with their power consumption.\nGlobal Feature Extractor. We also need to engage in feature\nextraction at the level of entire blocks or DNNs, aiming to encapsu-\nlate the global power feature. Incorporating global features helps\nto circumvent potential misrepresentations that may arise from the\nlocality of features. Specifically, this task includes two facets.\n2",
  "①Extraction of Macro Structural Features: This encompasses\nanalyzing macro parameters of DNNs, such as the number of layers,\ndepth, types, residual, and branching structures. These parameters\nserve to illustrate the overall scale and the complexity of its topo-\nlogical structure, providing insights into the global power patterns.\n②Statistics and Aggregation of Features: This entails aggre-\ngating all the fine-grained features to generate a comprehensive\nsummary, including calculations of the net",
  "hensive\nsummary, including calculations of the network’s overall FLOPs,\ntotal number of parameters, and so forth. Additionally, an analy-\nsis of the proportions of various components can be conducted to\nelucidate the computational patterns.\n2.1.3 Power Behavior Similarity Clustering. We divide the oper-\nators in the network based on their power feature by employing\na power behavior similarity clustering method. This process not\nonly maps and quantifies the power distance between operators\nbut al",
  "tifies the power distance between operators\nbut also divides different power blocks based on this mapping. The\npower view, composed of these power blocks, serves as a logical\nintermediate representation that intuitively presents the main paths\nand areas where power usage is concentrated within the network.\nSuch clustering is crucial for subsequent energy efficiency opti-\nmization, as it allows us to target groups of operators with similar\npower characteristics for more effective DVFS instrumenta",
  "haracteristics for more effective DVFS instrumentation set.\nAlgorithm 1 embodies the core logic and implementation pro-\ncess of the clustering algorithm. Firstly, considering that in a multi-\ndimensional feature space, different features may have different\nscales and dimensions. The Mahalanobis distance [3] naturally ad-\njusts the scale of these features through the covariance matrix.\nTherefore, we quantify the power behavior similarity in the\nfeature space using the Mahalanobis distance.\nNext, ",
  "ature space using the Mahalanobis distance.\nNext, by introducing an inter-operator distance regularization\nterm in the distance calculation, we ensure that only physically ad-\njacent operators are considered, avoiding the erroneous clustering\nof operators that are not adjacent but have similar features.\nThen, based on the neighborhood radius ( 𝜖) and minimum num-\nber of points in a cluster ( 𝑚𝑖𝑛𝑃𝑡𝑠 ) hyperparameter of the DBSCAN\nalgorithm [ 2], the network is divided into multiple power blocks,\n",
  "he network is divided into multiple power blocks,\nforming a power view. This process not only simplifies the complex\nnetwork structure but also provides an intuitive and effective basis\nfor making DVFS target frequency decisions, greatly enhancing the\noperability and precision of adaptive DVFS.\nFinally, The post-processing of clustering results is primarily\naimed at adjusting and optimizing the outcomes of clustering to\nensure that the generated power blocks are continuous and prac-\ntically feas",
  "power blocks are continuous and prac-\ntically feasible within the network’s hierarchical structure. Post-\nprocessing not only deals with isolated points to ensure that each\nblock is appropriately addressed but also involves adjusting size,\nshape, or membership of clusters to achieve better power view.\nIn this part, building on basic power behaviors and relationships,\nwe have introduced clustering to enhance the understanding of\nDNN power patterns. This clustering has facilitated the implemen-\nta",
  ". This clustering has facilitated the implemen-\ntation of adaptive DVFS instrumentation points granularity.\n2.1.4 Adaptive Target Frequency Decision-Making. Utilizing the\npreviously constructed power view, we identify power blocks, guid-\ning the direction and methodology for precise energy efficiency\ncontrol. In this phase, DVFS instrumentation points are set before\neach block of the DNNs, based on the established power view. In\nthis process, the PowerLens adaptive presets target frequencies ati",
  " PowerLens adaptive presets target frequencies atinstrumentation points based on the characteristics and require-\nments of each power block. Specifically, as illustrated in Figure 2,\nwe employ a decision model where the input is the global features\nof each power block, and the output is the target frequency for that\nblock. For example, in the case of computation-intensive blocks,\nthe decision model will opt to increase the target frequency to alle-\nviate computational load. Conversely, for memor",
  "e-\nviate computational load. Conversely, for memory-intensive blocks,\nconsidering energy efficiency requirements, it will decide to reduce\nthe frequency. Then, during the inference of DNNs, adaptive DVFS\nare performed according to the preset instrumentation points and\ntarget frequencies, thereby enhancing energy efficiency.\nAlgorithm 1 Power Behavior Similarity Clustering\nRequire: Scaled power-sensitive deepwise features 𝑋;\nClustering hyperparameter: neighborhood radius 𝜖, least num-\nber of oper",
  "ter: neighborhood radius 𝜖, least num-\nber of operators 𝑚𝑖𝑛𝑃𝑡𝑠 ;𝛼,𝜆for distance calculation.\nEnsure: Power view\n1:𝑛←number of layers in 𝑋\n2:Compute covariance matrix 𝐶of𝑋\n3:𝑃←pseudo-inverse of 𝐶\n4:Initialize distance matrix 𝐷∈R𝑛×𝑛with zeros\n5:foreach pair of layers 𝑖,𝑗in𝑋do\n6:𝐷[𝑖,𝑗]←√︁\n(𝑋[𝑖]−𝑋[𝑗])⊤𝑃(𝑋[𝑖]−𝑋[𝑗])\n{Mahalanobis distance}\n7:end for\n8:Initialize spacing regularization matrix 𝑅∈R𝑛×𝑛with zeros\n9:foreach pair of layers 𝑖,𝑗in𝑋do\n10:𝑅[𝑖,𝑗]← exp(−𝜆·|𝑖−𝑗|){operator spacing}\n11:end for\n12:𝐷𝑖𝑠𝑡",
  "exp(−𝜆·|𝑖−𝑗|){operator spacing}\n11:end for\n12:𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 final←𝛼·𝐷[𝑖,𝑗]+(1−𝛼)·𝑅[𝑖,𝑗]\n13:𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠←DBSCAN(𝐷final,𝜖,𝑚𝑖𝑛𝑃𝑡𝑠)\n14:𝑛𝑒𝑤𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑠←processClusters(𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑠)\n{Ensure clusters are contiguous and non-overlapping}\n15:return𝑛𝑒𝑤𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑠\n2.2 Model Training Phase\nThe model training phase is a foundational phase of the PowerLens,\nencompassing multiple steps such as dataset generation, model\ntraining. In this phase, through a carefully designed data generator\nand training process, key datasets for pred",
  "erator\nand training process, key datasets for prediction are constructed,\nand two core prediction models are trained: the clustering parameter\nprediction model and the target frequency decision model.\nFigure 3: The clustering parameter prediction model.\nAs illustrated in Figure 2, the role of the dataset generator is to\ngenerate two groups of datasets. It first uses a DNN generator to\nproduce a large variety of neural networks by randomly combin-\ning features mentioned in section 2.1.2. These ne",
  "\ning features mentioned in section 2.1.2. These networks are then\nsubjected to clustering algorithms with varying hyperparameters,\nresulting in corresponding power views. Each block in the power\n3",
  "view is deployed at all frequencies to select test data that achieves\nthe optimal energy efficiency. Ultimately, two groups of datasets\nare outputted: Dataset A contains the global features of the neu-\nral networks and their corresponding clustering hyperparameters.\nMeanwhile, Dataset B includes the global features of each block\nand its optimal frequency.\nThe clustering hyperparameter prediction model is trained with\nthe Dataset A, with its network structure depicted in Figure 3. As\ndescribed in",
  "rk structure depicted in Figure 3. As\ndescribed in the global feature extractor of section 2.1.2, the global\nfeatures of the DNN are divided into macro structural features and\nstatistics features. Structural features are input at the beginning\nstage of the model to establish a basic understanding of the DNN\nstructure. Statistics features are input during the mid-stage of the\nnetwork to further enhance the prediction accuracy based on the\nexisting structural understanding. This model achieves an ",
  " structural understanding. This model achieves an accuracy\nof 92.6%1on the test set.\nThe decision model, trained with Dataset B, is shown in Figure\n4. It predicts the target frequency settings based on the global\nfeatures of each block. This is fundamentally a classification task,\ninvolving choosing between several frequency levels supported by\nthe hardware, and it achieves an accuracy of 94.2%1on the test set.\nMoreover, even in cases of prediction deviation, the predicted target\nfrequency is on",
  "on deviation, the predicted target\nfrequency is only one or two level away from the actual optimal\nfrequency, thereby not significantly compromis.\nFigure 4: The target frequency decision model.\nAfter extensive training and validation, prediction models pro-\nvides solid predictive data support for the entire framework, ensur-\ning the accuracy and effectiveness of optimization.\n2.3 Framework Overhead Analysis.\n2.3.1 Offline Overhead Analysis. PowerLens works offline, and its\noffline overhead prima",
  "Lens works offline, and its\noffline overhead primarily arises from model training and work-\nflow. Training the clustering hyperparameter prediction model and\nthe decision model requires data collection and training until con-\nvergence. Moreover, this method eliminates the need for manual\nintervention, as transferring it to a new hardware platform simply\ninvolves the automated generation of datasets and training. The\noverhead from workflow mainly involves feature extraction, hyper-\nparameter pred",
  "involves feature extraction, hyper-\nparameter prediction, clustering, predicting target frequencies, and\ngenerating power views. These are all completed before executing\ninference, hence they do not affect the performance of runtime.\n2.3.2 Runtime Performance Overhead Analysis. The runtime over-\nhead brought by the framework stems from two sources: ①The\nDVFS commands consume processor resources, thereby creating\noverhead. However, PowerLens achieves adaptive DVFS instrumen-\ntation granularity th",
  "ves adaptive DVFS instrumen-\ntation granularity through clustering, thereby targeting only the\nkey power blocks and reducing performance degradation caused by\n1A total of 8000 random networks were generated, resulting in 31,242 block data. They\nare divided into training, validation, and test sets in an 80%-10%-10% ratio.frequent frequency adjustments. Consequently, this precise control\ncan effectively manage and balance such overhead, ensuring that it\nthere is no significantly impact the overall",
  "at it\nthere is no significantly impact the overall runtime performance.\n②PowerLens adaptively sets frequencies for each block, including\nturning down frequency for some blocks, which may lead to in-\ncreased time overhead for these blocks. However, this performance\nsacrifice is made to achieve a higher energy efficiency ratio. This\ntrade-off is often worthwhile, as it maintains or enhances overall\nenergy efficiency while sacrificing only a small portion of perfor-\nmance. Importantly, this approac",
  "ortion of perfor-\nmance. Importantly, this approach does not alter the structure or\ncomputational intensity of the neural network, thus not affecting\nthe accuracy of inference.\n3 EXPERIMENTAL EVALUATION\n3.1 Experimental Setup\nHardware Platforms. We deploy the PowerLens on two platforms,\nNVIDIA Jetson AGX and NVIDIA Jetson TX2. Some necessary run-\ntime environments are installed, such as Jetpack 4.6.2, Ubuntu 16.04,\nCUDA 10.2, torchvision 0.12, PyTorch 1.12.0, working mode MAXN\non AGX and TX2. Tw",
  "Torch 1.12.0, working mode MAXN\non AGX and TX2. Two platforms are configured with different\nGPU frequencies and batch sizes: On the AGX, frequencies ranges\nfrom 114MHz to 1370MHz across 14 levels. On the TX2, frequencies\nranges from 114MHz to 1300MHz across 13 levels. To calculate the\nenergy efficiency, we monitor real-time power by the performance\nmanagement tool (tegrastats) in Jetson platform.\nModels and Datasets. As shown in Table 1, 12 DNNs from\ntorchvision with different sizes and computat",
  "from\ntorchvision with different sizes and computational complexity are\nused in our prediction accuracy and energy efficiency comparison\nexperiments. The image data used for our test inference process is\nfrom ImageNet. Each energy efficiency test is required to be run 50\ntimes on randomized different inputs to obtain an average result.\nBaselines for comparison. To demonstrate the effectiveness\nand superiority of the proposed PowerLens, we use three base-\nlines for comparison: ①The built-in method",
  "e base-\nlines for comparison: ①The built-in method (BiM) [ 7]. We choose\nthe ondemand method built into both hardware platforms, which\nare widely used frequency scaling methods relying on historical\nhardware. ②TheFPG [5]. A new heuristic DVFS method is called\nFPG-C+G in this section. This method dynamically adjusts the CPU\nand GPU frequencies during runtime based on performance, power,\nenergy delay product, and CPU/GPU utilization. ③TheFPG-G is\na variant of the FPG-C+G that retains the ondemand ",
  " variant of the FPG-C+G that retains the ondemand for CPU and\nonly adjusts the frequency scaling strategy for the GPU.\nExperimental Metric. To quantitatively assess energy effi-\nciency, we employ the equation 1 to calculate energy efficiency [ 20].\nEnergy efficiency is considered a positive indicator, with higher\nvalues indicating better performance in terms of energy utilization.\n𝐸𝐸𝑚𝑜𝑑𝑒𝑙 =𝐼𝑚𝑎𝑔𝑒𝑠\n𝐸=𝐹𝑃𝑆×𝑡\n¯𝑃×𝑡=𝐹𝑃𝑆\n¯𝑃(𝐼𝑚𝑎𝑔𝑒𝑠/𝐽), (1)\nwhere𝑖𝑚𝑎𝑔𝑒𝑠 is the number of images processed by the model. 𝐸\nis ",
  "the number of images processed by the model. 𝐸\nis the energy consumption, ¯𝑃is the average power, and 𝑡is the\ninference time. 𝐹𝑃𝑆 represents frames per second.\n3.2 Results\n3.2.1 Energy efficiency improvement. In Table 1, we present the\nexperimental results for energy efficiency improvement. Compared\nto BiM, PowerLens achieves an average improvement of 57.85% on\n4",
  "TX2 and an average of 119.42% on AGX. In addition, PowerLens\nachieves an average energy efficiency improvement of 18.39% on\nTX2 and 27.31% on AGX compared to the FPG-G method. Compared\nto the FPG-C+G method, which configures both CPU and GPU\nfrequencies simultaneously, PowerLens achievies an average energy\nefficiency improvement of 13.53% on TX2 and 15.97% on AGX,\ndespite only configuring GPU frequencies for PowerLens.\nTable 1: Results of energy efficiency improvement.\n(a) Energy efficiency impr",
  "efficiency improvement.\n(a) Energy efficiency improvement on TX2\nmodel name Block1BiM2FPG-G2FPG-CG2\nalexnet 1 38.60% 2.94% 1.31%\ngooglenet 1 30.10% 6.89% 4.32%\nvgg19 2 43.40% 23.00% 20.76%\nmobilenet_v3 1 29.76% 6.55% 3.96%\ndensenet201 3 35.76% 7.32% 5.53%\nresnext101 4 79.79% 25.97% 21.07%\nresnet34 1 41.86% 4.82% 1.45%\nresnet152 3 59.85% 32.88% 24.10%\nregnet_x_32gf 3 123.80% 15.47% 11.23%\nregnet_y_128gf 4 131.71% 29.12% 20.59%\nvit_base_16 1 36.95% 40.46% 24.70%\nvit_base_32 1 42.67% 25.32% 23.39%\n",
  " 40.46% 24.70%\nvit_base_32 1 42.67% 25.32% 23.39%\nAverage 57.85% 18.39% 13.53%\n(b) Energy efficiency improvement on AGX\nmodel name Block1BiM2FPG-G2FPG-CG2\nalexnet 1 26.17% 10.55% 3.80%\ngooglenet 2 113.78% 7.55% 5.81%\nvgg19 2 134.30% 37.78% 20.66%\nmobilenet_v3 1 144.37% 6.40% 3.56%\ndensenet201 2 132.36% 11.49% 9.35%\nresnext101 3 131.40% 38.78% 20.11%\nresnet34 2 133.72% 3.97% 2.34%\nresnet152 4 129.27% 49.87% 36.98%\nregnet_x_32gf 2 129.40% 12.39% 8.89%\nregnet_y_128gf 6 144.34% 45.37% 24.30%\nvit_bas",
  "89%\nregnet_y_128gf 6 144.34% 45.37% 24.30%\nvit_base_16 1 104.87% 67.90% 36.21%\nvit_base_32 1 104.87% 67.90% 36.21%\nAverage 119.42% 27.31% 15.97%\nFrom Table 1, we can also make the following observations:\n①Smaller networks, such as 𝐴𝑙𝑒𝑥𝑁𝑒𝑡 and𝑀𝑜𝑏𝑖𝑙𝑒𝑁𝑒𝑡𝑉 3, lack a\nsufficient number of operators for clustering. This limitation hin-\nders the potential to achieve significant gains with adaptive DVFS\napproaches. As a result, there is only few effect on improving en-\nergy efficiency. ②Complex networks ",
  " improving en-\nergy efficiency. ②Complex networks can be clustered into more\npower blocks, and the number of blocks is positively correlated\nwith energy efficiency improvement. For example, the comparison\nbetween𝑅𝑒𝑠𝑁𝑒𝑡 34and𝑅𝑒𝑠𝑁𝑒𝑡 152, and between 𝑅𝑒𝑔𝑁𝑒𝑡 _𝑋_32𝐺𝐹\nand𝑅𝑒𝑔𝑁𝑒𝑡 _𝑌_128𝐺𝐹.③For networks composed of repeated com-\nponents, PowerLens treats continuous repeated components as a\nwhole block and makes decisions for the optimal frequency. This is\nfacilitated by the considering of power behavior ",
  "\nfacilitated by the considering of power behavior similarity in clus-\ntering. For instance, PowerLens treats the connections of repeated\n𝑡𝑟𝑎𝑛𝑠𝑓𝑜𝑟𝑚𝑒𝑟 modules in the 𝑉𝑖𝑇 model as a large power block.\n3.2.2 The performance in the inference task flow. We randomly\nassemble 100 inference tasks by combining the DNNs listed in\nTable 1. Each task includes 50 three-channel 224×224images.\n1Blocks represent the number of power blocks obtained in this dnn by clustering.\n1The BiM, FPG-G and FPG-C+G columns ar",
  "clustering.\n1The BiM, FPG-G and FPG-C+G columns are the energy efficiency (EE) gains of Power-\nLens relative to the comparative benchmark methods ( 𝐸𝐸𝑝𝑜𝑤𝑒𝑟𝑙𝑒𝑛𝑠 -𝐸𝐸𝐵𝑖𝑀)/𝐸𝐸𝐵𝑖𝑀.As shown in Figure 5, in the experimental results for task flow\nprocessing, PowerLens exhibits the lowest energy consumption and\nthe highest energy efficiency among the four methods. However,\nthere is a certain degree of increase in inference time, due to the\nruntime overhead discussed in section 2.3.2. Compared to FPG-G,\nFP",
  " discussed in section 2.3.2. Compared to FPG-G,\nFPG-CG, and BiM, PowerLens achieves energy reductions of 26.60%,\n22.18%, and 48.58% on TX2, and 28.95%, 18.45%, and 50.64% on\nAGX, respectively. In terms of time, PowerLens increases task flow\nprocessing time by 6.13%, -0.54%, and 9.91% on TX2, and 14.03%,\n-2.30%, and 16.82% on AGX, respectively. Moreover, PowerLens\nachieves energy efficiency gains of 36.24%, 28.49%, and 94.48% on\nTX2, and 40.75%, 22.62%, and 102.60% on AGX, respectively.\n(a) Energ",
  "2.62%, and 102.60% on AGX, respectively.\n(a) Energy(𝐽)\n(b) Time(𝑠)\n(c) Energy efficiency (𝐼𝑚𝑎𝑔𝑒𝑠/𝐽)\nFigure 5: The experimental results in task flow processing.\n3.2.3 Ablation studies. To demonstrate the effectiveness of power\nbehavior similarity clustering in Algorithm 1, we compare Power-\nLens with two variations that employ different strategies. Specifi-\ncally, P-R represents the approach where the clustering algorithm\nis replaced with random block partitioning, and P-N denotes the\napproach th",
  "lock partitioning, and P-N denotes the\napproach that does not use any clustering algorithm and directly\nmakes frequency decisions for entire DNN. The energy efficiency\nlosses relative to PowerLens for these two methods are shown in\nTable 2. PowerLens with Algorithm 1 outperforms the others.\nTable 2: The energy efficiency loss for different clustering\nDNN ModelsTX2 AGX\nP-R P-N P-R P-N\nalexnet -26.49% -20.55% -31.49% -3.45%\ngooglenet -34.06% -8.15% -99.43% -8.06%\nvgg19 -30.57% -25.75% -74.25% -17.",
  " -99.43% -8.06%\nvgg19 -30.57% -25.75% -74.25% -17.36%\nmobilenet_v3 -49.31% -19.18% -43.02% -10.18%\ndensenet201 -25.23% -9.13% -27.71% -14.73%\nresnext101 -69.52% -31.88% -23.85% -28.95%\nresnet34 -66.84% -6.25% -85.46% -8.62%\nresnet152 -62.35% -21.59% -49.05% -27.49%\nregnet_x_32gf -35.78% -16.61% -69.37% -18.17%\nregnet_y_128gf -21.40% -16.37% -50.17% -68.55%\nvit_base_16 -42.62% -5.06% -96.81% -11.29%\nvit_base_32 -47.06% -1.58% -21.33% -2.46%\nAverage -42.60% -15.17% -55.99% -18.28%\n5",
  "e -42.60% -15.17% -55.99% -18.28%\n5",
  "3.3 Overhead Analysis\nThe time overhead of PowerLens can be divided into offline stage\noverhead (discussed in section 3.3.1) and runtime stage overhead\n(discussed in section 3.3.2). The offline overhead of PowerLens is\nshown in Table 3. Moreover, to understand the runtime overhead,\nwe have changed the DVFS level for 100 times and measured its\naverage time overhead, which is 50ms for the device used in the\nexperiments. The time overhead caused by turning down frequency\ndepends on the specific sce",
  "turning down frequency\ndepends on the specific scenario and is analyzed in Figure 5.\nTable 3: Offline overhead of PowerLens.\nPhase TX2 AGX\nModel Trainingclustering hyperparameter\nprediction model20h 15h\ndecision model 6h 4.5h\nWorkflowfeature extraction 10s 10s\nhyperparameter prediction 320ms 150ms\nclustering 60s 60s\ndecision of each block 220ms 130ms\n4 RELATED WORK\nModel modification. Numerous works has been conducted on\nmodifying models to achieve energy efficiency improvements [ 4,\n17], such a",
  "ve energy efficiency improvements [ 4,\n17], such as pruning [ 17], quantization [ 4], knowledge distillation\n[8]. In addition, some studies have shortened the inference time\nby selectively skipping certain operators or blocks [ 16]. However,\nthese methods inevitably disrupt the original design and structure\nof DNNs, and may even result in reduced accuracy [18].\nDVFS methods are favored due to their ability to optimize en-\nergy efficiency without modifying DNNs, and they are typically\northogonal ",
  "modifying DNNs, and they are typically\northogonal to most model modification research [ 14]. Traditional\nDVFS methods typically follow predefined rules and struggle to\neffectively manage complex workloads [ 6]. Hence, researchers have\nexplored various rule-based [ 7] and learning-based [ 9] DVFS. Re-\ncently, NeuOS [ 1] has been customized for DNN tasks, and zTT [ 6]\nintroduces factors like quality of service. However, these methods\nprimarily rely on hardware states and historical information gat",
  " on hardware states and historical information gath-\nered from previous executions [ 14], essentially following a heuristic\nthinking. They still cannot fully address the limitations mentioned\nin section 1, such as frequency ping-pong and lag responses.\nRecent approaches have explored synergizing DVFS technology\nwith factors like batchsize [ 15], adaptively balancing throughput\nand power, which however are orthogonal to the adaptive DVFS in\nthis paper. Some other methods considers expanding the c",
  "aper. Some other methods considers expanding the control\nscope of DVFS, such as integrating CPU and GPU control strategies\n[5,19]. However, the substantial differences between CPU and GPU\ntasks can lead to a significant declines in control efficiency for one\nside due to resource changes on the other side. Nevertheless, the\nidea of coordinating multiple components for DVFS inspires our\nfuture directions for extending PowerLens.\n5 CONCLUSION\nThe PowerLens extracts power-sensitive features from DNN",
  "werLens extracts power-sensitive features from DNNs and\nperforms clustering based on power behavior similarity, accurately\nidentifying critical power blocks. We preset DVFS instrumentation\npoints and target frequencies in front of each power block based\non the clustered power view. The experimental results show that\nPowerLens achieves an average energy efficiency improvements of88.64%, 22.85%, and 14.75%, respectively, compared to three baseline\nmethods. In the future, we will incorporate more c",
  "methods. In the future, we will incorporate more configurable opti-\nmization options into PowerLens, such as CPU DVFS and batchsize.\nAdditionally, we plan to apply PowerLens in cloud servers, where\nmore complex and diverse tasks can yield greater benefits.\nACKNOWLEDGMENTS\nThis work was partially supported by the National Key R&D Pro-\ngram of China (2022YFB4501600), the National Natural Science\nFoundation of China (No.62102390), the special fund for Jiangsu\nNatural Resources Development (Innovati",
  "or Jiangsu\nNatural Resources Development (Innovation Project of Marine\nScience and Technology, No.JSZRHYKJ202218), and the National\nKey Laboratory of Science and Technology on Space Micrwave\n(No.HTKJ2022KL504021).\nREFERENCES\n[1]Soroush Bateni and Cong Liu. 2020. NeuOS: A Latency-Predictable Multi-\nDimensional Optimization Framework for DNN-driven Autonomous Systems. In\nUSENIX Annual Technical Conference. 371–385.\n[2]Rupanka Bhuyan and Samarjeet Borah. 2023. A Survey of Some Density Based\nCluster",
  "orah. 2023. A Survey of Some Density Based\nClustering Techniques. CoRR abs/2306.09256 (2023).\n[3]Qiang Chen, Weizhong Yu, and et al. 2023. Rooted Mahalanobis distance based\nGustafson-Kessel fuzzy C-means. Inf. Sci. 644 (2023), 118878.\n[4]Insu Choi, Jae-Youn Hong, JaeHwa Jeon, and Joon-Sung Yang. 2023. RQ-DNN:\nReliable Quantization for Fault-tolerant Deep Neural Networks. In 60th ACM/IEEE\nDesign Automation Conference, DAC 2023. IEEE, 1–2.\n[5]Meruyert Karzhaubayeva, Aidar Amangeldi, and Jurn-Gyu P",
  "ert Karzhaubayeva, Aidar Amangeldi, and Jurn-Gyu Park. 2023. CNN\nWorkloads Characterization and Integrated CPU-GPU DVFS Governors on Em-\nbedded Systems. In IEEE Embedded Systems Letters. 1–1.\n[6]Seyeon Kim, Kyungmin Bin, Sangtae Ha, Kyunghan Lee, and Song Chong. 2021.\nzTT: Learning-Based DVFS with Zero Thermal Throttling for Mobile Devices.\nGetMobile Mob. Comput. Commun. 25, 4 (2021), 30–34.\n[7]Young Geun Kim, Joonho Kong, and Sung Woo Chung. 2018. A Survey on Recent\nOS-Level Energy Management T",
  "8. A Survey on Recent\nOS-Level Energy Management Techniques for Mobile Processing Units. IEEE\nTrans. Parallel Distributed Syst. 29, 10 (2018), 2388–2401.\n[8]Shaojie Li and et al. 2023. Distilling a Powerful Student Model via Online Knowl-\nedge Distillation. IEEE Trans. Neural Networks Learn. Syst. 34, 11 (2023).\n[9]Xiao Li, Lin Chen, Shixi Chen, and et al. 2022. Power Management for Chiplet-\nBased Multicore Systems Using Deep Reinforcement Learning. In IEEE Computer\nSociety Annual Symposium on V",
  "ng. In IEEE Computer\nSociety Annual Symposium on VLSI, ISVLSI 2022. IEEE, 164–169.\n[10] Xinmei Li, Lei Mo, and et al. 2023. Approximation-Aware Task Deployment on\nHeterogeneous Multicore Platforms With DVFS. IEEE Transactions on Computer-\nAided Design of Integrated Circuits and Systems 42, 7 (2023), 2108–2121.\n[11] Xiangjie Li, Yingtao Shen, An Zou, and Yehan Ma. 2023. EENet: Energy Efficient\nNeural Networks with Run-time Power Management. In 60th ACM/IEEE Design\nAutomation Conference, DAC 2023.",
  "h ACM/IEEE Design\nAutomation Conference, DAC 2023. IEEE, 1–6.\n[12] Chengdong Lin, Kun Wang, Zhenjiang Li, and Yu Pu. 2023. A Workload-Aware\nDVFS Robust to Concurrent Tasks for Mobile Devices. In ACM MobiCom 2023,\nMadrid, Spain, October 2-6, 2023. ACM, 19:1–19:16.\n[13] Weihong Liu, Jiawei Geng, Zongwei Zhu, and et al. 2022. Sniper: cloud-edge\ncollaborative inference scheduling with neural network similarity modeling. In\nDAC ’22: 59th ACM/IEEE Design Automation Conference. ACM, 505–510.\n[14] Franc",
  "gn Automation Conference. ACM, 505–510.\n[14] Francisco Mendes and et al. 2022. Decoupling GPGPU voltage-frequency scaling\nfor deep-learning applications. J. Parallel Distributed Comput. 165 (2022), 32–51.\n[15] Seyed Morteza Nabavinejad, Sherief Reda, and Masoumeh Ebrahimi. 2022. Coor-\ndinated Batching and DVFS for DNN Inference on GPU Accelerators. IEEE Trans.\nParallel Distributed Syst. 33, 10 (2022), 2496–2508.\n[16] Lois Orosa, Skanda Koppula, and et al. 2022. EcoFlow: Efficient Convolutional\nD",
  "nd et al. 2022. EcoFlow: Efficient Convolutional\nDataflows for Low-Power Neural Network Accelerators. CoRR (2022).\n[17] Richard Petri, Grace Li Zhang, Yiran Chen, and et al. 2023. PowerPruning: Select-\ning Weights and Activations for Power-Efficient Neural Network Acceleration.\nIn60th ACM/IEEE Design Automation Conference, DAC 2023. IEEE, 1–6.\n[18] Adrian Schwaiger, Kristian Schwienbacher, and Karsten Roscher. 2022. Beyond\nTest Accuracy: The Effects of Model Compression on CNNs. In Proceedings o",
  "cts of Model Compression on CNNs. In Proceedings of the\nWorkshop on Artificial Intelligence Safety 2022 (SafeAI 2022), Vol. 3087.\n[19] Qizhen Weng, Wencong Xiao, and et al. 2022. MLaaS in the Wild: Workload\nAnalysis and Scheduling in Large-Scale Heterogeneous GPU Clusters. In 19th\nUSENIX Symposium on Networked Systems Design and Implementation, NSDI 2022.\n[20] Chunrong Yao and et al. 2021. Evaluating and analyzing the energy efficiency of\nCNN inference on high-performance GPU. Concurr. Comput. P",
  "erence on high-performance GPU. Concurr. Comput. Pract. Exp. (2021).\n[21] Chunrong Yao and et al. 2022. EAIS: Energy-aware adaptive scheduling for CNN\ninference on high-performance GPUs. Future Gener. Comput. Syst. 130 (2022).\n6",
  "Efficient Weight Mapping and Resource Scheduling\non Crossbar-based Multi-core CIM Systems\nHanjie Liu†, Sifan Sun†, Aifei Zhang‡, Haiyan Qin†, Yutong Wu‡, Minhao Gu‡,\nShihang Fu‡, Shuaikai Liu‡, Baosen Liu‡, Wang Kang∗†\n†National Key Laboratory of Spintronics, Hangzhou International Innovation Institute;\nSchool of Integrated Circuit Science and Engineering, Beihang University, China\n‡Zhicun Research Lab, Beijing, China\n∗Corresponding author: wang.kang@buaa.edu.cn\nAbstract —Crossbar-based computin",
  "kang@buaa.edu.cn\nAbstract —Crossbar-based computing-in-memory (CIM) sys-\ntems facilitate large-scale parallel multiply-and-accumulate\n(MAC) operations, while a domain-specific compiler (DSC) plays\na pivotal role in optimizing the deployment of neural network\nalgorithms on such systems. With the development of multi-core\nand large-core architectures, some key compiler problems such\nas high parallel processing, resource utilization, and crossbar\narray assignment methods have not been solved. For l",
  "ray assignment methods have not been solved. For low-latency\napplication scenarios, we have designed a resource scheduling\nstrategy for our hardware system based on stream data pro-\ncessing to reduce the latency caused by intra-core and inter-\ncore communication. Additionally, a weight mapping strategy has\nbeen developed to maximize the potential of crossbar arrays in\nconvolutional neural networks (CNNs) deployment. Experimental\nresults on our multi-core eFlash-based CIM system-on-chip (SoC)\ndem",
  "lti-core eFlash-based CIM system-on-chip (SoC)\ndemonstrate that these two technologies help CNNs achieve\na 76% reduction in latency, a 30% improvement in resource\nutilization, and the use rate of crossbar array that can reach up\nto 94.7%.\nIndex Terms—CIM, CNN, Compiler, Weight mapping, Re-\nsource schedule\nI. I NTRODUCTION\nIn recent years, the extensive application of convolutional\nneural networks (CNNs) in machine learning has generated\na substantial demand for efficient architectures to manage\n",
  "tial demand for efficient architectures to manage\nthese computationally and data-intensive tasks. In response,\nvarious neural network accelerators [1], [2] have emerged.\nHowever, these CMOS-based devices, which adhere to the von\nNeumann architecture, are now facing the challenge posed by\n“the memory wall” [3].\nComputing-in-memory (CIM) technology provides a\npromising solution for accelerating artificial intelligence\ninference by performing analog computations directly in\nmemory, potentially redu",
  " computations directly in\nmemory, potentially reducing latency and power consumption.\nTo effectively implement CIM, various techniques have been\nproposed, including commercial memory types such as\nSRAM, DRAM, and flash memory, as well as emerging\nnon-volatile memory (NVM) technologies like ReRAM,\nPCM, FeFET, and MRAM [4]. The two-dimensional crossbar\narrays constructed from these devices exhibit high storage\ndensity and the capability for parallel in-situ computation,\nthus garnering significant ",
  "l in-situ computation,\nthus garnering significant academic interest.\nThe development of macro designs for crossbar-based CIM\naccelerators has advanced rapidly, yet there remains a lack of\ndetailed consideration for the execution of CNNs, necessitating\nThis work was supported by the Beijing MSTC Program\n(Z231100007423019), Beijing Natural Science Foundation (L223004),\nNatural Science Foundation of China (62274008).manual intervention for actual deployment. As task complex-\nity increases and neura",
  "ployment. As task complex-\nity increases and neural networks continue to deepen, the\ndemand for computational power also grows. Consequently,\nmacro designs are evolving towards multi-core and large-core\narchitectures, and there is an increasing call for automated\ndeployment solutions.\nCurrently, the automation of compilation in some CIM sys-\ntems focuses on small-core implementation strategies [5], [6].\nThese strategies enhance parallelism by partitioning convolu-\ntion weights and distributing t",
  "titioning convolu-\ntion weights and distributing them across different cores for\ncomputation. However, this splitting introduces the need for\nsumming and concatenating multi-core output results, which\ncan accumulate significant quantization errors during actual\ncomputations [7]. While large-core computation eliminates\nthese issues, it presents new challenges in optimizing resource\nutilization and data flow.\nIn our work, we achieve automated optimization and deploy\nCNNs on a multi-core CIM system",
  "ization and deploy\nCNNs on a multi-core CIM system, resulting in efficient, low-\nlatency, and highly parallel task execution. Building on multi-\ncore collaboration and large crossbar arrays, we propose two\ntechnical improvements for resource utilization. Our specific\ncontributions are as follows:\n•A multi-level abstract architecture and execution logic\nfor a multi-core CIM system is proposed, serving as the\nfoundation for detailed research on CNNs execution.\n•A scheduling process is designed to ",
  "s execution.\n•A scheduling process is designed to improve system\ncomputational resource utilization and optimize latency\ncaused by bandwidth limitations through resource pool-\ning, static random allocation, and iterative evaluation.\n•A new weight mapping strategy is designed for lager-\ncore, which leverages weighted expansion to exploit the\npotential of crossbar arrays in processing convolutions.\n•Experimental results on our multi-core CIM system-on-\nchip (SoC) demonstrate a 76% reduction in lat",
  "-on-\nchip (SoC) demonstrate a 76% reduction in latency, a\n30% improvement in resource utilization, and a crossbar\narray utilization rate of up to 94.7%.\nII. B ACKGROUND AND MOTIVATION\nFig. 1 illustrates the basic operational unit for analog vector-\nmatrix multiplication (VMM) in CIM systems, implemented\nwithin a two-dimensional crossbar array structure. This com-\nputational strategy can be categorized into two types based on\nthe physical properties of the devices [8].2025 62nd ACM/IEEE Design Au",
  "es of the devices [8].2025 62nd ACM/IEEE Design Automation Conference (DAC) | 979-8-3315-0304-8/25/$31.00 ©2025 IEEE | DOI: 10.1109/DAC63849.2025.11132743\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  "A. Crossbar-based CIM\nFig. 1(a) shows the floating-gate memory cells array. In the\nsubthreshold region, the leakage current of floating-gate de-\nvices exhibits an exponential dependence on the gate voltage.\nAt the same time, the gate voltage difference can represent the\nratio of input to output current. Therefore, we can program\nthe device by adjusting the charge at the cell intersections and\nthe peripheral unit charge, with the output current component\nexpressed as: IΣi=WiIi. In the NVM crossba",
  "mponent\nexpressed as: IΣi=WiIi. In the NVM crossbar structure\nshown in Fig. 1(b), the conductance value of each cell is\nprogrammable, and the current at each intersection is given\nby:IΣi=GiVi. According to Kirchhoff’s law, the cumulative\ncurrent value in each column is the sum of its components.\nThus, the crossbar array completes the VMM operation.\nFig. 1. Analog VMM circuits with Floating-gate crossbar and NVM crossbar.\nNote the difference in Op-Amp gain requirements.\nNVM devices have fast writ",
  "-Amp gain requirements.\nNVM devices have fast write speeds and high write en-\ndurance, showcasing strong development potential. In contrast,\nfloating-gate devices boast high density and mature processes,\nmaking them more advantageous in large-core designs [9].\nGiven that our SoC is built on floating-gate technology, our\napproach focuses on studying CIM systems based on large-\nscale floating-gate arrays.\nB. Auto Development for CNNs\nTo achieve automated deployment of CNNs in CIM systems,\nindustry",
  "omated deployment of CNNs in CIM systems,\nindustry and academia [10], [11] have introduced domain-\n...\nAdd & Concat Small \nCore Small \nCore \nLarge \nCore \nFig. 2. Weight mapping of CNNs on different size of crossbar. KwandKh\nrepresent the width and height of convolution kernel, CinandCoutrepresent\nthe number of input and output channels. Weights are flattened and storage\nin the crossbar.specific compilers (DSC). These compilers typically con-\nsist of frontend and backend. The frontend is responsi",
  " of frontend and backend. The frontend is responsible\nfor hardware-agnostic optimizations, including computational\ngraph and operator optimization, often converting other oper-\nators into matrix computation operators such as convolution\nand fully connected. The backend emphasizes optimization\nstrategies for the storage and computation of matrices involved\nin matrix-vector multiplication (MVM) as shown in Fig. 2.\nIn systems based on NVM, to perform computations of\nlarger-scale weight matrices wit",
  "m computations of\nlarger-scale weight matrices within the limited size of crossbar\narrays. Studies [5], [11] have adopted methods that split the\noriginal weights and distribute them across multiple cores for\ncomputation. This approach addresses the problem and en-\nhances computational parallelism, Nevertheless, reorganizing\nthe computation results introduces additional computational\noverhead and quantization errors.\nIn floating-gate and future NVM-based systems, when the\nweight matrix is smaller",
  "M-based systems, when the\nweight matrix is smaller than the crossbar array, the entire\nmatrix can be stored in a single core, thus eliminating the\nneed for splitting computations and subsequent summation and\nstitching operations. However, maintaining high parallelism\nbecomes a new challenge. Additionally, with the increase in\nsingle-core computational tasks, the optimization space for\ndata flow both within and between cores also expands.\nIII. A RCHITECTURE OF CIM S YSTEM\nWe propose an abstract a",
  "CHITECTURE OF CIM S YSTEM\nWe propose an abstract architecture for a multi-core CIM\nsystem, which is compatible with widely adopted multi-level\nstructures [12]. Based on this architecture, we developed an\ninference execution model tailored for low-latency application\nscenarios, enabling comprehensive analysis and evaluation of\noptimization strategies in the compilation backend.\nA. Hardware Abstraction\nFig. 3 illustrates the proposed multi-level architecture,\nwhich sequentially expands from the Ch",
  "chitecture,\nwhich sequentially expands from the Chip level to the Core,\nMatrix Computing Unit (MCU) and Crossbar levels.\nAt the highest level, the architecture consists of a two-\ndimensional homogeneous array of computing modules and a\nglobal buffer, where each computing module includes a CIM\ncore and an inter-buffer. Modules are interconnected through a\nNetwork-on-Chip (NoC). The CNN parameters are pre-stored\nin the cores, and data exchange between cores occurs through\nthe inter-buffer and glob",
  "een cores occurs through\nthe inter-buffer and global buffer. Different cores execute\noperations asynchronously, using a trigger-off mechanism for\nsynchronization during inter-core communication.\nEach core comprises a control unit, an inner-buffer, a set\nof MCUs, and other computation units (OCUs). These OCUs\ninclude element-wise operations (e.g., batch normalization),\nvector operations (e.g., pooling), and transformation operations\n(e.g., sampling). The computation units within the core can\nonly",
  "g). The computation units within the core can\nonly access the data stored in the inner-buffer.\nEach MCU consists of input/output first-in-first-out (FIFO)\nqueues, pre-processing and post-processing units, and a set\nof crossbar arrays. The pre- and post-units perform operations\nsuch as shifting, clipping, expansion, and activation to support\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  " IEEE Xplore.  Restrictions apply.",
  "Inner-BufferControl LogicTransfer \nScalar \nVector Matrix\nUnitRouting\nGobal Buffer......\n...\n... ......\n...\n...\nCIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer \n......\nCIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer \nCIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer Routing Routing\nRouting Routing Routing\nRouting Routing Routing\nRouting Routing Routing\nCIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer ...\n",
  "M\nCore\nIntra- \nBuffer CIM\nCore\nIntra- \nBuffer ...\nArray\nControlInput FIFO \nOutput FIFOPre-processing \nUnit \nPost-processingCIM\nBlock\nDAC \nWL Decoder \nADCTIA...........................BL Decoder \nADCTIA\nADCTIA\nADCTIAFig. 3. Multilevel architecture of CIM system. Multiple cores interconneted through NoC in the top level. Each core contains various computing units, with\nMCU featuring multiple crossbars that share a common pre- and post-processing system. (a) Chip level. (b) Core level. (c) MCU leve",
  "stem. (a) Chip level. (b) Core level. (c) MCU level. (d) Crossbar level.\nthe fusion and acceleration of common operations before and\nafter on convolution layers in CNNs.\nSince the crossbar operates in the analog domain while the\nother circuits in the system function in the digital domain, ba-\nsic peripheral circuits such as ADC/DAC and transimpedance\namplifiers (TIA) are also required in the CIM Block.\nB. Inference Execution\nAlgorithmic network tasks are divided into multiple chip\npipelines for ",
  "asks are divided into multiple chip\npipelines for execution. Each pipeline covers a subnet, with\nits outputs stored in the global buffer, serving as the input for\nthe subsequent pipelines.\nEach chip pipeline is further divided into tasks, completed\neither within a single core or collaboratively across multiple\ncores. The outputs of each task, along with intermediate results\nacross cores, are written into the inter-buffer.\nInner-core computations interact through a bank of inner-\nbuffer. Each com",
  "interact through a bank of inner-\nbuffer. Each computation unit is allocated specific input and\noutput spaces, facilitating parallel computation. Once a unit’s\ndesignated input data is exhausted or its output space is filled,\nit must wait for the dependent units to process these data.\nIV. M ETHODOLOGY\nGiven that the frequency of digital modules is typically\nmuch higher than that of crossbar arrays, we consider the\ncomputation time of the crossbar arrays TMV M and the\ntransfer of intermediate dat",
  " arrays TMV M and the\ntransfer of intermediate data TDTacross multiple levels as\nthe primary components of latency T:\nT=TDT+TMV M =X\niData Li\nBand Li+X\njFoutj\nCoutj(1)\nData LiandBand Lirepresent the data volume and band-\nwidth at each level, respectively. FoutjandCoutjrepresent\nthe volume of output feature map and the output channel of\nthe corresponding MVM layer, respectively. To reduce latency\nin the proposed architecture, two primary steps are involved:\n•Optimize data transfer latency by allo",
  " involved:\n•Optimize data transfer latency by allocating computa-\ntional resources and intermediate data storage resources\nthrough resource scheduling.\n•Optimize crossbar latency by using a weighted expansion\nstrategy for weight mapping, which enhances resource\nallocation on individual crossbar arrays.A. Resource Scheduling\nAlgorithm 1 outlines the overall process of scheduling.\nInitially, a resource management pool based on a multi-\nlevel architecture is instantiated according to the configurat",
  "ecture is instantiated according to the configuration\nparameters. Next, the computation graph of the CNN to be\ndeployed is loaded and partitioned into multiple subgraphs.\nComputational and storage resources are then allocated to\nthe operators in each layer in the subgraphs, generating the\ncomplete data flow. Finally, iterative optimization is performed\non the data flow to minimize latency.\nAlgorithm 1 Resource Scheduling Algorithm\nRequire: ArchParams, CompGraph, EvalFunc\nEnsure: Optimal resource",
  "rams, CompGraph, EvalFunc\nEnsure: Optimal resource allocation and data flow\n1:Update resource pool by ArchParams // Resource Statis-\ntics\n2:foreach node in CompGraph do\n3: ifrandom probability <Pthen\n4: Create subgraph from current node\n5: foreach sub node in subgraph do\n6: Randomly allocate func to sub node\n7: end for //Resource Allocation\n8: else Update(P)\n9: end if\n10:end for //Network Partition\n11:foreach operator pair in CompGraph do\n12: buffer set: (diff subgraph) ? L0 : (diff core) ? L1 :",
  "fer set: (diff subgraph) ? L0 : (diff core) ? L1 : L2\n13: buffer allocate\n14:end for //Data Flow Generation\n15:foreach generation do\n16: latency =EvalFunc (data flow )\n17: Select best solutions based on latency\n18: Apply crossover to create new population\n19:end for //Evaluation Iteration\n20:Return best solution found\na) Network Partition: To achieve a larger solution search\nspace, we design the computation graph partitioning and\nresource allocation process to be highly stochastic. During the\ngr",
  "ion process to be highly stochastic. During the\ngraph partitioning process, we determine whether to cut at the\ncurrent node and generate a subgraph with a probability P,\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  "which is expressed as:\nP=1\n1 +ek(R−R0),0≤R≤1 (2)\nWhere Rrepresents the remaining resource rate. kis the\nscaling factor and R0is the threshold parameter.\nb) Resource Allocation and Dataflow Generation: For\ncomputational resources, nodes’ attributes are matched with\nthe corresponding functional computation units in the resource\npool without restricting the allocation to specific cores. For\nstorage resources, only sizes are allocated, not specific ad-\ndresses. The inner-buffer (L2) is allocated in ",
  "d-\ndresses. The inner-buffer (L2) is allocated in banks, while\nthe inter-buffer (L1) is assigned variable lengths, requiring a\nbalance between resource usage and bandwidth bottlenecks.\nThe global buffer (L0) acts as an intermediary for subgraphs,\nwith its space partitioned based on the size of the final output\nof each subgraph. Following this, memory spaces at each level\nare matched according to the data dependencies between nodes\nin the subgraph, using a “proximity to computation units first”\np",
  ", using a “proximity to computation units first”\nprinciple, ultimately generating a complete data flow.\nc) Evaluation Iteration: An evolutionary algorithm is\nemployed to search for the optimal solution within the solution\nspace. The objective function is defined to minimize latency\nof data move:\nTDT=X\nCsgTDSG + max\nk(TDCT k) (3)\nTDCT = 2∗(X\nlayerTl′\nex+X\nlayerTl\nin) (4)\nTDSG =Foutln\nBand L0, Tl′\nex=Datal′\nBand L1, Tl\nin=Datal\nBand L2(5)\nThe total data transfer latency TDTis composed of the\nsubgr",
  " data transfer latency TDTis composed of the\nsubgraph data latency TDSG and the maximum core task data\nlatency TDCT .Csgrepresents the number of subgraphs, and\nFoutlndenotes the output feature map size of the last node\nin each subgraph. The TDCT is further divided into intra-core\nlatency Tinand inter-core latency Tex, which are computed\nas the corresponding data volumes divided by the bandwidth\nof the respective level buffer.\nIn the mutation phase, adjustments are made by altering the\nownership ",
  "e, adjustments are made by altering the\nownership of the endpoint nodes of subgraphs and alternating\nthe computational units of the execution nodes. This method\nallows the system to explore various configurations, ultimately\nidentifying the optimal solution that minimizes latency. Typi-\ncally, this results in a data flow with very few cross points, as\nindicated by the red lines in Fig. 4.\nB. Weight Mapping\nDue to the varying scales of crossbar arrays and the weight\nparameters of different networ",
  "rays and the weight\nparameters of different network layers, the utilization of cross-\nbars significantly impacts overall computational efficiency.\nIt is crucial to develop an optimal method to automatically\nallocate the entire network’s parameters to the arrays, thereby\nfacilitating the complete model deployment in CIM systems.\nAdditionally, since each storage cell within the crossbar also\nfunctions as a computation unit, weight replication is a vital\nConvolution\n&\nFully \nConnected\nBank2Bank1Ban",
  "vital\nConvolution\n&\nFully \nConnected\nBank2Bank1Bank0\nBank3\n…………Config\n4Config\n3Config\n2Config\n1ControllerInner -BufferCIM\nCoreMatrix Computation Unit\nCoreBatch \nNormlization\nMax \nPooling\nUp \nSamplingScalar\nVector\nTransforInner Core\nData Flow Subgraph\nCore -task\nCore -pipeThe CIM core microarchitecture\noverlap reduce and blocking eliminateThe space hierarchy Fig. 4. Ideal smooth data flow lines within and between cores.\napproach to enhancing computational parallelism and crossbar\narray utilizatio",
  "tational parallelism and crossbar\narray utilization.\na) Integer Linear Programming: As shown in Fig. 6(a-\nc), mapping an 8-layer network onto a limited space. Different\ncolored blocks represent the weight parameters of various\nMVM layers. In traditional sequential mapping methods,\nweight blocks are mapped to the crossbar array in the order\nthey appear in the computation graph. When a block cannot\nfit into the available space, a new space is required, resulting\nin significant unused space. To add",
  "red, resulting\nin significant unused space. To address this issue, an Integer\nLinear Programming (ILP) space mapping strategy [13] has\nbeen proposed as an effective method to optimize crossbar\nutilization. This approach treats the boundary constraints of\nweight arrangement as an integer constraint problem, with\nthe position of weight blocks as the ILP’s objective. Through\nlinear programming, a candidate set of all feasible solutions\nis constructed, and the optimal solution is selected.\nb) Weight",
  "d, and the optimal solution is selected.\nb) Weight Replication: Replication strategies vary across\ndifferent hierarchical levels. At the macro level, weight repli-\ncation blocks are placed across different cores, directly en-\nhancing parallelism by leveraging asynchronous inter-core op-\nerations. However, on a single crossbar array, to compute two\ninputs within the same cycle, it is necessary to concatenate the\ninputs. Due to the computational characteristics of crossbars,\nthe fundamental approa",
  "aracteristics of crossbars,\nthe fundamental approach involves diagonally separating the\nreplicated weight blocks from the original ones to prevent\ncomputational interference. This approach necessitates signifi-\ncant zero-padding, as illustrated in Fig. 5(b). Given the sliding\nwindow nature of convolution operations, there is data reuse\nbetween adjacent input frames. Consequently, by merging\nBias MatrixWeight Matrixx0\nx1\nb0b1\ny0 y1\ny0 y100\n00W00W01\nW10W11\n00\n00W00W01\nW10W11\nBiasWeightx1\nx1\nx2\npos",
  "1\nW10W11\n00\n00W00W01\nW10W11\nBiasWeightx1\nx1\nx2\npost -processb0b1\ny0y1b0b1\ny2y300 W00W01\nW10W11\n00W00W01\nW10W11\nBiasWeightx0\nx1\nx2\npost -processb0b1\ny0y1b0b1\ny2y3n extension\nW00W01\nW10W11\npost -process\n(a) (b)  (c) x0\nFig. 5. (a) The original computation in crossbar array. (b) Weight replication\nfor parallel computing and (c) OMM-like method to reduce zero padding.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restriction",
  "026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  "L0\nL1L2L3\nL4\nL5L6\nL8\nL0L1\nL3L4L7L5\nL6L2\nL8L7\n(a) (b)  (c) \nL0\nL1L2L3 L4\nL7L5\nL6L0 L1\nL2\nL3 L4L7\nL5L6\nL0\nL1L2\nL3L7\nL5\nL6L4\n(d) (e)  (f) Fig. 6. Traditional sequential mapping methods (a)-(b), and ILP methods\nto improve the use ratio of crossbar. (d)-(e) ILP + OMM-like method for\nmapping and weighted skew on the bottleneck layer (L0 and L4).\ninput data, the zero-padding introduced by weight replication\ncan be minimized. This method is similar in concept to the\noverlapped mapping method (OMM) [14].",
  "ncept to the\noverlapped mapping method (OMM) [14].\nClearly, within a single large core, the OMM-like can help\nthe system achieve an equivalent trade-off between crossbar\narea and computational efficiency. When increasing the output\nby M times, the crossbar utilization η(example for horizontal\ndirection) can be determined using (6) and (7), where Sarray\ndenotes the area of the crossbar array.\nT′\nMV M =Fout\nM∗Cout(6)\nη=M∗Cout∗[Kw∗Kh+ (M−1)Kw]\nSarray(7)\nTherefore, we have refined the original ILP b",
  "y(7)\nTherefore, we have refined the original ILP by incorporating\nthe horizontal and vertical expansion of each layer’s weight\nparameters as new variables in the optimization process.\nAdditionally, maximizing utilization has been introduced as\none of the primary objectives. This enhancement has led to\nan increase in large core utilization to over 90%, and the\nsubstantial reduction in latency is directly proportional to the\ndegree of weight expansion.\nc) Weighted extension: In convolutional compu",
  "ion.\nc) Weighted extension: In convolutional computations,\nthe execution efficiency of different layers varies, often related\nto their sliding window operations. To mitigate the bottlenecks\ncaused by excessive sliding window operations, we introduced\na weighted scoring mechanism on top of the ILP + OMM-like\napproach. The reward function is designed as follows:\nNw/h=\u0016IFw/h−Kw/h+ 2Pw/h\nSw/h\u0017\n+ 1 (8)\nN=Nw∗Nh (9)\nFRewards =N\nMx1\nα, α > 1 (10)Where: N is the number of sliding window operations for\nth",
  " is the number of sliding window operations for\nthe current layer. IFw/h represents the width and height of\nthe input feature map. K, P, and S denote the kernel, padding,\nand stride, respectively. M is a normalization adjustment\nparameter, and FRewards is a fitted exponential function, where\nx is the extension factor based on the original weights. In\nthe process of weighted extension solving in Fig. 6(d-f), array\nresources are prioritized for allocation to the bottleneck layers,\nresulting in fur",
  "ocation to the bottleneck layers,\nresulting in further reduction of latency.\nV. EVALUATION\nA. Experiment Setup\na) Architecture Configuration: The final evaluation ex-\nperiments were conducted directly on our SoC, which employs\nan eFlash floating-gate crossbar scheme. This SoC features a\nthree-level buffer and consists of four large cores intercon-\nnected via a NoC. Each core contains eight MCUs and five\nOCUs, with MCU and OCUs executing in parallel, though\nonly one MCU can execute at a time. The",
  "el, though\nonly one MCU can execute at a time. The MCUs are organized\nin pairs to form a pipelined ping-pong structure, achieving one\nMVM every 120ns. The eFlash cell’s precision is 8-bit, with\ninputs, outputs, and weights of crossbar all represented as 8-bit\nfixed-point numbers. Detailed configurations are in Table I.\nTABLE I\nMETRICS OF OUR MULTI -CORE E FLASH -BASED CIM S OC\nComponent Params Specific Params Specific\nChip core array 4 - -\nGlobal buffer size 4MB bandwidth 8GB/s\nCore MCU 2X4 OCU ",
  " buffer size 4MB bandwidth 8GB/s\nCore MCU 2X4 OCU 5\nDAC 1152 ADC 320\nInter buffer size 512KB bandwidth 32GB/s\nInner buffer bank num 8 bandwidth 64GB/s\nbank size 32KB\nCrossbar weight size 1152*960 bias size 32*960\nlatency MCU 240ns OCU 4ns\nb) benchmarks and baselines: We used commonly em-\nployed networks in computer visual scenarios (Yolov5, Gsr,\nGvfe, Resnet50 and Unet) as benchmarks. Linear schedul-\ning (sequential allocation in order) and OMM-like mapping\nstrategies were selected as the baseli",
  "ike mapping\nstrategies were selected as the baselines for our scheduling\nand mapping comparisons. We obtained latency measurements\nthrough direct testing on the SoC. Additionally, we devel-\noped a performance analysis simulator capable of event-level\nsimulation in multi-level architectures, allowing us to analyze\nother performance metrics. These include different levels of\nbuffer usage, computational unit utilization, and the time\nconsumption of specific components.\nB. Experiment Result\na) Resou",
  "specific components.\nB. Experiment Result\na) Resource Utilization: Fig. 8(left) illustrates the re-\nduction in the number of chip pipelines for network tasks\nwith varying complexities. This reduction indicates that more\noperators are processed within a single chip pipeline, leading\nto a maximum increase in computational resource utilization\nby up to 1.4 times. Additionally, storage resource utilization\nimproved by up to 30%. Fig. 8(right) shows the variation\nAuthorized licensed use limited to: Z",
  "he variation\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  "/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni000",
  "0000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c\n/uni00000013/uni000000",
  "013/uni00000011/uni0000001c\n/uni00000013/uni00000011/uni00000017/uni00000015\n/uni00000013/uni00000011/uni00000016/uni00000014/uni00000013/uni00000011/uni00000016/uni0000001b\n/uni00000013/uni00000011/uni00000015/uni0000001a/uni00000013/uni00000011/uni00000016/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000017/uni0000005c/uni00000052/uni0000004f/uni00000052/uni00000059/uni00000018/uni00000056\n/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000056",
  "48/uni00000044/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/u",
  "uni0000001c/uni00000015\n/uni00000013/uni00000011/uni00000018/uni00000019\n/uni00000013/uni00000011/uni00000017/uni0000001c/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000017/uni00000014\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000016/uni00000014/uni0000004a/uni00000056/uni00000055\n/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/un",
  "uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000018\n/uni00000013/uni00000011/uni00000016/uni00000018\n/uni00000013/uni00000011/uni00000015/uni0000001b/uni",
  "ni00000013/uni00000011/uni00000015/uni0000001b/uni00000013/uni00000011/uni00000016/uni00000015\n/uni00000013/uni00000011/uni00000015/uni00000019/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000015/uni00000018/uni0000004a/uni00000059/uni00000049/uni00000048\n/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055 /uni00000052/uni00000058/uni00",
  "0000048/uni00000055 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000014\n/uni00000013/uni00000011/uni00000018/uni0000001c\n/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000018/uni00000016\n/uni00000013/uni000",
  "000011/uni00000018/uni00000016\n/uni00000013/uni00000011/uni00000017/uni00000016\n/uni00000013/uni00000011/uni00000016/uni0000001a\n/uni00000013/uni00000011/uni00000015/uni0000001c/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000018/uni00000013\n/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055 /uni00000052/uni00000058/uni00000",
  "0048/uni00000055 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000056/uni00000046/uni0000004b/uni00000048/uni00000047/uni00000058/uni0000004f/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/uni00000018/uni00000015\n/uni00000013/uni00000011/uni00000017/uni00000018/uni00000013/uni00000011/uni00000017/uni0000001c\n/uni000000",
  "013/uni00000011/uni00000017/uni0000001c\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000016/uni00000019\n/uni00000013/uni00000011/uni00000015/uni0000001b/uni00000058/uni00000051/uni00000048/uni00000057\n/uni00000056/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000044/uni0000004f /uni0000002c/uni0000002f/uni00000033 /uni00000032/uni00000030/uni00000030/uni00000010/uni0000004f/uni0000004c/uni0000004e/uni00000048 /uni0000002c/uni000000",
  "04c/uni0000004e/uni00000048 /uni0000002c/uni0000002f/uni00000033/uni0000000e/uni00000032/uni00000030/uni00000030/uni00000010/uni0000004f/uni0000004c/uni0000004e/uni00000048 /uni00000052/uni00000058/uni00000055/uni00000003/uni00000050/uni00000044/uni00000053/uni00000053/uni0000004c/uni00000051/uni0000004aFig. 7. The latency improvement with different strategy on benchmarks.\nin data transfer latency between different levels of buffers.\nAlthough the latency of L1 increased, the higher bandwidth\nof ",
  " latency of L1 increased, the higher bandwidth\nof L1 compared to L0 resulted in an overall 8% reduction in\ntotal latency.\n/uni0000005c/uni00000052/uni0000004f/uni00000052/uni00000059/uni00000018/uni00000056 /uni0000004a/uni00000056/uni00000055 /uni0000004a/uni00000059/uni00000049/uni00000048 /uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000018/uni00000013 /uni00000058/uni00000051/uni00000048/uni00000057/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/u",
  "/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000017/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000014/uni00000011/uni00000017/uni0000005b\n/uni00000014/uni000",
  "000011/uni00000017/uni0000005b\n/uni00000014/uni00000011/uni00000014/uni0000005b/uni00000014/uni00000011/uni00000016/uni0000005b\n/uni00000014/uni00000011/uni00000015/uni0000001b/uni0000005b\n/uni00000014/uni00000011/uni00000015/uni0000005b/uni00000014/uni00000011/uni00000016/uni0000005b\n/uni00000014/uni00000011/uni00000013/uni0000001c/uni0000005b\n/uni00000014/uni00000011/uni00000013/uni00000019/uni0000005b/uni00000014/uni00000011/uni00000014/uni0000001a/uni0000005b/uni00000014/uni00000011/uni00000",
  "0001a/uni0000005b/uni00000014/uni00000011/uni00000015/uni00000015/uni0000005b/uni00000035/uni00000048/uni00000056/uni00000052/uni00000058/uni00000055/uni00000046/uni00000048/uni00000003/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni0000002a/uni00000048/uni00000051/u",
  "/uni00000042/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000057/uni0000004c/uni00000046\n/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000042/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000057/uni0000004c/uni00000046\n/uni00000030/uni00",
  "0000057/uni0000004c/uni00000046\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000042/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000005c/uni00000052/uni0000004f/uni00000052/uni00000059/uni00000018/uni00000056 /uni0000004a/uni00000056/uni00000055 /uni0000004a/uni00000059/uni00000049/uni00000048 /uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000018/uni00000013 /uni00000058/uni00000051/uni00000048/uni0000",
  "00013 /uni00000058/uni00000051/uni00000048/uni00000057/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000018/uni00000015\n/uni00000013/uni00000011/uni00000017/uni00000019/uni00000013/uni00000011/uni00000019/uni00000014\n/uni00000013/uni00000011/uni00000017/uni0000001",
  "14\n/uni00000013/uni00000011/uni00000017/uni0000001a/uni00000013/uni00000011/uni00000018/uni0000001b/uni00000014/uni00000011/uni00000015/uni0000005b\n/uni00000014/uni00000011/uni00000014/uni0000005b/uni00000014/uni00000011/uni00000015/uni0000005b /uni00000014/uni00000011/uni00000015/uni0000005b/uni00000014/uni00000011/uni00000016/uni0000005b/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000030/uni00000052/uni00000059/uni00000048/uni00000003/uni0000002f/uni00000044/uni00000057/un",
  "uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni0000002f/uni00000013/uni00000042/uni00000025/uni00000058/uni00000049/uni00000049/uni00000048/uni00000055/uni00000042/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055\n/uni0000002f/uni00000013/uni00000042/uni00000025/uni00000058/uni00000049/uni00000049/uni00000048/uni00000055/uni00000042/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000057/uni0000004c/uni00000046/uni000",
  "0000048/uni00000057/uni0000004c/uni00000046/uni0000002f/uni00000014/uni00000042/uni00000025/uni00000058/uni00000049/uni00000049/uni00000048/uni00000055/uni00000042/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000057/uni0000004c/uni00000046\n/uni0000002f/uni00000014/uni00000042/uni00000025/uni00000058/uni00000049/uni00000049/uni00000048/uni00000055/uni00000042/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055\nFig. 8. The comparison of resource utilization for linear-m",
  "he comparison of resource utilization for linear-method and our\nscheduler, and the latency change in different level of buffer.\nb) Crossbar Utilization: Fig. 9 displays the improve-\nments in area utilization and computational efficiency of\ncrossbar arrays by different mapping strategies. In the case\nof large cores, ILP does not optimize performance without\nweight replication. However, with weight replication, there is\na noticeable increase in area utilization, peaking at 94.7%.\nAlthough the over",
  "a utilization, peaking at 94.7%.\nAlthough the overall area utilization decreases slightly after\napplying weighted skewing resources to address bottleneck\nlayers, the latency is further improved. The final scheme\nachieves an average latency improvement of 28.2% compared\nto OMM-like strategies.\n/uni0000005c/uni00000052/uni0000004f/uni00000052/uni00000059/uni00000018/uni00000056 /uni0000004a/uni00000056/uni00000055 /uni0000004a/uni00000059/uni00000049/uni00000048 /uni00000055/uni00000048/uni0000005",
  "49/uni00000048 /uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000018/uni00000013 /uni00000058/uni00000051/uni00000048/uni00000057/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni",
  "ni00000013/uni00000013/uni00000011/uni0000001c/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001c/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000055/uni00000052/uni00000059/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni000000",
  "0050/uni00000048/uni00000051/uni00000057/uni00000013/uni00000011/uni0000001b/uni0000001b\n/uni00000013/uni00000011/uni0000001b/uni00000015/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001c\n/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000011/uni0000001c/uni00000016/uni00000013/uni00000011/uni0000001c/uni00000017/uni0000001a\n/uni00000013",
  "1/uni0000001c/uni00000017/uni0000001a\n/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/uni0000001b/uni0000001c\n/uni00000013/uni00000011/uni0000001b/uni00000016/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000016\n/uni00000013/uni00000011/uni0000001c/uni00000024/uni00000055/uni00000048/uni00000044/uni00000003/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/un",
  "uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000005c/uni00000052/uni0000004f/uni00000052/uni00000059/uni00000018/uni00000056 /uni0000004a/uni00000056/uni00000055 /uni0000004a/uni00000059/uni00000049/uni00000048 /uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000018/uni00000013 /uni00000058/uni00000051/uni00000048/uni00000057/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni",
  "ni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni0000001c/uni0000005b\n/uni00000013/uni00000011/uni00000019/uni00000018/uni0000005b\n/uni00000013/uni00000011/uni00000018/uni0000001b/uni0000005b/uni00000013/uni00000011/uni0000001c/uni00000018/uni0000005b\n/uni00000013/uni00000011/uni00000019/uni00000015/uni0000005b/uni00000030/uni00000039/uni00000030/uni00000003/uni00",
  "00000030/uni00000039/uni00000030/uni00000003/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni00000032/uni00000030/uni00000030/uni00000010/uni0000004f/uni0000004c/uni0000004e/uni00000048 /uni0000002c/uni0000002f/uni00000033/uni0000000e/uni00000032/uni00000030/uni00000030/uni00000010/uni0000004f/uni0000004c/uni0000004e/uni00000048 /uni00000",
  "004f/uni0000004c/uni0000004e/uni00000048 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000048/uni00000047/uni00000003/uni00000028/uni0000005b/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051\nFig. 9. Crossbar use ratio and computing latency of different mapping strategy.\nc) Latency: Fig. 7 shows the latency performance of\nbenchmark deployments on our SoC, corresponding to the\ncombination of all optimization strategies. The data in",
  "nation of all optimization strategies. The data indicates\nthat our resource scheduling and weight mapping strategiesindependently contribute to a maximum latency reduction of\n10% and 70%, respectively. For the combined deployment on\nthe Yolov5s network, the overall latency was reduced by 76%.\nVI. C ONCLUSION\nThis work pioneers the automatic optimization and deploy-\nment of multi-core, large-array CIM SoCs. Previous research\nhas often overlooked domain-specific compilers in multi-\ncore, large-arr",
  "omain-specific compilers in multi-\ncore, large-array systems, with some efforts still confined to\nsimulation stages. We addressed the growing demand for low-\nlatency edge scenarios by developing optimization strategies\nfor resource scheduling and weight mapping. Experimental\nresults demonstrate a 30% improvement in resource utilization,\na 94.7% crossbar array utilization, and a 76% reduction in\nlatency based on actual chip measurements.\nREFERENCES\n[1] K. H. Lee and et al., “A Low-Power Processor",
  "S\n[1] K. H. Lee and et al., “A Low-Power Processor With Configurable\nEmbedded Machine-Learning Accelerators for High-Order and Adaptive\nAnalysis of Medical-Sensor Signals,” JSSC, vol. 48, no. 7, pp. 1625-\n1637, 2013.\n[2] T. Chen and et al., “DianNao: a small-footprint high-throughput acceler-\nator for ubiquitous machine-learning,” in ASPLOS, pp. 269–284, 2014.\n[3] K. Lee and et al., “A Charge-Sharing based 8T SRAM In-Memory\nComputing for Edge DNN Acceleration,” DAC, pp. 739-744, 2021.\n[4] C. Wol",
  " Acceleration,” DAC, pp. 739-744, 2021.\n[4] C. Wolters and et al., “Memory Is All You Need: An Overview of\nCompute-in-Memory Architectures for Accelerating Large Language\nModel Inference,” arXiv: 2406.08413 [cs.AR].\n[5] R. Pelke and et al., “Mapping of CNNs on multi-core RRAM-based\nCIM architectures,” VLSI-SoC, pp. 1-6, 2023.\n[6] K. E. Jeon and et al., “Weight-Aware Activation Mapping for Energy-\nEfficient Convolution on PIM Arrays,” ISLPED, pp. 1-6, 2023.\n[7] J. Bai and et al., “CIMQ: A Hardwar",
  "1-6, 2023.\n[7] J. Bai and et al., “CIMQ: A Hardware-Efficient Quantization Frame-\nwork for Computing-In-Memory-Based Neural Network Accelerators,”\nTCAD, vol. 43, no. 1, pp. 189-202, 2023.\n[8] X. Guo and et al., “Fast, energy-efficient, robust, and reproducible\nmixed-signal neuromorphic classifier based on embedded NOR flash\nmemory technology,” IEDM, pp. 6.5.1-6.5.4, 2017.\n[9] Y . Feng and et al., “A Novel Array Programming Scheme for Large\nMatrix Processing in Flash-Based Computing-in-Memory (CI",
  " Processing in Flash-Based Computing-in-Memory (CIM) With\nUltrahigh Bit Density,” TED, vol. 70, no. 2, pp. 461-467, 2023.\n[10] C. Yang and et al., “CIMAX-Compiler: An End-to-End ANN Compiler\nfor Heterogeneous Computing-in-Memory Platform,” WCCCT, pp. 152-\n157, 2023.\n[11] X. Sun and et al., “PIMCOMP: A Universal Compilation Framework\nfor Crossbar-based PIM DNN Accelerators,” DAC, pp. 1-6, 2023.\n[12] L. Han and et al., “A Convolution Neural Network Accelerator Design\nwith Weight Mapping and Pipeli",
  " Accelerator Design\nwith Weight Mapping and Pipeline Optimization,” DAC, pp. 1-6, 2023.\n[13] T. Bai and et al., “An End-to-End In-Memory Computing System Based\non a 40-nm eFlash-Based IMC SoC: Circuits, Toolchains, and Systems\nCo-Design Framework,” TCAD, vol. 43, no. 6, pp. 1729-1740, 2024.\n[14] Z. Zhu and et al., “Mixed Size Crossbar based RRAM CNN Accelerator\nwith Overlapped Mapping Method,” ICCAD. pp, 1–8, 2018.\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05",
  " to: Zhejiang University. Downloaded on January 05,2026 at 07:17:08 UTC from IEEE Xplore.  Restrictions apply.",
  "iTaskSense: Task-Oriented Object Detection in\nResource-Constrained Environments\nSungHeon Jeong, Hamza Errahmouni Barkam, Hyunwoo Oh, Hanning Chen, Tamoghno Das, Zhen Ye, Mohsen Imani\nUniversity of California, Irvine, USA\n{sungheoj, herrahmo, hyunwooo, hanningc, tamoghnd, zye14, m.imani}@uci.edu\nAbstract —Task-oriented object detection is increasingly essential for\nintelligent sensing applications, enabling AI systems to operate au-\ntonomously in complex, real-world environments such as autonomou",
  "complex, real-world environments such as autonomous\ndriving, healthcare, and industrial automation. Conventional models often\nstruggle with generalization, requiring vast datasets to accurately detect\nobjects within diverse contexts. In this work, we introduce iTask, a task-\noriented object detection framework that leverages large language models\n(LLMs) to generalize efficiently from limited samples by generating an ab-\nstract knowledge graph. This graph encapsulates essential task attributes,\na",
  "is graph encapsulates essential task attributes,\nallowing iTask to identify objects based on high-level characteristics rather\nthan extensive data, making it possible to adapt to complex mission\nrequirements with minimal samples.\niTask addresses the challenges of high computational cost and re-\nsource limitations in vision-language models by offering two configuration\nmodels: a distilled, task-specific vision transformer optimized for high\naccuracy in defined tasks, and a quantized version of th",
  "cy in defined tasks, and a quantized version of the model for\nbroader applicability across multiple tasks. Additionally, we designed a\nhardware acceleration circuit to support real-time processing, essential\nfor edge devices that require low latency and efficient task execution.\nOur evaluations show that the task-specific configuration achieves a 15%\nhigher accuracy over the quantized configuration in specific scenarios,\nwhile the quantized model provides robust multi-task performance. The\nhardw",
  " provides robust multi-task performance. The\nhardware-accelerated iTask system achieves a 3.5x speedup and a 40%\nreduction in energy consumption compared to GPU-based implementa-\ntions. These results demonstrate that iTask ’s dual-configuration approach\nand situational adaptability offer a scalable solution for task-specific\nobject detection, providing robust and efficient performance in resource-\nconstrained environments.\nI. I NTRODUCTION\nTask-oriented object detection [1], [2] is crucial for a",
  "riented object detection [1], [2] is crucial for a wide range of\napplications, from autonomous driving and surveillance to healthcare\nand industrial automation [2], [3]. Unlike general object detection,\ntask-oriented detection focuses on identifying and localizing objects\nwithin a specific contextual frame, allowing AI systems to make\ninformed decisions and initiate context-driven actions [4]. Meeting\nthe demands of task-oriented detection often requires models to adapt\nto complex mission requir",
  "requires models to adapt\nto complex mission requirements, which are frequently defined by\nabstract characteristics rather than explicit labels. This dependency\non nuanced contextual understanding challenges traditional models,\nwhich typically rely on large, annotated datasets to achieve high\naccuracy [5]. Furthermore, implementing these models in real-world\nscenarios introduces additional constraints, such as computational and\nmemory efficiency, particularly in resource-constrained environments\n",
  "particularly in resource-constrained environments\nlike edge devices.\nAnother challenge is that traditional vision-language models, such\nas MDETR [6], used to solve tasks [7] are very resource-intensive,\nconsuming high amounts of memory and power. This is particularly\nevident in intelligent sensing applications, where edge devices must\noperate efficiently with limited computational resources and potentially\nminimal data samples [8]. To address these demands, we present\nintelligent Task Oriented A",
  "se demands, we present\nintelligent Task Oriented Aritficial Intelligence (iTask), a task-oriented\nobject detection framework designed to excel in complex missions and\nadapt effectively to a wide range of real-world tasks.\nConsider a complex mission scenario where a system must identify\nany object that could be used to open a bottle of beer. In a traditional\nTraditional Object detection\nTask -Oriented Object detection\nTask -Oriented Object Detection (TOOD)\nFind objects that help \nme extract a lem",
  "on (TOOD)\nFind objects that help \nme extract a lemon slice \nfrom my teacup.Traditional object detection lists  all \nobjects , and a manual selection of \ncorrect ones  is needed.\nUnscalable  due to lack of \ngeneralization  for any prompt\nOur iTaskSense  maps prompt text to detected objects via an affinity \nmatrix, enabling focused, automatic responses to any prompt .\nFig. 1: iTask framework for task-oriented object detection: Unlike\ntraditional detection, iTask uses an affinity matrix for prompt-",
  "tection, iTask uses an affinity matrix for prompt-based,\ncontext-aware responses, enhancing precision and scalability across\ndomains while tackling high computational demands and complex\nobject semantics.\nmachine learning context, a model would need vast amounts of labeled\ndata to capture all potential objects capable of performing this action,\nsuch as bottle openers, lighters, or even certain types of spoons\n[2]. However, real-world scenarios often require generalizing from\nvery few samples, wh",
  "ten require generalizing from\nvery few samples, which is nearly impossible for traditional methods\nwithout comprehensive datasets. iTask addresses this challenge by\nusing a large language model (LLM) to generate an abstract knowledge\ngraph that encapsulates the essential attributes required to perform\nthe task, such as “firm leverage” or “rigid edge”. This high-level\nunderstanding allows iTask to define these characteristics and identify\nobjects that meet them, enabling generalization even with ",
  "that meet them, enabling generalization even with minimal\ndata samples.\nFor example, in a scenario where the goal is to extract a lemon\nslice from a glass, iTask can analyze objects that share attributes\nrelevant to the task, as shown in Figure 1. While a knife, fork, or\nspoon might be typical choices, an unexpected tool like a pen could\nalso serve the purpose, yet it may not appear in conventional datasets\nunless trained with vast samples. By leveraging the LLM-generated\nknowledge graph, iTask ",
  "veraging the LLM-generated\nknowledge graph, iTask can identify and rank objects based on their\nfeature alignment with task requirements, selecting the most suitable\noption even in cases where there is no perfect match or multiple\ncandidates.\nThis capability is further enhanced through iTask’s situational\nawareness, which enables real-time mission adaptability. When de-\nployed on edge devices, iTask can interact dynamically with its\nenvironment to analyze content based on specific mission objecti",
  " analyze content based on specific mission objectives.\nFor instance, if an edge device is instructed to “identify objects\ncapable of providing leverage,” iTask can assess available objects\nand select those that match the task-defined attributes . This real-time\nadaptability not only reduces dependency on extensive datasets but\nalso significantly improves resource efficiency by focusing on task-\nrelevant data, optimizing computational load, and conserving energy.2025 62nd ACM/IEEE Design Automati",
  "nserving energy.2025 62nd ACM/IEEE Design Automation Conference (DAC) | 979-8-3315-0304-8/25/$31.00 ©2025 IEEE | DOI: 10.1109/DAC63849.2025.11133060\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "Reasoning -Guided Task -Specific AI \nItem to \ntake lemon \nout of tea \nglass\nVision Transformer\nSegmentation\nText  Transformer \nAnalogical \nReasoning\nTop 1\nTop 2✓Tongs or Tweezers\n✓Fork or Skewer\n✓Spoon\n✓Tea Strainer✓F1= Precision grips, long handle, … \n✓F2= Fine mesh, , liquid drain, broad surface, ..\n✓F3= Sharp point, long handle, spearing, …\n✓F4= Fine mesh, sturdy frame, …\nF1 F2F3F4\n0.9\n0.2 0.10.6 0.7 0.4\n0.1 0.3\n0.7 0.9 0.6 0.5\n0.1 0.1 0.1 0.1\nAffinity Matrix\nTaskCLIP  Inference \nCost Breakdo",
  "\nAffinity Matrix\nTaskCLIP  Inference \nCost Breakdown\niTaskSense  Inference \nCost Breakdown\nQuality Loss ≅ 10%Segmentation\n85%\nQuality Loss = 0%\nViT\n48% Segmentation\n45%ViT\n13%Reasoning\n2%\nReasoning\n7%72.42 W\n19.73 W\nFig. 2: The iTask framework for reasoning-guided task-specific object selection and efficient hardware acceleration: Given the prompt,\nsegmented objects are encoded and matched to functional features (F1–F4) via an affinity matrix. Analogical reasoning ranks items (e.g.,\nforks, tweez",
  "alogical reasoning ranks items (e.g.,\nforks, tweezers) based on task-relevant attributes. Hardware optimization for taskCLIP inference: Smaller CLIP models reduce cost but\nmay lose quality; knowledge distillation improves their accuracy. A unified ASIC design accelerates segmentation (convolution-based) and ViT\n(attention-based) tasks on the same hardware, achieving significant computational efficiency.\nTo overcome the limitations of traditional vision-language mod-\nels in intelligent sensing, i",
  "vision-language mod-\nels in intelligent sensing, iTask is built with two configurations\ntailored for different mission requirements. The first configuration\nuses knowledge distillation to create a smaller, task-specific vision\ntransformer that maximizes detection performance for specific tasks.\nWhile smaller models reduce computational costs, they often lose\nquality compared to larger models. iTask addresses this challenge\nby employing knowledge distillation techniques to make smaller\nCLIP model",
  "distillation techniques to make smaller\nCLIP models as accurate as their larger counterparts, ensuring high-\nquality performance without the associated cost overhead. The second\nconfiguration employs quantization to compress the model, enabling\nversatility across multiple tasks with reduced computational demands\n[9], [10]. These configurations allow iTask to provide efficient task-\noriented detection in a wide variety of real-world scenarios while\nmaintaining adaptability and responsiveness.\nAdd",
  "e\nmaintaining adaptability and responsiveness.\nAdditionally, a hardware circuit was designed to accelerate iTask ,\nenabling rapid processing near the sensor and meeting the real-\ntime demands of intelligent sensing environments. Since segmentation\nand vision transformers (ViT) represent fundamentally different net-\nwork architectures—segmentation being convolution-based and ViT\nattention-based—specific hardware accelerators are required for each.\nTo address this challenge, we propose a unified A",
  "\nTo address this challenge, we propose a unified ASIC architecture\nthat accelerates both ViT and segmentation tasks on the same hard-\nware platform, significantly improving computational efficiency for\ntaskCLIP inference. This design is essential for edge-based AI appli-\ncations, where high-speed processing and minimal latency are critical.\nThrough this combined approach of model optimization, situational\nadaptability, and hardware acceleration, iTask delivers a scalable,\nefficient solution tail",
  "iTask delivers a scalable,\nefficient solution tailored for resource-constrained environments where\nrobust task-specific detection is essential.\nThe contributions of this paper are threefold: (1) the introduction of\niTask , a framework that integrates LLM-generated knowledge graphs\nfor task-specific generalization with minimal samples, achieving a\n15% improvement in accuracy over traditional quantized configura-\ntions in specific scenarios; (2) dual configuration models, including\nknowledge disti",
  "al configuration models, including\nknowledge distillation and quantization, for efficient task-specific and\nmulti-task object detection; and (3) a hardware acceleration circuit\nthat enables real-time, low-latency performance in intelligent sensing\napplications, delivering a 3.5x speedup and a 40% reduction in energy\nconsumption compared to standard GPU-based implementations These\nadvancements make iTask a scalable, efficient solution for complex,task-oriented detection in resource-constrained en",
  "task-oriented detection in resource-constrained environments.\nII. R ELATED WORK\nA. Vision-Language Models\nVision-Language Models (VLMs) integrate visual and textual data\nfor multimodal representation, bridging visual content with textual\nqueries. VinVL, for example, achieves high performance in multi-\nmodal tasks, utilizing a vision-language-enhanced object detection\nmodel [11]. MiniVLM adapts VLM principles for resource-constrained\nenvironments using a Two-stage Efficient Extractor (TEE) and a\n",
  "using a Two-stage Efficient Extractor (TEE) and a\ncompact transformer-based fusion module inspired by EfficientDet\nand MiniLM [12]–[14]. Despite their strengths, VLMs remain on the\ncritical path for decision-making, where costly and non-interpretable\noutputs present challenges. Guided by MiniVLM’s efficiency, we\ndesign iTask to combine intelligent sensing with LLMs, addressing\ntask-oriented detection challenges in edge computing scenarios while\nmaintaining interpretability and computational effi",
  "aintaining interpretability and computational efficiency.\nB. Task-Oriented Object Detection\nTask-oriented object detection [1], [2] focuses on identifying\nobjects relevant to specific tasks, requiring robust reasoning to address\ncomplex demands. Task-aligned One-stage Object Detection (TOOD)\nenhances classification and localization by generating task-interactive\nfeatures to reduce spatial misalignment [15]. Similarly, Wang et al.\n[16] developed a task-oriented degraded image enhancement network,",
  " task-oriented degraded image enhancement network,\nachieving precision improvements of up to 1.6% on degraded datasets\nlike URPC2020 [17]. However, achieving efficiency in TOOD remains\nchallenging due to the reliance on large, compute-intensive models. To\naddress these issues, our work integrates TOOD with intelligent sens-\ning and lightweight LLMs to enhance detection precision, efficiency,\nand generalization in resource-constrained edge environments, while\nleveraging hardware optimizations for",
  "ments, while\nleveraging hardware optimizations for task-specific inference.\nC. Intelligent Sensors\nIntelligent sensors have become a transformative innovation in\nautonomous systems by enabling near-sensor data processing, which\nreduces latency, enhances privacy, and minimizes bandwidth through\nselective data acquisition and compression. Huang et al. [18] demon-\nstrated notable energy savings using near-sensor AI models that\nprocess only frames of interest. Similarly, Zhang et al. [19] showed tha",
  " interest. Similarly, Zhang et al. [19] showed that\nadaptive thresholds improve compression and reconstruction accuracy,\n2\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "while Lu et al. [20] achieved dynamic threshold adjustments, en-\nhancing performance. Despite these advancements, intelligent sensing\nfaces significant challenges in algorithm-hardware co-optimization\n[21]. Our framework addresses these challenges by identifying regions\nof interest (ROIs) that enable lightweight sensors to filter irrelevant\ndata efficiently. Combined with task-oriented object detection and\nVLMs, this approach significantly improves the energy efficiency\nand adaptability of intel",
  "es the energy efficiency\nand adaptability of intelligent sensing systems. To achieve real-time\ndetection and reasoning in edge environments with limited resources,\nwe design and implement a domain-specific ASIC accelerator to\nenhance our model’s performance.\nD. Hardware Design for Intelligent Sensing\nHardware design is critical for intelligent sensing applications,\nwhere energy efficiency and accuracy are constrained by sensor type,\ndata resolution, and model complexity. Taha et al. [22] develop",
  "on, and model complexity. Taha et al. [22] developed a\nlow-power wireless transmission architecture that activates only criti-\ncal sensors, leveraging near-sensor deep learning to balance efficiency\nand accuracy. Similarly, Ballard et al. [23] optimized spectral encoding\nfor hyperspectral satellites, significantly reducing costly data transmis-\nsion overheads. Huang et al. [18] designed a near-sensor object de-\ntection model capable of capturing frames of interest, eliminating the\nneed for high-",
  "frames of interest, eliminating the\nneed for high-bandwidth transmission. However, hardware limitations\noften prevent running multiple algorithms like segmentation and ViT\non the same platform. To overcome this, our work introduces a unified\nASIC design that supports both convolution-based segmentation and\nattention-based ViT tasks. This architecture enables efficient taskCLIP\ninference by combining task-oriented detection with energy-efficient\nhardware acceleration, ensuring scalability and ada",
  "ardware acceleration, ensuring scalability and adaptability in dynamic\nedge environments.\nIII. F RAMEWORK\nA. Core Design of iTask for Efficient Task-Driven Detection\nIgnited by prior research [1], iTask initially integrates the open-\nvocabulary capability of OpenAI CLIP with the reasoning power of\nLLM. Subsequently, it emphasizes model compression along with\nhardware and software co-design for deploying the framework on edge.\nIt employs a multi-stage process to bridge the semantic gap between\nta",
  "tage process to bridge the semantic gap between\ntask affordances and visual object features, optimizing both accuracy\nand efficiency for edge computing scenarios.\nFigure 2 presents the framework of iTask . The first stage of\ntheiTask framework involves general object detection using pre-\ntrained models to identify potential object candidates. In this step,\nbounding boxes and object features are generated for all items in the\nscene. To ensure effective reasoning and selection, iTask integrates\nan",
  "ctive reasoning and selection, iTask integrates\nan LLM-guided parsing mechanism to extract task-specific affordance\nattributes, such as ”precision grip” or ”long handle,” relevant to the\ngiven prompt. These attributes are encoded as visual features, enabling\na unified embedding space for images and text.\nIn the second stage, task-specific object selection is performed using\na transformer-based aligner module. Inspired by TaskCLIP, the aligner\nrecalibrates embeddings from the vision-language mode",
  "alibrates embeddings from the vision-language model (VLM),\naddressing misalignment between object images (e.g., nouns) and their\nvisual attributes (e.g., adjective phrases). The aligner’s multi-layer\ndesign incorporates self-attention and cross-attention mechanisms to\nrefine the embeddings, ensuring they accurately represent task-specific\naffordances. This recalibration improves the semantic alignment and\nensures high-quality reasoning-driven selection of objects.\nFinally, iTask employs a traina",
  "ection of objects.\nFinally, iTask employs a trainable scoring function to process\nthe affinity matrix generated by embedding alignment. This scoring\nfunction ranks objects based on their suitability for the specified task,\nABounding box \nimage and mission\nTextual properties \nof mission: \nsmall, pointed, non-\nslip, and ergonomicEmbedding generation\nText \nembeddings\nB\nCLIP Vision \nTransformer\nCLIP T ext \nTransformerTeacher Model Teacher image \nembedding\nAligned Teacher \nimage embeddingAligned T ex",
  "dding\nAligned Teacher \nimage embeddingAligned T ext embeddings\nTransformer \naligner\nSmall Vision \nTransformerText and image embedding alignmentC\nStudent Model\n𝑳=𝝀𝟏∥𝑬𝒕−𝑬𝒔∥𝟏​+𝝀𝟐​𝑳𝑪𝑬(𝒚𝒕,𝒚𝒔)Loss function ( 𝑳)Fig. 3: Diagram of the iTask framework with knowledge distillation.\nThe teacher model (CLIP ViT) generates image and text embeddings\ndistilled to a smaller ViT model. A transformer aligner enhances align-\nment for task-specific object detection, addressing low performance\nwhen using pre-aligned ",
  "addressing low performance\nwhen using pre-aligned image embeddings.\nenabling robust task-oriented detection. By leveraging the inherent ca-\npabilities of VLMs and combining them with reasoning mechanisms,\niTask achieves scalable and efficient detection, outperforming single-\nstage approaches in terms of generalizability and adaptability.\nB. Lightweight Adaptation of TaskCLIP\nTo address the computational demands of the iTask framework,\nwe introduce a strategy for lightweight adaptation, optimizin",
  "e a strategy for lightweight adaptation, optimizing the\nperformance of large vision-language models (VLMs) like TaskCLIP\nfor resource-constrained environments. This approach employs two\nkey techniques, affinity mimicking and weight inheritance, to compress\nand accelerate the model while maintaining robust task-oriented de-\ntection capabilities. Affinity mimicking allows the lightweight model\nto replicate the cross-modal feature alignment of a larger VLM. The\nimage-text affinity scores generated ",
  "ger VLM. The\nimage-text affinity scores generated by the large model, Alarge, are\ndistilled into the lightweight model, Alight, using the following loss\nfunction:Laffinity =∥Alarge−Alight∥2\nF,\nwhere ∥ · ∥ Fdenotes the Frobenius norm. This ensures that the\nlightweight model retains essential semantic understanding for task-\nspecific detection. To improve initialization, critical weights from the\nlarge model, θlarge, are transferred to the lightweight model, θlight, using\na selective masking strat",
  "ght model, θlight, using\na selective masking strategy:\nθlight=M⊙θlarge,\nwhere Mis a learnable mask identifying important weights, and ⊙\ndenotes element-wise multiplication. By integrating these techniques,\nwe significantly reduce the model size and computational complexity\nofiTask while preserving its task-specific detection performance. As\nshown in the inference cost breakdown on the right side of Figure 2,\nthe proposed approach reduces reasoning and segmentation costs,\nenabling efficient and a",
  "g and segmentation costs,\nenabling efficient and accurate task-driven detection across diverse\nreal-world applications.\nWhile smaller CLIP models reduce computational cost, they may\nsuffer from quality degradation; however, knowledge distillation ef-\nfectively enhances their accuracy. To further support this efficiency,\na unified ASIC design accelerates both segmentation (convolution-\nbased) and ViT (attention-based) tasks on the same hardware. This\nhardware optimization achieves significant com",
  "his\nhardware optimization achieves significant computational efficiency\nby seamlessly integrating the distinct requirements of these tasks,\nmaking TaskCLIP inference highly suitable for resource-constrained\nenvironments such as edge computing systems.\nC. Knowledge Distillation for Task-Specific Precision\nTo address the computational demands of the iTask framework\nwhile maintaining high task-specific performance, we propose a\nknowledge distillation technique that uses EfficientNet as a starting\np",
  "n technique that uses EfficientNet as a starting\npoint to train a lightweight student model as a substitute for the vision\ntransformer in CLIP, as shown in Figure 3. This approach targets\nthe challenge of achieving the quality of large CLIP models without\n3\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "Weight\t&\tBias\tBuﬀer\n(Ping-Pong\tSR)\nFeature\tBuﬀer\n(Ping-Pong\tSR)\nHost\tInterface\nOutput\tBuﬀer\n(Ping-Pong\tSR)\nPE\nMatrix\nOut\tBuﬀer\nScheduler\tSRs\nWS\n/\nIS\n/\nOS\n\t\nDist.\nInput\nController\nOutput\nController\nBias/PSum\nI\nw\nPSum\n×\nPE\nWeight\nInput\n'd0\nInstr.\tMem.\nBias/PSum\nI\nw\nPSum\n×\nPE\nWeight\nInput\n'd0\n+\n+\nw\n11\nw\n12\nw\n13\nw\n21\nw\n22\nw\n23\nw\n31\nw\n32\nw\n33\nI\n11 \nI\n12 \nI\n13\nb\n11\nb\n13\nb\n21\nb\n22\nb\n23\nInput\tRShi\nI\n21 \nI\n22 \nI\n23\nI\n31 \nI\n32 \nI\n33\nI\n41 \nI\n42 \nI\n43\nO\n31\nO\n22\nO\n13\nO\n21\nO\n12\nO\n11\nOutput\tDShi\nb\n12\nBias/PS",
  "O\n22\nO\n13\nO\n21\nO\n12\nO\n11\nOutput\tDShi\nb\n12\nBias/PSum\nw\nI\nPSum\n×\nPE\nInput\nWeight\n'd0\n+\nI\n11\nI\n21\nI\n31\nI\n12\nI\n22\nI\n32\nI\n13\nI\n23\nI\n33\nw\n11 \nw\n21 \nw\n31\nb\n11\nb\n13\nb\n21\nb\n22\nb\n31\nWeight\tRShi\nw\n12 \nw\n22 \nw\n32\nw\n13 \nw\n23 \nw\n33\nO\n13\nO\n22\nO\n31\nO\n12\nO\n21\nO\n11\nOutput\tDShi\nb\n12\nWS\nIS\nswap\nOS\nI\n11 \nI\n12 \nI\n13\nw\n11\nw\n11\nw\n12\nw\n12\nw\n12\nInput\tRShi\nI\n12 \nI\n13 \nI\n14\nI\n13 \nI\n14 \nI\n15\nI\n21 \nI\n22 \nI\n23\nO\n11\nO\n11\nO\n11\nO\n12\nO\n12\nO\n13\nw\n11\nch0\nch2\nch1\nWeight\tDShi\nGoal:\tCONV33\nkernel:\t\nw\n11\n,…,w\n33\nGoal:\t\nI⋅w\t+\tb\nI,\t",
  "l:\tCONV33\nkernel:\t\nw\n11\n,…,w\n33\nGoal:\t\nI⋅w\t+\tb\nI,\tw,\tb\n:\tmatrix\nGoal:\t\nI⋅w\t+\tb\nI,\tw,\tb\n:\tmatrix\nO\n13\nO\n12\nO\n13\nOutput\nOutput\nOutput\nMHSA\n\t-item:\t196\n\t-dim:\t\t192\n\t-headCount:\t3\nMLP\n\t-input:\t\t196x192\n\t-output:\t192\n\t-hiddenLayer:\t768\nSub-batches\nCyc.-Acc.\nSimulator\nAssembler\nFind\tBest\tDataﬂow\nInstruction\tCodes\nDataﬂow\tSettings\nFront-end\nAI\tModel\nExtract\tSub-batches\nMM\t196x192\t192x192\nMM\t196x192\t192x192\nMM\t196x192\t192x192\nMM\t196x\t64\t\t64x196\nMM\t196x196\t196x\t64\n…\nMM\t196x192\t192x768\nMM\t196x768\t768x192\n",
  "6\t196x\t64\n…\nMM\t196x192\t192x768\nMM\t196x768\t768x192\nBatch0-\tWS\nBatch1-\tWS\nBatch2-\tWS\nBatch3-\tOS\nBatch4-\tIS\n…\nBatchN-\tIS\nHW\nConﬁg.\n-Cols:\t32\n-Rows:\t32\n-FIFODepth:\t128\n(a)\n(e)\nSched.\nSRs\n(b)\n(c)\n(d)Fig. 4: The hardware/software stack for iTask acceleration. ( a) The proposed systolic-array-based hardware accelerator architecture. ( b) Weight\nstationary dataflow implemented with our PE design. ( c) Input stationary dataflow implemented with our PE design. ( d) Output stationary\ndataflow implemented w",
  "ign. ( d) Output stationary\ndataflow implemented with our PE design. ( e) The compiler stack for the proposed accelerator architecture.\nincurring their computational costs. Our distilled model balances\naccuracy and efficiency effectively, making it suitable for resource-\nconstrained environments.\nIn this setup, the original CLIP vision transformer serves as the\nteacher model, guiding the training of a compact student model. The\nstudent model employs EfficientNet’s compound scaling methodol-\nogy ",
  "oys EfficientNet’s compound scaling methodol-\nogy [24], which systematically balances network depth, width, and\nresolution to achieve both efficiency and accuracy. This ensures that\nthe student captures essential visual patterns while aligning semanti-\ncally with textual embeddings. The distillation process minimizes a\ncombined loss function:\nL=λ1∥Et−Es∥1+λ2LCE,\nwhere EtandEsare the embeddings from the teacher and student\nmodels, respectively, and LCEis the cross-entropy loss between the\noutput ",
  "d LCEis the cross-entropy loss between the\noutput distributions of the two models. The L1 loss preserves the\nsemantic alignment between teacher and student embeddings, while\nthe cross-entropy loss ensures consistency in task-specific predictions.\nTo stabilize training and reduce representational gaps, knowledge is\ntransferred progressively in multiple stages. This approach ensures\nthat the student model iteratively refines its representations while\navoiding overfitting the teacher’s embeddings. ",
  "le\navoiding overfitting the teacher’s embeddings. As depicted in Figure 3,\na task-specific transformer aligner further refines the student model’s\nembeddings to enhance task-related affordances. This recalibration\naddresses subtle visual details crucial for precise object detection in\nresource-constrained environments.\nIV. H ARDWARE ACCELERATION\nAccelerating iTask benefits when utilizing unified hardware ar-\nchitecture accelerating both CNN and ViT. This requires a balance\nbetween general-purpos",
  "iT. This requires a balance\nbetween general-purpose and domain-specific nuances. The existing\nparallel processing hardware, such as GPU and FPGA, have negatively\nbiased nuances in terms of iTask acceleration. The former is biased\ntoward general-purpose nuance, enabling scalability but lacking energy\nefficiency. The latter can organize the dataflow more flexibly but lacks\ncomputing power due to its intrinsic architectural limitations, thus,\noptimized to to accelerate specific architectures. Moreo",
  "zed to to accelerate specific architectures. Moreover, current\ndomain-specific accelerators, whether FPGA-based or ASIC-based,\ntypically concentrate solely on either CNN or VIT acceleration,\nrendering them ill-suited to support state-of-the-art vision language\nmodels. To solve all these challenges, we designed the hardwareaccelerator for ASIC implementation, expanding the scalability to\ndiverse task-oriented applications for iTask .\nFig. 4 presents the hardware/software stack designed to acceler",
  "ts the hardware/software stack designed to accelerate\ntheiTask . The stack consists of the systolic array-based hardware ac-\ncelerator, and the domain-specific compiler to generate the instruction\ncodes executed on the accelerator.\nA. Hardware Accelerator\nFor most computation kernels in ViT and CNN workloads, we\ndesigned the systolic array architecture with 8-bit MAC in each PE,\nwhich enhances data reusability and overcomes memory-bound limi-\ntations. The unified PE design in Fig. 4(b)-(d) enabl",
  "ions. The unified PE design in Fig. 4(b)-(d) enables to support three\ndataflows, including the weight stationary (WS), input stationary (IS),\nand output stationary (OS), with proper organization of multiplexers\n(MUXes) in weight and partial sum (PSum) registers. This ensures\nthe optimal dataflow execution for each computation layer, practically\naccelerating diverse ViT and CNN configurations.\nOur architecture organized the input buffers (feature, weight, bias)\nas shift registers (SRs) with a pin",
  " weight, bias)\nas shift registers (SRs) with a ping-pong strategy, enabling seamless\nfeeding to the systolic array (see Fig. 4(a) PE Matrix). This avoids\nusing SRAM macros in ASIC design, as SRAM would present the\ncritical-path delay due to its addressing circuit, making it a bottleneck\nin our design, which adopts 8-bit operations to ensure high-speed\noperations. This not only increases the maximum frequency but also\nthe energy efficiency, as the ping-pong SRs do not require much\nmemory compared",
  " ping-pong SRs do not require much\nmemory compared to the SRAM buffer. Another notable characteristic\nis the adoption of scheduler SRs (see Fig. 4(a)) in both I/O buffers.\nThe scheduler SRs automatically generate the sequence required for\nthe systolic array computation, which is presented in Fig 4(b)-(d)\nfor each dataflows, to implement ViT and CNN operations from the\noriginal matrix data structure. This eliminates the host computer’s data\nre-ordering workloads, including pre- and post-processin",
  "ering workloads, including pre- and post-processing.\nB. Compiler Stack\nThe compiler stack, illustrated in Fig. 4(e), organizes a three-stage\noperation. The front-end part extracts the lower-level computation\nlayers, e.g., matrix multiplication or convolution, from the high-level\nmachine learning model architectures, including CNN and ViT. This\nenables the swift diversification of the model’s hyperparameters.\nThe computation layers extracted from the front-end are cast to the\ncycle-accurate simul",
  "the front-end are cast to the\ncycle-accurate simulator with systolic array configuration (M ×N). The\nsimulator executes each computation layer with supporting dataflows,\n4\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "Fig. 5: Examples of task-oriented object detection with iTask, where\nspecific objects are identified based on the task context. Tasks include\n’Strike ball,’ ’Pouring,’ ’Shaking,’ ’Eating,’ ’Throwing,’ and ’Riding,’\nwith detected objects highlighted in blue. This approach enables\ncontext-aware object recognition tailored to task-specific requirements.\nderiving the optimal dataflow for each layer. This simulator is devel-\noped upon Scalesim [25], modifying the cycle calculation to apply our\naccele",
  "odifying the cycle calculation to apply our\naccelerator’s data-path using ping-pong SR FIFO.\nFinally, the assembler produces the instruction code binary to\nexecute given machine learning tasks using specific hardware configu-\nrations. With the optimized dataflow for each layer from the simulator\nexecuted using the same configurations, the optimal code for the ML\nworkload is generated.\nV. E VALUATION\nThis section presents the evaluation of the iTask framework across\nvarious task-oriented object d",
  "sk framework across\nvarious task-oriented object detection tasks, highlighting the perfor-\nmance of our original configuration and quantized and knowledge\ndistillation configurations. We use AP@0.5 as our primary metric for\nevaluating task-oriented object detection, as it offers a suitable balance\nbetween precision and recall, especially relevant for task-specific\napplications that prioritize accurate identification over general object\ndetection. AP@0.5 means at a 50% Intersection over the Union",
  " AP@0.5 means at a 50% Intersection over the Union (IoU)\nthreshold, is chosen as the primary evaluation metric due to its robust-\nness in balancing precision and recall for task-specific object detection.\nThe machine learning side experiments were conducted on an NVIDIA\nRTX 4090, with hardware synthesis performed through 28 nmCMOS\ntechnology using Synopsys Design Compiler, specifically 0.81v @\n125C corner. The accelerator is designed with Chisel [26] for swift\ndeployment across diverse systolic ",
  "[26] for swift\ndeployment across diverse systolic array configurations. In this eval-\nuation, we selected the 128 ×128, a widely used configuration in\nother works. Table III presents the configurations and results. We also\nanalyze the energy and speed our hardware accelerator achieves in\ntask-oriented scenarios. In task-oriented applications, the focus is often\non detecting objects that meet specific criteria rather than maximizing\naccuracy across all detected instances. AP@0.5 allows us to prio",
  "s all detected instances. AP@0.5 allows us to prioritize\nobjects that align closely with each task’s intended functionality or\naffordances, making it ideal for scenarios where the utility of an object\n(e.g., can it hold flowers?) is more critical than precise localization.\nHigher IoU thresholds, while beneficial for general detection, may\npenalize task-oriented models by undervaluing objects that partially\nmeet task requirements, whereas AP@0.5 provides a balanced metric\nfor functional correctne",
  "rovides a balanced metric\nfor functional correctness in such cases.\nA.iTask Configuration Performance\nTable I showcases the performance of state-of-the-art (SOA) models\nfor task-oriented object detection on COCO tasks, including our\niTask implementation and its variations. Among these, iTask* (YOLO)\nachieves the highest average performance (45.01), demonstrating supe-\nrior task-specific accuracy while maintaining competitive results across\n21.6%31.1% 32.8%\n12.8%37.7%Quality Enhancement (%)\nItera",
  "1.1% 32.8%\n12.8%37.7%Quality Enhancement (%)\nIterations of trainingTinyCLIP\n20-Knowledge distillation\na.) b.)30-40-40-60-\n50- 30-\n20-\n10-\nAP @ 0.5\n1000-\n2000-\n3000-\n4000-\n5000-Fig. 6: (a) The graph shows the quality improvement of the student\nmodel implemented using knowledge distillation compared to tiny-\nCLIP, (b) convergence of TinyCLIP and Knowledge distillation at Task\n1.\nall tasks. This establishes our iTask framework as a leader in the SOA\nfor task-oriented object detection.\nOur iTask imp",
  "\nfor task-oriented object detection.\nOur iTask implementation includes two configurations: a region-\nbased RCNN approach and an efficient YOLO-based single-shot\ndetector. iTask (YOLO) achieves an average performance of 44.51,\nwhile iTask* (YOLO) further improves accuracy with task-specific\ntuning. These results highlight the robustness and adaptability of iTask\nacross diverse detection scenarios.\nWith iTask setting the SOA, the next step focuses on quantized\nversions of the model to reduce compu",
  "on quantized\nversions of the model to reduce computational cost, memory usage,\nand inference time. The goal is to create lightweight, fast, and accurate\nconfigurations suitable for deployment in resource-constrained envi-\nronments such as edge devices and real-time applications, extending\nthe impact of iTask to practical use cases.\nThe lightweight version, iTask Tiny, introduces a trade-off between\naccuracy and computational efficiency. Across tasks, iTask Tiny con-\nfigurations exhibit a quality",
  "sks, iTask Tiny con-\nfigurations exhibit a quality loss ranging from approximately 8% to\n12% relative to iTask (YOLO), depending on the specific quantization\nsettings. For example, more aggressive quantization configurations like\n(8-4) experience higher performance degradation, while less aggressive\nconfigurations such as (10-16) achieve reduced quality loss. Despite\nthis degradation, iTask Tiny retains sufficient accuracy for many\ntask-specific applications, providing a highly efficient alterna",
  "applications, providing a highly efficient alternative for\ndeployment in resource-constrained environments.\nThis quality loss reflects the inherent challenges in balancing\nlightweight design and task-specific accuracy. However, iTask Tiny\ndemonstrates that, through careful quantization and optimization,\nlightweight models can still deliver effective task-oriented object\ndetection while significantly reducing computational requirements.\nThese results highlight the potential for further refinement",
  "lts highlight the potential for further refinement of quantized\nmodels to bridge the gap between efficiency and performance for real-\nworld systems.\nB. Knowledge Distillation Results\nFigure 5 illustrates examples of task-oriented object detection using\niTask for the lightweight version of our model. Specific objects are\nidentified based on the context of various tasks, such as ’Strike\nball, ’ ’Pouring, ’ ’Shaking, ’ ’Eating, ’ ’Throwing, ’ and’Riding. ’ Detected\nobjects are highlighted in blue, ",
  "ding. ’ Detected\nobjects are highlighted in blue, demonstrating the model’s ability to\nperform context-aware object recognition tailored to task-specific re-\nquirements. This contextual adaptability underscores the effectiveness\nof the lightweight iTask framework in aligning object detection with\ntask-specific affordances.\nMoving to the quantitative analysis, Figure 6 provides deeper\ninsights into the performance improvements achieved through knowl-\nedge distillation. Subfigure (a) compares the ",
  "wl-\nedge distillation. Subfigure (a) compares the quality enhancement\nof EfficientViT, implemented using knowledge distillation, against\nTinyCLIP. The results clearly show that knowledge distillation leads\nto a significantly higher ceiling in terms of performance, with Ef-\nficientViT outperforming TinyCLIP in achieving task-specific quality\n5\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "om IEEE Xplore.  Restrictions apply.",
  "TABLE I: Comparison of different models across tasks with iTask variations.\nTask GGNN [27] TOIST [28] TOIST* [28] iTask (RCNN) [1] iTask (YOLO) [1] iTask* (YOLO) [1]\n1 36.6 44.0 45.8 44.23 44.23 48.31\n2 29.8 39.5 40.0 43.71 43.71 46.23\n3 17.2 23.5 26.9 31.65 31.64 30.71\n4 43.6 52.8 58.3 65.54 65.54 62.02\n5 21.0 23.0 32.5 37.46 37.46 37.74\nMean 29.64 36.56 40.7 44.52 44.51 45.01\nTABLE II: Comparisons to the prior AI accelerators.\nA3[29] SpAtten [30] Sanger [31] ViTCoD [32] ReDAS [33] This Work\nTa",
  "0] Sanger [31] ViTCoD [32] ReDAS [33] This Work\nTarget AI Model Transformer Sparse Transformer Sparse Transformer Sparse ViT General Purpose ViT + CNN\nTechnology ( nm) 40 40 55 28 28 28\nPrecision INT9 INT12 4 & 16 bits Not specified INT8 INT8\nFrequency ( MHz ) 1000 1000 500 500 1150 1850\nArea ( mm2) 2.08 18.71 16.9 3.0 20.77 6.14\nPower (mW) 110.422 2600∗2760 323.9 - 8330\nPeak Performance (TOPS) 0.22 2.00 2.56 0.256 18.84 30.31\nCompute Density (TOPS / mm2) 0.11 0.11 0.15 0.09 0.90 4.93\nEnergy Eff",
  "PS / mm2) 0.11 0.11 0.15 0.09 0.90 4.93\nEnergy Efficiency (TOPS / W) 1.99 0.77 0.93 0.79 - 3.64\n∗Execluded the DRAM power for fair comparison.\nTABLE III: Hardware Specifications.\nParameter Array Size Total Area Max Power Max Frequency Max Throughput\nValue 128×128 6.14mm28.33W 1.85GHz 30.31 TOPS\nTABLE IV: Model configurations for hardware evaluation\nLabel ViT CNN Param. Count (M)\nA TinyClip8 Resnet-18 19.96\nB TinyClip8 Faster R-CNN∗21.54\nC EfficientViT-m0 YOLOv4-tiny 24.41\nD TinyClip8 YOLOv4-tiny",
  "ntViT-m0 YOLOv4-tiny 24.41\nD TinyClip8 YOLOv4-tiny 24.13\nE EfficientViT-m5 Resnet-18 59.41\nF TinyClip32 Faster R-CNN∗74.67\nG EfficientViT-m5 YOLOv4-tiny 63.59\nH TinyClip32 YOLOv4-tiny 77.27\n∗Resnet-50 backbone (by fourth bottleneck block) used.\nimprovements. For instance, while both models improve over time, Ef-\nficientViT exhibits a sharper and more consistent quality enhancement,\ndemonstrating its capability to bridge the gap between lightweight\ndesign and task-specific accuracy.\nSubfigure (b)",
  "t\ndesign and task-specific accuracy.\nSubfigure (b) highlights the convergence behavior of TinyCLIP\nand the knowledge-distilled EfficientViT for Task 1. Knowledge\ndistillation demonstrates not only faster convergence but also superior\nfinal performance compared to TinyCLIP. This result underscores the\neffectiveness of leveraging a teacher-student paradigm to enhance the\ncompact model’s capacity for complex visual understanding.\nOverall, knowledge distillation enables lightweight models like\nEffic",
  "distillation enables lightweight models like\nEfficientViT to achieve task-specific accuracy comparable to larger\nmodels while maintaining efficiency. This approach ensures a robust\nand scalable solution for edge computing scenarios, as it balances\ncomputational constraints with the precision required for task-oriented\nobject detection.\nC. Energy Efficiency and Speedup Analysis\nTo evaluate compute performance, we configured the ViT/CNN\npairs within the iTask framework as detailed in Table IV, wit",
  "n the iTask framework as detailed in Table IV, with\ncorresponding compute latencies presented in Fig. 7. The results\ndemonstrate that our accelerator efficiently accommodates diverse\nconfigurations, spanning varying parameter counts and model architec-\ntures. This flexibility facilitates the selection of optimal configurations\ntailored to specific tasks and constraints, enhancing scalability and\nleveraging the strengths of both ViT and CNN configurations across\na range of deployment scenarios.\nT",
  "urations across\na range of deployment scenarios.\nTable II compares the state-of-the-art (SOTA) AI accelerators, show-\ning our accelerator has better compute density and energy efficiency\nA B C D E F G H0200400600Latency ( µs) CNN\nViTFig. 7: The latency breakdowns on CNN/ViT architectures executed\non the hardware accelerator.\nthan other works. This result originates from the dedicated design\ndiscipline for the iTask acceleration, including only the necessary\nmodules and replacing on-chip SRAM wit",
  "e necessary\nmodules and replacing on-chip SRAM with ping-pong SRs. At\nthe same time, other accelerators adopted hardware sparse attention\nsupport to reduce the required throughput itself by pruning [30]–\n[32] or aware of general-purpose acceleration for most AI-related\ncomputations [33]. Another reason is that the newer semiconductor\ntechnology node, 28 nm, is used to synthesize our accelerator.\nVI. C ONCLUSIONS\nIn conclusion, iTask delivers a powerful framework for task-\noriented object detecti",
  "werful framework for task-\noriented object detection by integrating hardware acceleration, flex-\nible model configurations, and situational awareness. Our hardware\nacceleration circuit significantly reduces latency and energy costs,\nmaking edge deployment feasible for real-time applications. The dual\nconfigurations—knowledge distillation for task-specific accuracy and\nquantization for versatile, multi-task generalization—offer adaptable\nperformance that meets diverse task requirements without ex",
  "ce that meets diverse task requirements without excessive\ncomputational demands. By enabling intelligent sensing, iTask selec-\ntively processes relevant data, optimizing resource use and providing\nan efficient, context-aware solution tailored for resource-constrained\nenvironments, from autonomous systems to real-time surveillance.\nACKNOWLEDGEMENTS\nThis work was supported in part by the DARPA Young Fac-\nulty Award, the National Science Foundation (NSF) under Grants\n#2127780, #2319198, #2321840, #",
  "(NSF) under Grants\n#2127780, #2319198, #2321840, #2312517, and #2235472, the Semi-\nconductor Research Corporation (SRC), the Office of Naval Research\nthrough the Young Investigator Program Award, and Grants #N00014-\n21-1-2225 and #N00014-22-1-2067, Army Research Office Grant\n#W911NF2410360. Additionally, support was provided by the Air\nForce Office of Scientific Research under Award #FA9550-22-1-0253,\nalong with generous gifts from Xilinx and Cisco.\n6\nAuthorized licensed use limited to: Zhejiang",
  "co.\n6\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "REFERENCES\n[1] H. Chen, W. Huang, Y . Ni, S. Yun, Y . Liu, F. Wen, A. Velasquez, H. Latapie, and M. Imani,\n“Taskclip: Extend large vision-language model for task oriented object detection,” arXiv preprint\narXiv:2403.08108 , 2024.\n[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object\ndetection with transformers,” CoRR , vol. abs/2005.12872, 2020.\n[3] G. Pantazopoulos, J. Bruyere, M. Nikandrou, T. Boissier, S. Hemanthage, B. K. Sachish, V . Shah,\nC. Do",
  "ier, S. Hemanthage, B. K. Sachish, V . Shah,\nC. Dondrup, and O. Lemon, “Vica: Combining visual, social, and task-oriented conversational\nai in a healthcare setting,” in Proceedings of the 2021 International Conference on Multimodal\nInteraction , ICMI ’21, (New York, NY , USA), p. 71–79, Association for Computing Machinery,\n2021.\n[4] J. Sawatzky, Y . Souri, C. Grund, and J. Gall, “What Object Should I Use? - Task Driven Object\nDetection,” in CVPR , 2019.\n[5] J. Tang, G. Zheng, J. Yu, and S. Yang,",
  ", 2019.\n[5] J. Tang, G. Zheng, J. Yu, and S. Yang, “Cotdet: Affordance knowledge prompting for task driven\nobject detection,” 2023.\n[6] A. Kamath, M. Singh, Y . LeCun, G. Synnaeve, I. Misra, and N. Carion, “Mdetr-modulated detec-\ntion for end-to-end multi-modal understanding,” in Proceedings of the IEEE/CVF international\nconference on computer vision , pp. 1780–1790, 2021.\n[7] C. Xie, Z. Zhang, Y . Wu, F. Zhu, R. Zhao, and S. Liang, “Described object detection: Liberating\nobject detection with f",
  "ject detection: Liberating\nobject detection with flexible expressions,” Advances in Neural Information Processing Systems ,\nvol. 36, 2024.\n[8] Y . D. Kwon, R. Li, S. I. Venieris, J. Chauhan, N. D. Lane, and C. Mascolo, “Tinytrain: Resource-\naware task-adaptive sparse training of dnns at the data-scarce edge,” 2024.\n[9] M. Tan and Q. V . Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,”\nCoRR , vol. abs/1905.11946, 2019.\n[10] K. Wu, H. Peng, Z. Zhou, B. Xiao, M. Liu, ",
  "9.\n[10] K. Wu, H. Peng, Z. Zhou, B. Xiao, M. Liu, L. Yuan, H. Xuan, M. Valenzuela, X. S. Chen,\nX. Wang, H. Chao, and H. Hu, “Tinyclip: Clip distillation via affinity mimicking and weight\ninheritance,” in Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV) , pp. 21970–21980, October 2023.\n[11] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y . Choi, and J. Gao, “Vinvl: Revisiting\nvisual representations in vision-language models,” in Proceedings of the IEEE/CVF Conf",
  "guage models,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pp. 5579–5588, 2021.\n[12] M.-A. Tnani, P. Subarnaduti, and K. Diepold, “Efficient feature learning approach for raw\nindustrial vibration data using two-stage learning framework,” Sensors , vol. 22, no. 13, p. 4813,\n2022.\n[13] M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient object detection,” in Proceed-\nings of the IEEE/CVF conference on computer vision and pattern r",
  "EE/CVF conference on computer vision and pattern recognition , pp. 10781–10790,\n2020.\n[14] J. Wang, X. Hu, P. Zhang, X. Li, L. Wang, L. Zhang, J. Gao, and Z. Liu, “Minivlm: A smaller\nand faster vision-language model,” arXiv preprint arXiv:2012.06946 , 2020.\n[15] C. Feng, Y . Zhong, Y . Gao, M. R. Scott, and W. Huang, “Tood: Task-aligned one-stage object\ndetection,” arXiv preprint arXiv:2108.07755 , 2021.\n[16] Y . Wang, J. Guo, R. Wang, W. He, and C. Li, “Tienet: task-oriented image enhancement n",
  " C. Li, “Tienet: task-oriented image enhancement network\nfor degraded object detection,” Signal, Image and Video Processing , vol. 18, no. 1, pp. 1–8, 2024.\n[17] C. Liu, H. Li, S. Wang, M. Zhu, D. Wang, X. Fan, and Z. Wang, “A dataset and benchmark\nof underwater object detection for robot picking,” in 2021 IEEE international conference on\nmultimedia & expo workshops (ICMEW) , pp. 1–6, IEEE, 2021.\n[18] W. Huang, A. Rezvani, H. Chen, Y . Ni, S. Yun, S. Jeong, and M. Imani, “A plug-in tiny ai modul",
  ", S. Jeong, and M. Imani, “A plug-in tiny ai module\nfor intelligent and selective sensor data transmission,” arXiv preprint arXiv:2402.02043 , 2024.\n[19] H. Zhang, J. Na, and B. Zhang, “Autonomous internet of things (iot) data reduction based on\nadaptive threshold,” Sensors , vol. 23, no. 23, p. 9427, 2023.\n[20] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, “Learning under concept drift: A review,”\nIEEE transactions on knowledge and data engineering , vol. 31, no. 12, pp. 2346–2363, 2018",
  "engineering , vol. 31, no. 12, pp. 2346–2363, 2018.\n[21] L. Yang and A. Shami, “Iot data analytics in dynamic environments: From an automated machine\nlearning perspective,” Engineering Applications of Artificial Intelligence , vol. 116, p. 105366,\n2022.\n[22] A. Taha, M. Alrabeiah, and A. Alkhateeb, “Enabling large intelligent surfaces with compressive\nsensing and deep learning,” IEEE access , vol. 9, pp. 44304–44321, 2021.\n[23] Z. Ballard, C. Brown, A. M. Madni, and A. Ozcan, “Machine learning a",
  "wn, A. M. Madni, and A. Ozcan, “Machine learning and computation-enabled\nintelligent sensor design,” Nature Machine Intelligence , vol. 3, no. 7, pp. 556–565, 2021.\n[24] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,”\ninInternational conference on machine learning , pp. 6105–6114, PMLR, 2019.\n[25] A. Samajdar, J. M. Joseph, Y . Zhu, P. Whatmough, M. Mattina, and T. Krishna, “A systematic\nmethodology for characterizing scalability of dnn accelerators ",
  "or characterizing scalability of dnn accelerators using scale-sim,” in 2020 IEEE\nInternational Symposium on Performance Analysis of Systems and Software (ISPASS) , pp. 58–68,\nIEEE, 2020.\n[26] J. Bachrach, H. V o, B. Richards, Y . Lee, A. Waterman, R. Avi ˇzienis, J. Wawrzynek, and\nK. Asanovi ´c, “Chisel: Constructing Hardware in a Scala Embedded Language,” in Proceed-\nings of the 49th Annual Design Automation Conference (DAC) , (San Francisco, CA, USA),\np. 1216–1225, 2012.\n[27] J. Sawatzky, Y . ",
  ", USA),\np. 1216–1225, 2012.\n[27] J. Sawatzky, Y . Souri, C. Grund, and J. Gall, “What object should i use?-task driven object\ndetection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pp. 7605–7614, 2019.\n[28] P. Li, B. Tian, Y . Shi, X. Chen, H. Zhao, G. Zhou, and Y .-Q. Zhang, “Toist: Task oriented instance\nsegmentation transformer with noun-pronoun distillation,” Advances in Neural Information\nProcessing Systems , vol. 35, pp. 17597–17611, 2022.\n[29] T",
  "g Systems , vol. 35, pp. 17597–17611, 2022.\n[29] T. J. Ham, S. J. Jung, S. Kim, Y . H. Oh, Y . Park, Y . Song, J.-H. Park, S. Lee, K. Park,\nJ. W. Lee, and D.-K. Jeong, “Aˆ3: Accelerating Attention Mechanisms in Neural Networks\nwith Approximation,” in 2020 IEEE International Symposium on High Performance Computer\nArchitecture (HPCA) , (San Diego, CA, USA), pp. 328–341, IEEE, Feb. 2020.\n[30] H. Wang, Z. Zhang, and S. Han, “SpAtten: Efficient Sparse Attention Architecture with Cascade\nToken and Hea",
  " Attention Architecture with Cascade\nToken and Head Pruning,” in 2021 IEEE International Symposium on High-Performance\nComputer Architecture (HPCA) , (Seoul, Korea (South)), pp. 97–110, IEEE, Feb. 2021.\n[31] L. Lu, Y . Jin, H. Bi, Z. Luo, P. Li, T. Wang, and Y . Liang, “Sanger: A Co-Design Framework\nfor Enabling Sparse Attention using Reconfigurable Architecture,” in MICRO-54: 54th Annual\nIEEE/ACM International Symposium on Microarchitecture , (Virtual Event Greece), pp. 977–991,\nACM, Oct. 2021.",
  "irtual Event Greece), pp. 977–991,\nACM, Oct. 2021.\n[32] H. You, Z. Sun, H. Shi, Z. Yu, Y . Zhao, Y . Zhang, C. Li, B. Li, and Y . Lin, “ViTCoD: Vision\nTransformer Acceleration via Dedicated Algorithm and Accelerator Co-Design,” in 2023 IEEE\nInternational Symposium on High-Performance Computer Architecture (HPCA) , (Montreal, QC,\nCanada), pp. 273–286, IEEE, Feb. 2023.\n[33] M. Han, L. Wang, L. Xiao, T. Cai, Z. Wang, X. Xu, and C. Zhang, “ReDas: A Lightweight\nArchitecture for Supporting Fine-Graine",
  "ightweight\nArchitecture for Supporting Fine-Grained Reshaping and Multiple Dataflows on Systolic Array,”\nIEEE Transactions on Computers , vol. 73, pp. 1997–2011, Aug. 2024.\n7\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:19:53 UTC from IEEE Xplore.  Restrictions apply.",
  "A Model -Specific End -to-End Design Methodology \nfor Resource -Constrained TinyML Hardware  \n \nYanchi Dong1, Tianyu Jia1,*, Kaixuan Du1, Yiqi Jing1, Qijun Wang2, Pixian Zhan2, Yadong Zhang2, Fengyun Yan2,  \nYufei Ma1, Yun Liang1, Le Ye1,3,* and Ru Huang1 \n1Peking University, Beijing, China , 2Nano Core Chip Electronic Technology, Hangzhou, China  \n3Advanced Institute of Information Techonology of Peking University, Hangzhou, China  \n*Corresponding Email : tianyuj@pku.edu.cn , yele@pku.edu.cn  \n",
  "ng Email : tianyuj@pku.edu.cn , yele@pku.edu.cn  \n \nAbstract— Tiny machine learn ing (TinyML) becomes \nappealing as it enables machine learning on resource -constrained \ndevices with ultra low energy and small form factor . In this paper, \na model -specific end -to-end design methodology is presented for \nTinyML hardware design. First, we intr oduce an end -to-end \nsystem evaluation method using Roofline models, which \nconsidering both AI and other general -purpose computing to \nguide the archit",
  "er general -purpose computing to \nguide the architecture design choices. Second, to improve the \nefficiency of AI computation, we develop an enhanced design s pace \nexploration framework, TinyScale , to enable optimal low-voltage \noperation for energy -efficient TinyML. Finally, we present a use \ncase driven design selection method to search the optimal \nhardware design across a set of application use cases. Our model -\nspecific design methodology is evaluated on both TSMC 22nm and \n55nm technol",
  "y is evaluated on both TSMC 22nm and \n55nm technology for MLPerf Tiny benchmark and a keyword \nspotting (KWS) SoC design. With the help of our end -to-end \ndesign methodology, an optimal TinyML hardware can be \nautomatically explored with significant energy and EDP \nimprovements for a diverse of TinyML use cases.  \nKeywords —TinyML, Accelerator, Design space exploration  \nI. INTRODUCTION  \nMachine learning (ML) has become a key technique in intelligent \nedge devices for numerous applications, su",
  "ligent \nedge devices for numerous applications, such  as AR/VR, healthcare, \nIoT, etc. With strong demand for edge intelligence  deployment , the \nedge devices are designed with tiny machine learning (TinyML) under \nextremely tight cost and energy constraints [1 -2]. The TinyML \napplications  span in a wide range fr om image classification to keyword \nspotting, etc [3], as shown in Fig. 1. The TinyML hardware is the edge \nplatform to support the end-to-end computation for end users. To enable \nu",
  "end-to-end computation for end users. To enable \nubiquitous deployment, TinyML devices need to be designed with low \npower, reso urce, and cost.  \nCurrently , TinyML applications are often deployed onto existing \nhardware, such as low -power MCUs in [1 -2]. However, the gap \nbetween the limited hardware resources and the required computing \nperformance is challenging to be met and keep incr easing, especially \nconsidering the rapid growth of the advanced machine learning models. \nTo alleviate th",
  "advanced machine learning models. \nTo alleviate the deployment challenges, innovations have been \nexplored across the design stacks from architecture -level to circuit -level \nand model optimizations. For e xample, to enhance ML computation \nperformance, domain -specific accelerators, e.g. neural processing units \n(NPUs), are commonly designed and deployed into edge chips, forming \na heterogenous architecture [4]. Neural architecture search (NAS) \napproach has bee n leveraged at model -level to ",
  " \napproach has bee n leveraged at model -level to automatic search optimal \nmodel architectures for specific hardware [5].  \nAlthough significant efforts have been dedicated to improving the \nAI performance, we show  the TinyML hardware needs to be designed \nfrom the perspective of  end-to-end execution. In fact, the AI \ncomputation is often executed in a small portion of time leading to the \nunder -utilization of NPU hardware [6 -7]. For example, for a keyword \nspotting (KWS) use case in Fig. 1",
  ", for a keyword \nspotting (KWS) use case in Fig. 1, the TinyML hardware needs to collect audio  data from microphones, then complete the analog -to-\ndigital conversion, digital signal processing (DSP) such as FFT, and AI \ninference, etc. In many Mobile SoC use cases [6], the AI execution only \nconsumes 50% of total latency. In the edge use cases summari zed in \n[7], the AI execution could be even less than 30%. Hence it is necessary \nto design TinyML hardware in an end -to-end perspective consid",
  "nyML hardware in an end -to-end perspective considering \nboth computation -intensive AI processing and other general -purpose \ncomputing for pre/post -processing (we use “non -AI” for short to refer \nthe processing not related to AI in this paper). As indicated by the \nAmdahl's Law, the system performance will be eventually bounded by \nthe non -AI computing if only accelerating AI computation.  \nLow Power  \n   < 0.1 WAI\n30~50%Runtime Breakdown\nImage  \nClassification\nAnomaly \nDetection\nTinyML App",
  "age  \nClassification\nAnomaly \nDetection\nTinyML Applications\nTimeAINon_AI\nin use idleCPU/DSP\nProcessingData\nTransfer\nMachine \nLearning TinyMLEmbedded \nSystem\nAI CPU\nLow Cost \n    < $3 Low R esource  \n  200 ~ 300 KBCPU/DSP\nProc.Data\nTransfer\nidle in useKeyword  \nSpotting\nNeural Network\nA/D\nAnalog \nSignalDigital \nCodeFeature \nVectors\nPre-op\nFFT\nMEL\nDCT\nMFCCEnd-to-End \nKWS\n \nFig. 1 TinyML and end -to-end execution for use cases.  \nAt the same time, TinyML device usually only needs to support one \nsp",
  "inyML device usually only needs to support one \nspecific set of use cases with a few ML models, such as speech \nrecognition in Amazon Alexa, or human detection in surveillance \ndevices. As li sted in MicroNet [2] and MLPerf Tiny Benchmark [3], the \nML models required by different applications vary greatly in terms of \nmodel size, operation number, bit precision, etc. Hence directly using \nexisting general -purpose MCUs or SoCs for TinyML applicatio ns is \nobviously neither energy -efficient nor ",
  "io ns is \nobviously neither energy -efficient nor cost -efficient. Designing model -\nspecific or application domain -specific TinyML hardware becomes the \nobvious choice. Google recently released an open -source framework \nCFU -playground [8] for custom TinyML hardware  design. However, \nthere are only limited acceleration units and currently can only be \nimplemented on FPGA. A systematic methodology is desirable for \ndomain -specific TinyML hardware design for  specific use scenari os. \nIn this",
  "ware design for  specific use scenari os. \nIn this work, we present an end -to-end d esign methodology to \nautomatically explore the optimal design for model -specific TinyML \nhardware. First, we introduce system evaluation method using Roofline \nmodels, which consider both AI and non -AI processing. The analysis \nshows the bottleneck of system often shifts from the AI to non -AI \noperations once the AI accelerator is overdesigned, which also indicates \nthe importance of designing TinyML with a ",
  "icates \nthe importance of designing TinyML with a system perspective. \nFurthermore, to improve the efficiency of AI computation, we develop \nan enhanced design space exploration (DSE) framework TinyScale , \nwhich provides the explorations in the TinyML design space with a \nwide voltage operation range. As the energy consumption is the key \nconcern for TinyML, our framework explores  minimum  energy \noperation using voltage and frequency scaling. Finally, a use case \ndriven design methodology is ",
  "Finally, a use case \ndriven design methodology is presented for a specific application or 2023 60th ACM/IEEE Design Automation Conference (DAC) | 979-8-3503-2348-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/DAC56929.2023.10247791\nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply.",
  "application domains. Based on the latency constraint of use cases, the \noptimal design parameters and operation conditions can be easily \nselected. Our model -specific end -to-end design methodology is \nvalidated through MLPerf Tiny benchmark and a TinyML SoC design \nfor KWS application. Evaluated using both TSMC 22nm and 55nm \ntechnologies, our methodology achieves up to 10.6×  energy and 26.5×  \nEDP  gain for end -to-end execution compared to a baseline architecture.  \nThe contributions of thi",
  " baseline architecture.  \nThe contributions of this paper are summarized as follows.  \n• We introduce a design methodology for model -specific \nTinyML hardware, which analyze system bottleneck using \nRoofline models with an end -to-end perspective.  \n• We introduce a DSE framework for TinyML enabling low -\nvoltage operations, which provide wider design space for \nenergy -constrained use cases.  \n• We introduce use case driven design and optimization, which \nguide the hardware architectu ral desi",
  "ion, which \nguide the hardware architectu ral design based on a set of \nTinyML use cases.  \n• We evaluate our methodology on 22/55nm for tasks in \nMLPerf Tiny benchmark and implement a 55nm TinyML \nSoC for KWS, with significant design benefits . \nII. TINYML BACKGROUND  \nTinyML Hardware.  Due to the requirement of  ubiquitous \ndeployment, low cost is one of the primary design targets for TinyML \nhardware. Therefore, only limited hardware resource and power \nbudget is allowed for TinyML chips. Tab",
  "and power \nbudget is allowed for TinyML chips. Table I lists a few examples of \nedge chips. To meet the low -cost and low -powe r targets, \nmicrocontroller units (MCUs) are the ideal hardware platforms [1 -2]. \nMCUs often include low -power CPUs, e.g. Cortex -M series, and a \nsmall amount of on -chip SRAM, e.g. hundreds of KB, leading to low \narea cost but limited performance and flexibili ty. E dge platforms with \nhardware acceleration enable higher computation performance. For \nexample, Nvidia",
  "gher computation performance. For \nexample, Nvidia Jetson Nano and Raspberry PI 4 contain Cortex -A \nseries CPUs and GPU modules. However, the power consumption is \none order magnitude large than the toler able power budget. Google \nexplored tightly coupled acceleration unit close to a RISC -V core for \nTinyML [8]. ARM introduces an edge solution combining efficient -\nCPU and NPU [9]. The heterogenous architecture is able to provide  \nboth high performance and flex ibility for general -purpose c",
  "erformance and flex ibility for general -purpose computing. \nIn this work, we focus on the TinyML design with tight power budget  \nin mW range, while our methodology can be extended . \nTABLE I.  EXAMPLES OF EDGE HARDWARE CHIPS \nDevice  CPU  Accel.  Memory  Power  \nSTM F446RE  M4 DSP 128KB  0.1W  \nRaspberry PI Pico  M0+  -- 264 KB  0.5W  \nJetson Nano  A57 GPU  4GB  5W \nARM [3]  M55  NPU  80-500KB  -- \nEnd-to-End Execution. For a typical TinyML use case, end -to-\nend execution is required from raw",
  " case, end -to-\nend execution is required from raw data acquisition from sensors, \nanalog -to-digital conversion, data pro cessing, and AI inference. \nHowever, the heterogeneous architecture in TinyML hardware is often \nbottlenecked by the expensive ML task offloading and \nCPU/accelerator under -utilization. The non -AI data processing \nconducted by CPU could take 50% to even 70% ru ntime in real use \ncases [6 -7]. The importance of end -to-end execution has also been \nrecognized by the communit",
  "xecution has also been \nrecognized by the community recently. A few ML processors have \nbeen  designed with reconfigurab le or tightly coupled architecture  to \nimprove the end -to-end perf ormance [10 -11]. \nTinyML Benchmark. TinyML performs ML inference at low -\npower edge for various application domains. A few datasets are \ncommonly used for each application domain, e.g. Google Speech \nCommands v2 dataset for keyword spotting. Standard benchmark  is \nnecessary to fairly evaluate performance, ",
  "rk  is \nnecessary to fairly evaluate performance, i.e. similar as the SPEC benchmark for CPU evaluations. MLPerf Tiny benchmark [3] is a \nrecent benchmark for popular TinyML tasks, which include visual \nwake words, keyword spotting, anomaly detection, and i mage \nclassification . In this work, we leverage MLPerf Tiny benchmark [3] \nand model variants in MicroNet [2] for evaluations.  \nSystem perf.AI \nboundNon-AI \nbound\nSystem eff.AI \nboundNon-AI \nbound\nAI perf. AI eff. System End -to-End Evaluat",
  "\nbound\nAI perf. AI eff. System End -to-End Evaluation\nEnhanced DSE \nw/ Voltage Scaling \nUse Case Driven \nArch. DecisionEnergyLatencyWide -Vol. \nPareto\nEnergyLatency Case 1Case 2Case 3Optimal DesignAI Processing - TinyScale Non AI Processing\nProcessing Unit : \nCPU/DSP/ASIC  EnergyLatency ASICDSPCPU\nArgmin total energy = AI energy + non -AI energy\ntotal latency = AI latency + non -AI latencySystem MetricsLatency \nconstraint\nArgmax \ntotal \nefficiencyLatency \nconstraintTinyML Use Cases\nSystem Eff. R",
  "yLatency \nconstraintTinyML Use Cases\nSystem Eff. Roofline System Perf. RooflineHigh eff. Non -AI  \nLow eff. Non -AI  Update Roofline Minimal Energy\nDesign Selection for AI and Non-AI\nSystem Evaluation\n \nFig. 2 Overview of end-to-end design methodology for TinyML.  \nIII. MODEL -SPECIFIC END-TO-END DESIGN METHOD  \nWe present an end -to-end design methodology for model -specific \nTinyML hardware with three steps, as illustrated in Fig. 2. First, the \nsystem performance and efficiency, which conside",
  " \nsystem performance and efficiency, which consider both AI and non -\nAI processing, are evaluated using Roofline models for sp ecific use \ncases. Two Roofline models are used interactively to search the \nminimal energy under target performance  (latency ) constraint. The \nsystem efficiency impact  from non -AI computation is identified to \nguide general -purpose comp utation selection. Second, to improve the \nefficiency of AI computation, we develop an enhanced DSE framework \nTinyScale , with l",
  "elop an enhanced DSE framework \nTinyScale , with low -voltage operation and use case driven design \nselection algorithm . Meanwhile, voltage and frequency adjustment is \nalso supported  according to system evaluation. Finally, the total system \nmetrics, i.e. energy and latency, are evaluated and trigger to the next \noptimization iteration.  \nTo explain our system design method, we use a KWS use case for \nillustration in this section. As show n in Fig.  1, a KWS system needs to \nsupport both non ",
  " Fig.  1, a KWS system needs to \nsupport both non -AI processing including FFT, DCT, in Mel Frequency \nCepstrum Coefficient (MFCC) feature extraction, and the AI inference \nto predict the keyword. In our experiment, the MFCC sampling rate is \nset to be 16KHz  or 8KHz , and frame length is 32ms with 16ms shift, \nwhich is similar as [12]. The non -AI computation can be processed by \neither low -power Cortex -M CPUs or in -house designed DSP. The AI \nmodel for our evaluations comes from the KWS CNN",
  " \nmodel for our evaluations comes from the KWS CNN model in [3]. We \nuse a  dedicated Simba -like [13] spatial accelerator for AI computation. \nThe CPU and AI accelerator share a limited global memory and are \ninterconnected by a standard AHB bus, which is common in MCUs. \nThe power numbers are generated at TSMC 22nm after place and route . \nA. System Evaluation for TinyML  \n1) Roofline Modeling for TinyML System  \nTargeting optimizing the end -to-end metrics of TinyML tasks, we \nfirst define sy",
  "o-end metrics of TinyML tasks, we \nfirst define system efficiency and performance as equations (1) and (2), \nwhich is the normalization of summation of both AI and non -AI \nprocessing. The system efficiency is normalized to the total energy for \nend-to-end completing  of a certain task. System performance  is \nnormalized after the reciprocal of latency for completing the task.  \nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplor",
  "on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply.",
  "𝑆𝑦𝑠𝑡𝑒𝑚  𝐸𝑓𝑓 .=𝑇𝑎𝑠𝑘\n𝑆𝑦𝑠.𝐸𝑛𝑒𝑟𝑔𝑦=𝑇𝑎𝑠𝑘\n𝐴𝐼 𝑜𝑝𝑠\n𝐴𝐼 𝑒𝑓𝑓.+𝑛𝑜𝑛𝐴𝐼  𝑜𝑝𝑠\n𝑛𝑜𝑛𝐴𝐼  𝑒𝑓𝑓.(1) \n𝑆𝑦𝑠𝑡𝑒𝑚  𝑃𝑒𝑟𝑓 .=𝑇𝑎𝑠𝑘\n𝑇𝑜𝑡𝑎𝑙  𝐿𝑎𝑡𝑒𝑛𝑐𝑦=𝑇𝑎𝑠𝑘\n𝐴𝐼 𝑜𝑝𝑠\n𝐴𝐼 𝑝𝑒𝑟𝑓 .+𝑛𝑜𝑛𝐴𝐼  𝑜𝑝𝑠\n𝑛𝑜𝑛𝐴𝐼  𝑝𝑒𝑟𝑓 .    (2) \nwhere 𝐴𝐼 𝑜𝑝𝑠 is the total operation number required for the AI \ntask, 𝐴𝐼 𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑐𝑦  is represented by OPS/W, same is for non -AI.  \nWe first adopt a baseline architecture with a Cortex -M4 CPU and \na 16-MAC spatial accelerator. Two KWS models with different size, \ni.e. large CNN L and small CNN S, and 512/256 -point FFT processing",
  "and small CNN S, and 512/256 -point FFT processing \nare defined as the AI and non -AI processing. Due to the slow frame \nrate of the audio signal and the low capacity of the non -AI processing, \nwe first observe the non -AI processing consume s more than 90% of \ntotal runtime for this use case (detailed results will be shown later), \nleading to a non -AI bounded system.  \nThe Roofline curves of the system at 22nm are shown in Fig. 3. \nUnder the same AI performance, it is observed that using an i",
  "ame AI performance, it is observed that using an in -house \ndesigned DSP ( point A ) can significantly improve non -AI computation, \nleading to 5×  system performance and 9.2×  efficiency improvement \ncompared with using Cortex -M4 CPU ( point B ). Compressing the AI \nmodel ( point A ) into a smaller size ( point C ) also all eviates the AI \ncomputation, resulting 2.4×  performance and 2.7×  efficiency increase. \nHowever, it is interesting to observe that improve AI performance \nitself will not ",
  "erve that improve AI performance \nitself will not improve the system roof, since this use case is eventually \nbounded by non -AI computation.  Before reaching the roof, non -AI \noperation number also has little impact on the system performance, e.g. \n512-point FFT in point A  versus 256 -point FFT in point D .  \nThe Roofline modeling provides the designer many architecture \ninsights to guide the design choices, e.g. system bottleneck could shift \nfrom AI  to non -AI computation once the AI accel",
  "\nfrom AI  to non -AI computation once the AI accelerator is \noverdesign ed. For example, deploying a high -efficiency AI accelerator, \ne.g. computing -in-memory engine with >10TOPS/W, will not help this \nsystem if the non -AI performance is low. This also indicates TinyML \nshould be designed in a system perspective rather than only focus on \nimproving AI efficiency.  \n0.1110100100010000\n1.00E-04 1.00E-03 1.00E-02 1.00E-01 1.00E+00 1.00E+01 1.00E+02\nAI effiency (TOPS/W)M4-CNNL-512FFT\nDSP4-CNNL-51",
  "02\nAI effiency (TOPS/W)M4-CNNL-512FFT\nDSP4-CNNL-512FFT\nM4-CNNS-512FFT\nDSP4-CNNS-512FFT\nM4-CNNL-256FFT\nDSP4-CNNL-256FFT\n1101001000100001000001000000\n1.00E-04 1.00E-03 1.00E-02 1.00E-01 1.00E+00 1.00E+01 1.00E+02\nAI effiency (TOPS/W)M4-CNNL-512FFT\nDSP4-CNNL-512FFT\nM4-CNNS-512FFT\nDSP4-CNNS-512FFT\nM4-CNNL-256FFT\nDSP4-CNNL-256FFT\nNorm. System Efficiency\n10-3System Perf. Roofline System Eff. RooflineNorm. System Performance\nAI Performance (TOPS) AI Efficiency (TOPS/W)10-210-1100101102\n10-810-610-410-2",
  "iciency (TOPS/W)10-210-1100101102\n10-810-610-410-210-310-210-110110-1100101102103104\nA\nBC\nD A\nBC\nD105\n \nFig. 3 Roofline modeling of 22nm TinyML hardware for KWS use \ncases with different FFT points and CNN models.  \n2) Optimal Design Exploration Flow  \nWe automatically explore the optimal TinyML design based on \nRoofline modeling under explicit latency and power constraints. For \ntypical TinyML use cases, the real -time operation, i.e. finish \ncomput ation within latency constraint, is the key r",
  "mput ation within latency constraint, is the key requirement to satisfy \nuser experience. Furthermore, the total power consumption is also a \nstrict design constraint since the DC/DC converter or LDO circuits only \ncan provide limited load current. Hence we always  search optimal \ndesign points under the strict performance and power constraints. Fig. \n4 shows the flowchart of our exploration flow. Given a use case with defined AI and non -AI workload, we start with an initial CPU and AI \nacceler",
  "load, we start with an initial CPU and AI \nacceleration unit and check whet her the system is bounded by AI or \nnon-AI via Roofline modeling. If the system is more bounded by AI, \nwe will optimize the AI acceleration unit through the TinyScale  \nframework, and then guide the non -AI optimization. If the system is \nmore bounded by non -AI at the beginning, we will select a better non -\nAI hardware to improve the system roof. Once the system efficiency \ndoes not increase after a few successive ite",
  "ency \ndoes not increase after a few successive iterations, the optimization is \nstopped with the optimal design choice.  \nWe adopt two optimizations fo r AI and non -AI computing , i.e \noptimal architectural exploration and voltage/frequency scaling. For \nexample, to improve non -AI processing, we support both architecture \nmodule improvement, such as offload task from CPU to a new DSP \nmodule, or scale voltage on the same CPU for energy saving. Fig. 4 also \nshows an example of improving the sys",
  "Fig. 4 also \nshows an example of improving the system efficiency by 3.84×  though \nthe voltage scaling for a same M4 CPU, while maintains the latency \nconstraint. The optimization iterations of searching the energy -optimal \ndesign have also been illustrated, i.e. from point A to F. To optimize AI \nprocessing, we use our TinyScale  framework to guide the architectural \nand voltage optimizations, which will be introduced in the following . \n100100010000100000\n0.0001 0.001 0.01 0.1 1 10 100System ",
  "010000100000\n0.0001 0.001 0.01 0.1 1 10 100System efficiency (Task/J)\nAI effiency (TOPS/W)System Eff. Roofline \n0.9v\n0.7v\n0.6v\n110100100010000\n0.0001 0.001 0.01 0.1 1 10 100System performance (1/s)\nAI effiency (TOPS/W)System Perf. Roofline \n0.9v\n0.7v\n0.6vSystem Perf. Roofline\nSystem Eff. RooflineSystem performance (1/s)\nAI Performance (TOPS)\nAI Efficiency (TOPS/W)10-1100\n10-2100102Latency constraint\noptimize \nNon-AI\ncheck\nlatency&\npoweroptimize AI \nwi TinyScale  bottleneck\nupdate\nroofline\nFailNo",
  "I \nwi TinyScale  bottleneck\nupdate\nroofline\nFailNon-AI AI \noutput optimal solutionPass\nNo\nYesinitial Non -AI & AI scheme\nNot improve  \nafter N \niterationsA B\nC D\n10-810-610-410-2101102System efficiency (Task/J) 10-4 102103104105\n3.84 xE F\nA B\nC D\nE F\n \nFig. 4 Flowchar t of system optimization method and improve roof \nthrough voltage scaling of non -AI processing unit.  \nB. Enhanced DSE Framework for TinyML  \nAs the diverse architectural and mapping design choices for AI \naccelerator design, we i",
  "ng design choices for AI \naccelerator design, we introduce a DSE framework TinyScale  to aid AI \noptimizations in TinyML. Previously, many DSE frameworks have \nbeen applied for NPU accelerators to aid the selection of computation \ndataflow and architectural parameters. For example, Timeloop  [14] and \nAccelergy [ 15] uses exhaustive and random approach  to find out data \nmapping strategy for optimal performance and energy. Sparseloop [16] \nsupports explorations and evaluations for sparse acceler",
  "ts explorations and evaluations for sparse accelerators based on \nanalytical model s. Medea [17] utilizes multi -objective evolutionary \nalgorithm to search the optimal des ign choice. Although these previous \nDSE frameworks provide design space for exploration, they only focus \non the explorations for NPU with spatial PE array architecture and \nmissing a methodology to guide AI design selection for heterogeneous \nTinyML hardwa re. \nTimeloop\nMapper\nModelmapping Process\nTiming \nRechar.\nAccelergy ",
  "er\nModelmapping Process\nTiming \nRechar.\nAccelergy  EstimatorEnergy Characterization\nArch.\nTinyML Hardware Search Space\nEnergy\nRef. TableDC/DC \nEfficiencyEvolutionary \nalgorithm \nPPA\nEnergy \nRechar.Memory NoC PE V/F\nNVM\ne.g. \neFlashGlobal Buffer < 256kB Weight Buffer < 32kB Voltage 0.6~0.9v\nPE     8 Input Buffer < 8kB Freq. Ratio 0.4~1\nAccum Buffer < 3kB\nMACs per PE  1~4\nlatency(s)energy(uJ)\n0                  0.01               0.028   10\n0.9v     0.8v\n0.7v     0.6vminimum_energy\n-36%\n \nFig. 5. ",
  "  0.8v\n0.7v     0.6vminimum_energy\n-36%\n \nFig. 5. Overview of TinyScale  framework and its saving benefit.  \nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply.",
  "TinyScale  is a DSE framework for TinyML with special support of \nwide -voltage operation, as shown in Fig. 5. We focus on the AI \naccelerators with a Simba -like [13] spatial architecture and utilize \nTimeloop/Accelergy framework [14 -15] to generate performance \nmodeling for different mapping conditions. To support accurate energy \nestimation under different voltages, we first recharacterize the timing \ninformation for standard cel l libraries using Cadence Liberate  tool. The \nrecharacterized",
  "using Cadence Liberate  tool. The \nrecharacterized low -voltage energy is combined with the Accelergy \nestimator [15] to generate accurate energy estimation for different \narchitectural and voltage choices. Furthermore, the conversion loss of \nDC/DC or LDO circuit  [18], is considered to better model the energy \nsaving at low voltage. To enable Pareto curve search, a multi -objective \nevolutionary algorithm [17] is leverage d to aid input design generation.  \nThe architecture choices of AI accel",
  "generation.  \nThe architecture choices of AI accelerator and t he saving result \nfrom TinyScale  for the KWS CNN L mode l is shown in Fig.5. We only \nallocate limited resources for AI to save the area cost. Hence the MAC \nnumber within each PE is limited to up to 4, and the on -chip memory \nsize is also constrained, e.g. the max global buffer is 64kB. We assume \nthe weights of AI models is stored within a non -voltatile memory, e.g. \neFlash, as in the MCU [1 -2]. TinyScale  is able to sweep the ",
  "n the MCU [1 -2]. TinyScale  is able to sweep the architectural \nand voltage choices and reports the energy/performance. Each plotted \npoint  in Fig. 5 indicates an architecture design and mapping choice. It \nis observed that 36% energy reduction is obtained by only optimizing \narchitectural parameters at nominal 0.9V. With enabling voltage scaling \nfrom  0.9V to 0.6V, additional 40% energy saving is obtained. We also \nobserved that the global buffer tends to be designed with larger size \nunder ",
  "ffer tends to be designed with larger size \nunder low -voltage, due to bandwidth mismatch between reduced PE \narray and NVM. With TinyScale  framework, we are able to explore the \nenergy -optimal accelerator for sp ecific AI model.  \nC. Use Case Driven Optimizations for TinyML  \nFor a TinyML hardware, it is normal to support a few AI models, \ne.g. customers may have different requirements of inference accuracy \nor keywords number for KWS application. It is necessary to take \nmultiple potential u",
  "ion. It is necessary to take \nmultiple potential use case models into considerations during hardware \ndesign. Hence we further leverage TinyScale  to explore the optimal \nhardware for a set of use cases.  \nWe still use KWS use case s for illustration. As shown in Fig. 6, \nthree common CNN mod els, e.g. CNN L, CNN M and CNN S, with \ndifferent model architectures, sizes, and accuracies are chosen for \ndeployment. The TinyML hardware is required to support all workloads \nunder same certain latency ",
  "support all workloads \nunder same certain latency constraints.  A hardware with the lowest total \nenergy con sumption or energy delay product (EDP) is desired across all \nuse case deployments. To evaluate the overall performance, we define \nthe Figure -of-Merit (FoM) based on either energy in (3) or EDP in (4).  \n𝐹𝑜𝑀 𝐸𝑛𝑒𝑟𝑔𝑦 =𝑎∗𝐸𝑛𝑒𝑟𝑔𝑦 𝐿+𝑏∗𝐸𝑛𝑒 𝑟𝑔𝑦𝑀+𝑐∗𝐸𝑛𝑒𝑟𝑔𝑦 𝑆(3) \n𝐹𝑜𝑀 𝐸𝐷𝑃 =𝑎∗𝐸𝑛𝑒𝑟𝑔𝑦 𝐿×𝐿𝑎𝑡𝑒𝑛𝑐𝑦 𝐿+\n𝑏∗𝐸𝑛𝑒𝑟𝑔𝑦 𝑀×𝐿𝑎𝑡𝑒𝑛𝑐𝑦 𝑀+𝑐∗𝐸𝑛𝑒𝑟𝑔𝑦 𝑆×𝐿𝑎𝑡𝑒𝑛𝑐𝑦 𝑆 (4) \nwhere  a, b, c are the utilization probabilities of each m",
  ", b, c are the utilization probabilities of each model in \npracti cal scenarios. For example, for a light -weight application \nscenario, half of its use cases may use smallest model CNN S, 30% uses \nCNN M, and 20% uses CNN L. The factors of a, b, c can set as 0.2, 0.3, \nand 0.5. Here we only discuss three models for illustratio n, our method \ncan be easily extended to multiple models in practical.  \nWe first generate the DSE exploration results by TinyScale  for each \nmodel. As shown in Fig. 6, ",
  "y TinyScale  for each \nmodel. As shown in Fig. 6, each model consumes notable different \nenergy and latency due to the different model archite ctures. Meanwhile, \nthe optimal architecture for one model does not works the best for other \nmodels. For example, we use the energy optimal design for CNN S as \nthe baseline. This architecture has a moderate performance in CNN M \nwhich has similar model size. B ut for larger model CNN S, it cannot \nsatisfy the latency constraint under the original voltag",
  "y the latency constraint under the original voltage and have to raise \nthe voltage.  \nLatency \nconstraintenergy(uJ)\nlatency(s)510\nEDP-optimal Energy -optimalFoM-37% -79%CNN L\nCNN M\nCNN Sa=0.2, b=0.3 c=0.5FOM EDP optimal\nFOM Energy  optImal\nEnergy S opt. for CNN s\nCNNL CNNM CNNS\nHyperparam.c(60,10,4,1,1)-\nc(76,10,4,2,1)-\nL(58)-FC(128)c(64,10,4,1,1)-\nc(48,10,4,2,1)-\nL(16)-FC(128)c(28,10,4,1,1)-\nc(30,10,4,2,1)-\nL(16)-FC(128)\nMemory 497.8KB 199.4KB 79.0KB\nOps 25.3M 17.3M 5.0M\nTrain 99.00% 98.60% 96.",
  "9.0KB\nOps 25.3M 17.3M 5.0M\nTrain 99.00% 98.60% 96.90%\nVal. 92.40% 92.20% 91.10%\nTest 92.70% 92.20% 91.60%\n0                      0.01                 0.02                  0.03CNN L_0.9v\nCNN L_0.8v\nCNN L_0.7v\nCNN L_0.6v\nCNN M_0.9v\nCNN M_0.8v\nCNN M_0.7v\nCNN M_0.6v\nCNN S_0.9v\nCNN S_0.8v\nCNN S_0.7v\nCNN S_0.6v \nFig. 6. Use case driven optimizations across three CNN models.  \nCompared to the architecture which only  works well for single use \ncase, our use case driven design exploration, i.e. summari",
  "r use case driven design exploration, i.e. summarized in \nAlgorithm 1, provides an architecture achieves 37% energy gain across \na set of use cases with meeting the latency constraint. To select a \nTinyML architecture arch best, we first select the architecture that can \nmeet the latency constraint cross all models. If the latency constraint is \nmet, we calculate the FoM of this architecture.  All the design choices \nthat meet the latency constraint will be traversed until the min imum \nFoM is f",
  "int will be traversed until the min imum \nFoM is found.  It worth to mentioning the target FoM can be chosen as \neither energy or EDP based on requirement of TinyML hardware. If \npursuing an EDP -optimal architecture, the method automatically \nexplored an architecture with 79% EDP gain.  \nAlgorithm 1 Use Case Driven Architecture Opt. Algorithm  \nInput:  Energy List [Dictionary  Ener 0 , Dictionary Ener 1, ..., \nDictionary Ener N-1], Latency List [Dictionary Latency 0 , \nDictionary  Latency 1, ..",
  "[Dictionary Latency 0 , \nDictionary  Latency 1, ..., Dictionary Latency N-1], Architecture \nList [ Arch 0, Arch 1, ..., ArchN-1], Latency Threshold Lth \nOutput:  Best Architecture for  N Workloads  arch best \narch = Arch 0[0], ener 0 = Ener 0[arch 0] \nFoM  = 0 \narch best  = 0 \nfor i : 0 → len(Arch 0) do \narch=Arch 0 [i] \nif Latency 0[arch] and  … and Latency N-1[arch]< Lth then  \n    if arch  Arch1 and ... and arch  Arch N-1 then \n    FoM Curr  = p0 × Ener 0(arch) + p1×Ener 1(arch)  \n+ ... + p",
  " = p0 × Ener 0(arch) + p1×Ener 1(arch)  \n+ ... + pN-1×Ener N-1(arch) // FoM Energy or FoM EDP \n        if -FoM Curr > -FoM  then \n            FoM  = FoM Curr \n            arch best = arch \n         end if  \n    end if  \nend if  \nend for  \nreturn arch best \nIV. BENCHMARK EVALUATIONS  \nA. Experiment Setup  \nTo evaluate the effective benefits of our design methodology, we \napplied our method onto TinyML design targeting the use cases from \nMLPerf Tiny benchmark [3], which includes four machine lear",
  "ny benchmark [3], which includes four machine learnin g \ntasks, keyword spotting (KWS), visual wake words (VWW), tiny image \nclassification (IC), and anomaly detection (AD). However, there is only \none ML model provided for each task category in the benchmark. To \nmimic the actual TinyML hardware that supports mu ltiple ML models, \nwe further extend the use case models based on MobileNet -V3 and \nanother open -source model zoo used in MicroNets [2]. The operation \nnumbers of ML models used in ou",
  "2]. The operation \nnumbers of ML models used in our evaluations are listed in Fig. 7. IC \nand VWW are visual processing tasks a nd usually require larger model, \nwhile AD only uses light -weight model s for anomaly detection.  \nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply.",
  "ResNet\nMobileNet\nV3-Small 0.75\nMobileNet\nV3-Small 1\n0    20    40    60    Operation (M)\nMicroNet -\nVWW -4MicroNet -\nVWW -2MobileNetV1\nFC-AutoEncoder\n(ops=0.559)\nAD-baseline\n(ops=0.52)\nDSCNN\nDSCNN -S\nDSCNN -M\nIC VWW AD KWSMLPerf IC1 IC2 VWW1\nVWW2 AD1 KWS1 KWS280     \nFig. 7. Evaluation models from MLPerf Tiny and MicroNets.  \nIC VWW AD KWS10%    30%    50%    70%    90%    M4 DSP M4 DSP M4 DSP M4 DSPAI non-AIDesign Exploration Space\n0.6~1.2VNon-AICortex -M0/M4\nDSP -SIMD4/8\n< 256kB AISpatial Arch",
  "ICortex -M0/M4\nDSP -SIMD4/8\n< 256kB AISpatial Arch. \n(4/8/16/32 MACs)...\n(Optional) CIM Arch. \nGlobal \nbuffer\nOff-chip \nIOSPI to FlashVoltage\n \nFig. 8. Runtime breakdown and architecture exploration space.  \nTo perform evaluations for a system with end -to-end execution, we \nfurther complement the pre/post -processing for each task based on real \napplication scenarios. For IC and VWW and tasks, the input is the \nimage with a size of 96× 96 or 32× 32 respectively, which is \npreprocessed by image ",
  " 32 respectively, which is \npreprocessed by image processing steps of demosaic and color \ncorrection. The demosaic processing uses bilinear interpolation during \nimplementation. For AD  task, we use log -Mel spectrogram as input \nfeature sent to AI accelerator with a frame size of 1024 and 128 Mel \nfilters. Five frames are combined to represent 640 dimensional input \nfeature vectors. For KWS, we include the MFCC to extracting feature, \nwhich includes pre -emphasis, framing, windowing, FFT, and M",
  "udes pre -emphasis, framing, windowing, FFT, and Mel \nfiltering and DCT. The sampling rate is set to a standard 16KHz, and \nframe length is 32ms with 16ms shift.  \nB. Evaluation for MLPerf Tiny Task  \nWe first characterize the end -to-end execution of MLPerf  Tiny \ntasks with a set of architectural choices at 22nm, as listed in Fig. 8. T o \nsupport non -AI computing, we provide low -power ARM Cortex -M \nseries CPUs and in -house DSP module choices with configurable \nSIMD acceleration. DSP module",
  "s with configurable \nSIMD acceleration. DSP module contains  dedicated single instruction \nmultiple data (SIMD) vector computation , and t he number of SIMD \nrepresents  the data parallelism in side DSP.  For the AI accelerator, we \nadopt commonly referred Simba -like spatial accelerator architecture, \nwith a small amount of PE number to satisfy the power constraint. The \nglobal buffer size is limited within 256kB, which can exchange data \nwith on -chip NVM or off -chip Flash. Our TinyScale  sup",
  "n -chip NVM or off -chip Flash. Our TinyScale  supports low -\nvoltage explorations for TSMC 22/55nm, with a wide voltage range \n0.6-0.9V a nd 0.6 -1.2V. We focus on TinyML architecture exploration \nsince the limited IO speed and access in real use cases.  \nThe runtime breakdown is first analyzed for an architecture with \nCortex -M4 or DSP for non -AI and a fixed AI accelerator with 16 MACs \nfor AI pr ocessing. It is observed that the non -AI processing varies \nnotably across different tasks. Esp",
  "essing varies \nnotably across different tasks. Especially, for KWS and AD tasks with \na moderate AI model size, the non -AI processing for the audio signal \nfeature extraction consume a large portion of runtime and bound the \nsystem performance. We further offload the non -AI from M4 CPU to a \nDSP module to parallel data processing, the roof of non -AI is increased \nand the runtime of non -AI bounded tasks, e.g. AD, can be significantly \nimproved.  \nWe then perform a 22nm desig n exploration for",
  ".  \nWe then perform a 22nm desig n exploration for specific models in \nMLPerf Tiny benchmark. To satisfy TinyML requirement, limited \npower budget is given as 20mA, and the latency constraints are set \nbased on the latest MLCommons v0.7 results for [3]. We still use a \ncommon heterogeneous ar chitecture with M4 CPU and a 16 MAC \naccelerator operating at nominal 0.9V as the exploration baseline. Fig. \n9 and Table 2 shows our automatic end -to-end design exploration \nfollowing the flowchart in Fig",
  "design exploration \nfollowing the flowchart in Fig. 4 with 10 iterations.  For IC and VWW which have lar ge AI model and relative light \nimage processing, M4 CPU is chosen for non -AI to reduce design cost \nand an AI accelerator with higher performance, e.g. 32 MACs, is \nselected for AI computation. In addition, the voltage/frequency is auto \nscaled down while mee t latency constraint.  A 3.5×  and 3.5×  energy \nsaving, i.e. 4.8×  and 2.1×  EDP gain, is obtained via the exploration of \nAI archit",
  "ain, is obtained via the exploration of \nAI architecture, mapping strategy, and operation conditions. For AD \nand KWS with small AI model and heavy non -AI computation for \nacoustic f eature extraction, an in -house DSP is selected to accelerate \nnon-AI computation. The system roof of AD and KWS is significantly \nincreased due to non -AI improvement, leading to more latency margin \nfor energy optimizations. Due to the light -AI requirement, o nly a few \nMACs, e.g. 8 MACs for AD, are allocated fo",
  " a few \nMACs, e.g. 8 MACs for AD, are allocated for AI. In addition, TinyScale  \nsuggests scaling the operation voltage further down to 0.6V without \nviolating latency requirement. Finally, more aggressive 10.6×  and 8.3×  \nenergy savings, i.e. 26.5×  and 22.4×  EDP improvement, are obtained .  \nTABLE II.  TINYML HARDWARE CHOICES FROM OUR METHODOLOGY  \n IC VWW  AD KWS  \nCPU  M4 M4 DSP4 DSP4 \nAccelerator  32 MAC  32 MAC  8 MAC  16 MAC  \nVoltage(v)  0.8 0.8/0.6  0.6 0.6 \nPower(mW)  26 7.01 0.94 3.",
  "  0.8 0.8/0.6  0.6 0.6 \nPower(mW)  26 7.01 0.94 3.17 \nLatency( ms) 13.05 42.9 9.27 50.7 \nLatency const. (ms) 20 60 20 60 \nEnergy Saving  3.0× 3.5× 10.6×  8.3× \nEDP Saving  4.8× 2.1× 26.5×  22.4× \nC. Evaluation for Multiple TinyML Use Cases  \nWe further perform the design optimizations for a set of use  cases. \nOur evaluation is based on the ML models in Fig. 7 and use the \noptimized architecture for single model in Table 2 as baseline. As \nshown in Table 3 and Table 4, our use case driven design",
  "in Table 3 and Table 4, our use case driven design method \nfurther improves the system metric of energy and EDP b y up to 39% \nand 79% considering the multiple use cases, due to the architectural or \noperation conditions adjustment to satisfy other use case model. For \nexample, as MobileNet V3 -small in IC task requires large input image \nwith a size of 224× 224, DSP is pre ferred to meet latency constraint. In \naddition, different voltage options are provided for different \nmodels.  Since the t",
  "s are provided for different \nmodels.  Since the two models in AD are similar, better FoM is achieved \nby adjusting the mapping strategy rather than changing \narchitecture.  The number of M AC units in KWS task is increased in \norder to meet the requirements of latency. In order to optimize the EDP \nFoM, the selected designs tend to work at 0.9v since frequency scaling \nis worse than energy scaling at low voltage.  \nTABLE III.  SYSTEM FOM IMPROVEMENT FOR ENER GY OPTIMIZATION  \n IC VWW  AD KWS  \n",
  "MENT FOR ENER GY OPTIMIZATION  \n IC VWW  AD KWS  \na, b, c 0.5,0.2,0.3  0.1,0.1,0.8  0.5,0.5  0.2,0.3,0.5  \nCPU  DSP8  M4 DSP4  DSP8  \nAccelerator  32 MAC  32 MAC  8 MAC  24 MAC  \nVoltage 0.8-0.9 0.6-0.9 0.6 0.6 \nFoM Improve  20% 25% 5% 39% \nTABLE IV.  SYSTEM FOM IMPROVEMENT FOR EDP OPTIMIZATION  \n IC VWW  AD KWS  \na, b, c 0.5,0.2,0.3  0.1,0.1,0.8  0.5,0.5  0.2,0.3,0.5  \nCPU  DSP8  M4 DSP4  DSP8  \nAccelerator  32 MAC  32 MAC  8 MAC  24 MAC  \nVoltage  0.9 0.9 0.9 0.9 \nFoM Improve  50% 65% 81% 70% ",
  "ge  0.9 0.9 0.9 0.9 \nFoM Improve  50% 65% 81% 70% \nWe also validated  our method across different technologies on \nTSMC 22  and 55nm. The sa me library characterization  process to \ngenerate energy table in TinyScale . As the saving results summarized \nin Table 5, our design methodology works well across technologies. \nDue to different voltages, i.e. 22nm/55nm has 0.9/1.2V nominal \nvoltage, 55nm shows a larger saving due to the wider scaling range.  \nAuthorized licensed use limited to: Zhejiang ",
  "e.  \nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply.",
  "TABLE V.  SAVING BENEFITS COMPARISON BETWEEN 22NM AND 55NM \nTask IC VWW  AD KWS  \n55nm  3.1× 3.9× 12.2× 9.5× \n22nm  3.0× 3.5× 10.6×  8.3× \nV. USE CASE STUDY : TINY KWS SOC \nBeyond the benchmark evaluations, we further appli ed our end -to-\nend design methodology onto a practical Tiny KWS SoC \nimplementation. The SoC is designed for support the  keyword  \nclassification from 30 keywords for Google Speech Commands v2 \ndataset. A set of ML models can be supported, including CNN, DS -\nCNN, and FC  w",
  "n be supported, including CNN, DS -\nCNN, and FC  with different model architectures. The SoC is set to have \na strict latency requirement to finish the end -to-end execution within \n100ms to fulfill the requirement of use interaction.  \nBased the requirements, we utilize our design methodology to guide \nthe d esign exploration, as shown in Fig. 10. Both the energy and EDP \noptimal design choices are labelled in Roofline modeling. To pursue \nlow-power for TinyML scenarios, we implemented the FoM ",
  "ower for TinyML scenarios, we implemented the FoM energy \noptimal design, which has a 13×  energy saving through the archi tecture \nand voltage operation condition exploration across all the target use \ncases.  \nDuring the implementation, a spatial CNN accelerator with a MAC \narray is allocated for the AI workload. For the general digital signal \nprocessing, an in -house designed DSP  module is utilized. We chose \nCortex -M0 CPU as the host to management the SoC control through \nAHB bus. The tar",
  "nagement the SoC control through \nAHB bus. The target SoC is implemented at 55nm, due to the \nconsideration of low cost and great energy saving benefit from voltage \nscaling. The SoC is also impleme nted with UPF design flow and on -\nchip regulators for voltage scaling. The layout view of the Tiny KWS \nSoC shown in Fig. 10.  \nAI EngineCortex \nM0\nIn-house \nDSP\nNorm. System Eff.System Eff. Roofline\nAI Efficiency (TOPS/W)10-410-2100102 100101102103104105\n10-1System Perf. RooflineNorm. System Perf.\n",
  "04105\n10-1System Perf. RooflineNorm. System Perf.\nAI Performance (TOPS)100101102103\n10-810-610-410-2FC\nDSCNN\nCNN\nTinyML KWS SoC Specs\nModels Ops Memory V/F\nCNN,\nDS-CNN,\nFC5-25M <300kBVoltage 0.6~1.2V\nFreq. 10 -50MHz\n \n106\n10-2FOM Energy  opt.FOM EDP opt.\nFOM EDP opt.FOM Energy  opt.\n \nFig. 10 Implemented 55nm KWS SoC with its optimizations.  \nVI. CONCLUSION  \nIn this paper, an end -to-end desig n methodology is presented for \nmodel -specific TinyML hardware design. Our method contains three \nmai",
  "ML hardware design. Our method contains three \nmain contributions, including end -to-end system evaluation method \nusing Roofline models, an enhanced wide -voltage design space \nexploration framework TinyScale , and use case driven design selection \nmethod. Our end -to-end design methodology is evaluated for a series \nof specific models from MLPerf Tiny benchmark and applied for a \n55nm Tiny KWS SoC design. Our design method shows significant \nenergy saving and perform ance improvement for TinyM",
  "ergy saving and perform ance improvement for TinyML use cases.  ACKNOWLEDGMENT  \nThis work was supported in part by National Key R&D Program of \nChina No. 2022YFB4400600, NSFC Grant No. 92164301 and \n62225401, and the 111 Project.  \nREFERENCES  \n[1] J. Lin et al., “MCUNet: tiny deep learning  on IoT devices”, NeurIPS , \n2020.  \n[2] C. Banbury, et al., “MicroNets: neural network architectures for \ndeploying TinyML applications on commodity microcontrollers”, \nMLSys , 2021.  \n[3] C. Banbury, et al",
  "ntrollers”, \nMLSys , 2021.  \n[3] C. Banbury, et al., “MLPerf Tiny Benchmark”, NeurIPS  Track on \nDatasets and Benchmar ks, 2021.  \n[4] H. Genc, et al., \"Gemmini: enabling systematic deep -learning \narchitecture evaluation via full -stack integration\",  IEEE DAC , 2021.  \n[5] Y. Sheng, et al., “The larger the fairer? small neural networks can \nachieve fairness for edge devices”, IEEE DAC , 2022. \n[6] M. Buch, et al., “AI tax in mobile SoCs: end -to-end performance \nanalysis of machine learning in",
  "o-end performance \nanalysis of machine learning in smartphones”, IEEE ISPASS , 2021.  \n[7] T. Jia, et al., “NCPU: An embedded neural CPU architecture on \nresource -constrained low power devices for real -time end -to-end \nperformance”, IEEE MICRO , 2020.  \n[8] S. Prakash, et al., “CFU playground: full -stack open -source \nframework for tiny machine learning (tinyML) acceleration on \nFPGAs”,  arXiv:2201.01863 , 2022.  \n[9] A. Skillman, et al., “A technical overview of Cortex -M55 and Ethos -\nU55”",
  "technical overview of Cortex -M55 and Ethos -\nU55”, IEEE Hot Chips 32 Symposium , 2020.  \n[10] Y. Ju, et al., “A 65nm systolic neural CPU processor for combined \ndeep learning and general -purpose computing with 95% PE \nutilization, high data loc ality and enhanced end -to-end performance”, \nIEEE ISSCC , 2022.  \n[11] K. Ueyoshi, et al., “DIANA: an end -to-end energy -efficient digital \nand analog hybrid neural network SoC”, IEEE ISSCC , 2022.  \n[12] W. Shan, et al., “A 510 -nW wake -up keyword -",
  "12] W. Shan, et al., “A 510 -nW wake -up keyword -spotting chip using \nserial -FFT-based MFCC and binarized depthwise separable CNN in \n28-nm CMOS”, IEEE Journal of Solid -State Circuits , 2021.  \n[13] Y. S. Shao, et al., “Simba: Scaling deep -learning inference with \nmulti -chip-module -based architecture”, IEEE MICRO , 2019.  \n[14] A. Parashar, et al., “Timeloop: A systematic approach to dnn \naccelerator evaluation”, IEEE ISPASS , 2019.  \n[15] Y. Wu, et al., “Accelergy: an architecture -level ",
  "Y. Wu, et al., “Accelergy: an architecture -level energy estimation \nmethodology for accelerator designs”, IEEE ICCAD , 2019.  \n[16] Y. Wu, et al., “Sparseloop: an analytical approac h to sparse tensor \naccelerator modeling”, IEEE MICRO , 2022.  \n[17] E. Russo, et al., “MEDEA: a multi-objective evolutionary approach \nto DNN hardware mapping”, IEEE DATE , 2022.  \n[18] R. Jain, et al., “A 0.45 –1 V fully -integrated distributed switched \ncapacitor DC -DC conv erter with high density MIM capacitor ",
  "DC -DC conv erter with high density MIM capacitor in 22 \nnm tri -gate CMOS”, IEEE Journal of Solid -State Circuits, 20 14\n \n0.11101001000\n0.0000001 0.000001 0.00001 0.0001 0.001 0.01System performance (1/s)\nAI perf (TOPS)System Perf. Roofline \n10100100010000100000100000010000000\n0.0001 0.001 0.01 0.1 1 10 100System efficiency (Task/J)\nAI effiency (TOPS/W)System Eff. Roofline \nM4_0.9v+16MAC_0.9v\nDSP4_0.9v+16MAC_0.9v\nM4_0.7v+32MAC_0.7v\nM4_0.8v+32MAC_0.6v\n1101001000\n0.0000001 0.000001 0.00001 0.000",
  "C_0.6v\n1101001000\n0.0000001 0.000001 0.00001 0.0001 0.001 0.01System performance (1/s)\nAI perf (TOPS)System Perf. Roofline \n10100100010000100000100000010000000\n0.0001 0.001 0.01 0.1 1 10 100System efficiency (Task/J)\nAI effiency (TOPS/W)System Eff. Roofline \nM4_0.9v+16MAC_0.9v\nDSP4_0.9v+16MAC_0.9v\nDSP4_0.6v+8MAC_0.6v\n0.11101001000\n0.0000001 0.000001 0.00001 0.0001 0.001 0.01System performance (1/s)\nAI perf (TOPS)System Perf. Roofline \n1001000100001000001000000\n0.0001 0.01 1 100System efficiency ",
  "01000001000000\n0.0001 0.01 1 100System efficiency (Task/J)\nAI effiency (TOPS/W)System Eff. Roofline \nM4_0.9v+16MAC_0.9v\nDSP4_0.9v+16MAC_0.9v\nDSP4_0.6v+16MAC_0.6v\n0.1110100100010000\n0.0000001 0.000001 0.00001 0.0001 0.001 0.01System performance (1/s)\nAI perf (TOPS)System Perf. Roofline \n10100100010000100000100000010000000\n0.0001 0.001 0.01 0.1 1 10 100System efficiency (Task/J)\nAI effiency (TOPS/W)System Eff. Roofline \nM4_0.9v+16MAC_0.9v\nDSP4_0.9v+16MAC_0.9v\nM4_0.8v+32MAC_0.8v\nSystem efficiency (",
  "+16MAC_0.9v\nM4_0.8v+32MAC_0.8v\nSystem efficiency (Task/J)System Perf. Roofline System Eff. RooflineSystem performance (1/s)\nAI Performance (TOPS) AI Efficiency (TOPS/W)10-1100101102\n10-710-510-310-410-2100102101102103104105\nSystem efficiency (Task/J)System Perf. Roofline System Eff. RooflineSystem performance (1/s)\nAI Performance (TOPS) AI Efficiency (TOPS/W)System efficiency (Task/J)System Perf. Roofline System Eff. RooflineSystem performance (1/s)\nAI Performance (TOPS) AI Efficiency (TOPS/W)Sy",
  "/s)\nAI Performance (TOPS) AI Efficiency (TOPS/W)System Perf. Roofline System Eff. RooflineSystem performance (1/s)\nAI Performance (TOPS) AI Efficiency (TOPS/W)IC VWW KWS AD\n103\n10-710-510-310-410-2100102102103104105\n10-1100101102\n10-710-510-310-410-2100102100101102\n10-710-510-310-410-2100102baseline\n100101102\n10-11.1x\n3.0x0.6x\n3.5x2.5x2.7x10.6xSystem efficiency (Task/J)baseline\nbaselinebaseline\nLatency \n(20ms)8.3 x104\n106107103\n101102103104105106107 103\n101102103104105106107 103\n106\nLatency \n(20",
  "107 103\n101102103104105106107 103\n106\nLatency \n(20ms)Latency \n(60ms)Latency \n(60ms) \nFig. 9 D esign exploration for end -to-end execution of four tasks . \nAuthorized licensed use limited to: Zhejiang University. Downloaded on January 05,2026 at 07:47:41 UTC from IEEE Xplore.  Restrictions apply."
]