please summarize : With the advent of artificial intelligence (AI), real-world applications are rapidly expanding, fueling a trend to embed AI capabilities into IoT devices across diverse fields. However, traditional servercentric data processing, such as cloud computing, faces significant energy and latency challenges due to processing and communication overloads. Consequently, distributing AI workloads to edge devices has become a promising solution, with recent research focusing on enabling on-device AI through TinyAI models that support lightweight, local computations [1], [2]. In energy-constrained edge environments, integrating Processingin-Memory (PIM) architectures has emerged as a promising approach for executing AI applications efficiently [3]–[8]. PIM enhances performance and energy efficiency in memory-intensive tasks, such as AI applications, by minimizing the overhead of data movement between processing and memory units. Early PIM designs primarily employed volatile memories like SRAM [9] and DRAM [10], but these designs faced challenges in storing large neural network weights due to the continuous power demands of volatile memory and issues such as SRAM leakage power and the periodic refresh cycles required by DRAM. To address these limitations, PIM architectures based on non-volatile memories (NVMs), such as MRAM [11], [12] and ReRAM [13], [14], were proposed. These designs achieve high energy efficiency in weight storage, especially when combined with power-gating techniques. However, NVMs may introduce additional read/write latency, potentially impacting overall neural network performance. Consequently, recent advances in PIM design have introduced hybrid architectures combining SRAM and NVM, known as Hybrid-PIM (H-PIM). These architectures use NVM to store weight data and SRAM as a buffer for input and output data, thereby enhancing both performance and energy efficiency [15]–[17]. However, H-PIM faces distinct limitations in achieving optimal energy efficiency during dynamic scenarios where inference loads fluctuate in real time on edge devices. For instance, an edge device running a YOLO model for real-time object detection experiences substantial variations in processing demand depending on the number of objects detected per video frame. Operating at a fixed performance level across all time intervals—typically set for peak computational load—inevitably leads to inefficient energy consumption. To address this, we observe that the mismatch between fixed computing resources and dynamically changing workloads has long been a challenge in traditional CPU-centric architectures. Established solutions, such as Dynamic Voltage and Frequency Scaling (DVFS) [18]–[21] and heterogeneous multi-processor architectures with high-performance and low-power cores [22]–[24], have been extensively researched for this purpose. Our focus centers on heterogeneous architectures, as DVFS continues to face significant challenges in edge devices due to added design complexities, such as DC-DC converters and real-time power monitoring. In contrast, heterogeneous architectures, exemplified by ARM’s big.LITTLE architecture [25], are widely implemented in commercial processors, effectively improving energy efficiency by adapting to dynamic computational loads. Building on this insight, we propose configuring PIM modules, which integrate memory and Processing Elements (PEs) for independent computation, into highperformance and low-power configurations. This approach allows PIM architectures to dynamically balance performance and energy consumption throughout the application runtime. To further elaborate this idea, we introduce Heterogeneous-Hybrid PIM (HH-PIM), an architecture designed to dynamically optimize performance and energy efficiency for AI applications on edge devices. HH-PIM integrates two distinct PIM modules: a HighPerformance (HP) PIM module and a Low-Power (LP) PIM module. Each module’s memory consists of a hybrid configuration of MRAM and SRAM banks, resulting in four types of memory: HP-MRAM, HP-SRAM, LP-MRAM, and LP-SRAM. Unlike conventional hybrid PIM architectures—where NVM is primarily allocated for weight storage and SRAM is reserved as an input-output buffer—HH-PIM adopts an adaptive approach. During periods of high computational demand, HH-PIM actively utilizes SRAM for weight storage as well, maximizing responsiveness to fluctuating inference loads. Additionally, to capitalize on HH-PIM’s architecture, we propose an optimal data distribution algorithm that minimizes energy consumption by dynamically adjusting data allocation across the four memory types. This combinatorial optimization algorithm allocates weight data across HP-MRAM, HP-SRAM, LP-MRAM, and LP-SRAM to reduce energy use while balancing workload between HP-PIM and LPPIM. With this design, HH-PIM efficiently adapts to the changing computational demands of AI applications, achieving significant energy savings without compromising performance.